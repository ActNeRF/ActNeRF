['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True

loading data:   0%|          | 0/4 [00:00<?, ?it/s]
loading data: 100%|██████████| 4/4 [00:00<00:00, 57.78it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/67 [00:00<?, ?it/s]
loading data:  10%|█         | 7/67 [00:00<00:00, 64.25it/s]
loading data:  21%|██        | 14/67 [00:00<00:00, 67.34it/s]
loading data:  31%|███▏      | 21/67 [00:00<00:00, 67.60it/s]
loading data:  42%|████▏     | 28/67 [00:00<00:00, 68.04it/s]
loading data:  52%|█████▏    | 35/67 [00:00<00:00, 67.55it/s]
loading data:  63%|██████▎   | 42/67 [00:00<00:00, 66.90it/s]
loading data:  73%|███████▎  | 49/67 [00:00<00:00, 65.51it/s]
loading data:  84%|████████▎ | 56/67 [00:00<00:00, 64.54it/s]
loading data:  94%|█████████▍| 63/67 [00:00<00:00, 62.15it/s]
loading data: 100%|██████████| 67/67 [00:01<00:00, 65.04it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Loss at initial point 0.14899221062660217
Loss after optimization 0.0991068184375763
logm result may be inaccurate, approximate err = 1.6740727733308998e-06
Loss at initial point 0.1810089647769928
Loss after optimization 0.1134435385465622
logm result may be inaccurate, approximate err = 1.1612125249852222e-06
Loss at initial point 0.18413622677326202
Loss after optimization 0.0691443607211113
logm result may be inaccurate, approximate err = 7.573328683990761e-07
Loss at initial point 0.23814469575881958
Loss after optimization 0.04807308688759804
logm result may be inaccurate, approximate err = 3.028488766272129e-07
Loss at initial point 0.2508445680141449
Loss after optimization 0.06920401751995087
logm result may be inaccurate, approximate err = 6.08777574977692e-07
Loss at initial point 0.21749144792556763
Loss after optimization 0.0791483074426651
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Loss at initial point 0.14893218874931335
Loss after optimization 0.08798429369926453
logm result may be inaccurate, approximate err = 1.6740727733308998e-06
Loss at initial point 0.18104450404644012
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 1.1612125249852222e-06
Loss at initial point 0.18418021500110626
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 7.573328683990761e-07
Loss at initial point 0.23808743059635162
Loss after optimization 0.12780256569385529
logm result may be inaccurate, approximate err = 3.028488766272129e-07
Loss at initial point 0.2509813904762268
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 6.08777574977692e-07
Loss at initial point 0.21755895018577576
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Loss at initial point 0.14885735511779785
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 1.6740727733308998e-06
Loss at initial point 0.1810314804315567
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 1.1612125249852222e-06
Loss at initial point 0.1841074824333191
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 7.573328683990761e-07
Loss at initial point 0.23818805813789368
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 3.028488766272129e-07
Loss at initial point 0.2509477436542511
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 6.08777574977692e-07
Loss at initial point 0.21761000156402588
Loss after optimization 0.12780258059501648
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 109.47357177734375
Step 1, loss: 114.44832611083984
Step 2, loss: 118.48886108398438
Step 3, loss: 121.84396362304688
Step 4, loss: 125.51504516601562
Step 5, loss: 128.75582885742188
Step 6, loss: 130.88560485839844
Step 7, loss: 132.8477325439453
Step 8, loss: 134.24351501464844
Step 9, loss: 135.6007080078125
Step 10, loss: 136.1363525390625
Step 11, loss: 136.19834899902344
Step 12, loss: 135.5742950439453
Step 13, loss: 134.36878967285156
Step 14, loss: 133.07911682128906
Step 15, loss: 131.63296508789062
Step 16, loss: 130.63136291503906
Step 17, loss: 129.53834533691406
Step 18, loss: 128.82005310058594
Step 19, loss: 127.95782470703125
Step 20, loss: 127.27430725097656
Step 21, loss: 126.44606018066406
Step 22, loss: 125.65765380859375
Step 23, loss: 124.92676544189453
Step 24, loss: 124.09680938720703
Step 25, loss: 123.36939239501953
Step 26, loss: 122.6506118774414
Step 27, loss: 121.97948455810547
Step 28, loss: 121.28992462158203
Step 29, loss: 120.65772247314453
Step 30, loss: 119.97482299804688
Step 31, loss: 119.41069793701172
Step 32, loss: 118.7765121459961
Step 33, loss: 118.2477798461914
Step 34, loss: 117.71914672851562
Step 35, loss: 117.17636108398438
Step 36, loss: 116.76731872558594
Step 37, loss: 116.33112335205078
Step 38, loss: 115.88866424560547
Step 39, loss: 115.5501708984375
Step 40, loss: 115.1840591430664
Step 41, loss: 114.84568786621094
Step 42, loss: 114.56156158447266
Step 43, loss: 114.27392578125
Step 44, loss: 114.00092315673828
Step 45, loss: 113.77191925048828
Step 46, loss: 113.5416259765625
Step 47, loss: 113.34323120117188
Step 48, loss: 113.1676025390625
Step 49, loss: 112.9308090209961
Step 50, loss: 112.80497741699219
Step 51, loss: 112.59840393066406
Step 52, loss: 112.4378890991211
Step 53, loss: 112.30538177490234
Step 54, loss: 112.15139770507812
Step 55, loss: 112.01377868652344
Step 56, loss: 111.91109466552734
Step 57, loss: 111.76032257080078
Step 58, loss: 111.6469497680664
Step 59, loss: 111.57218933105469
Step 60, loss: 111.44482421875
Step 61, loss: 111.34733581542969
Step 62, loss: 111.26693725585938
Step 63, loss: 111.19437408447266
Step 64, loss: 111.0937728881836
Step 65, loss: 111.0328140258789
Step 66, loss: 110.95787048339844
Step 67, loss: 110.8502197265625
Step 68, loss: 110.78160095214844
Step 69, loss: 110.71018981933594
Step 70, loss: 110.61085510253906
Step 71, loss: 110.56757354736328
Step 72, loss: 110.50372314453125
Step 73, loss: 110.45498657226562
Step 74, loss: 110.38772583007812
Step 75, loss: 110.34561920166016
Step 76, loss: 110.31901550292969
Step 77, loss: 110.26871490478516
Step 78, loss: 110.21143341064453
Step 79, loss: 110.16012573242188
Step 80, loss: 110.0999526977539
Step 81, loss: 110.0511703491211
Step 82, loss: 109.98184204101562
Step 83, loss: 110.00477600097656
Step 84, loss: 109.9421615600586
Step 85, loss: 109.86742401123047
Step 86, loss: 109.81881713867188
Step 87, loss: 109.78618621826172
Step 88, loss: 109.7596664428711
Step 89, loss: 109.71289825439453
Step 90, loss: 109.65846252441406
Step 91, loss: 109.62969970703125
Step 92, loss: 109.60072326660156
Step 93, loss: 109.55989074707031
Step 94, loss: 109.54859161376953
Step 95, loss: 109.48759460449219
Step 96, loss: 109.49034881591797
Step 97, loss: 109.44155883789062
Step 98, loss: 109.41776275634766
Step 99, loss: 109.41133117675781
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 109.4070053100586
Step 1, loss: 114.51919555664062
Step 2, loss: 118.54434204101562
Step 3, loss: 121.82135772705078
Step 4, loss: 125.48604583740234
Step 5, loss: 128.533447265625
Step 6, loss: 130.75474548339844
Step 7, loss: 132.71444702148438
Step 8, loss: 134.00537109375
Step 9, loss: 135.45664978027344
Step 10, loss: 136.08642578125
Step 11, loss: 136.146484375
Step 12, loss: 135.57008361816406
Step 13, loss: 134.37338256835938
Step 14, loss: 132.9054718017578
Step 15, loss: 131.66029357910156
Step 16, loss: 130.5802459716797
Step 17, loss: 129.5166778564453
Step 18, loss: 128.62855529785156
Step 19, loss: 127.83956909179688
Step 20, loss: 127.07007598876953
Step 21, loss: 126.32836151123047
Step 22, loss: 125.49906158447266
Step 23, loss: 124.77452087402344
Step 24, loss: 123.9894027709961
Step 25, loss: 123.30242156982422
Step 26, loss: 122.63299560546875
Step 27, loss: 121.91321563720703
Step 28, loss: 121.24654388427734
Step 29, loss: 120.64813232421875
Step 30, loss: 119.96997833251953
Step 31, loss: 119.38392639160156
Step 32, loss: 118.83674621582031
Step 33, loss: 118.36736297607422
Step 34, loss: 117.85220336914062
Step 35, loss: 117.357666015625
Step 36, loss: 116.91111755371094
Step 37, loss: 116.49369049072266
Step 38, loss: 116.1081771850586
Step 39, loss: 115.71791076660156
Step 40, loss: 115.31915283203125
Step 41, loss: 115.04400634765625
Step 42, loss: 114.77462768554688
Step 43, loss: 114.41227722167969
Step 44, loss: 114.20317077636719
Step 45, loss: 113.9441909790039
Step 46, loss: 113.7032699584961
Step 47, loss: 113.45792388916016
Step 48, loss: 113.32379150390625
Step 49, loss: 113.08817291259766
Step 50, loss: 112.92090606689453
Step 51, loss: 112.76620483398438
Step 52, loss: 112.58533477783203
Step 53, loss: 112.43685913085938
Step 54, loss: 112.31883239746094
Step 55, loss: 112.1436996459961
Step 56, loss: 112.0010757446289
Step 57, loss: 111.8597412109375
Step 58, loss: 111.75509643554688
Step 59, loss: 111.66414642333984
Step 60, loss: 111.50638580322266
Step 61, loss: 111.45252990722656
Step 62, loss: 111.32903289794922
Step 63, loss: 111.2352294921875
Step 64, loss: 111.20957946777344
Step 65, loss: 111.08134460449219
Step 66, loss: 111.01829528808594
Step 67, loss: 110.94678497314453
Step 68, loss: 110.8615951538086
Step 69, loss: 110.76590728759766
Step 70, loss: 110.69739532470703
Step 71, loss: 110.64260864257812
Step 72, loss: 110.61503601074219
Step 73, loss: 110.50509643554688
Step 74, loss: 110.44701385498047
Step 75, loss: 110.40717315673828
Step 76, loss: 110.33840942382812
Step 77, loss: 110.3146743774414
Step 78, loss: 110.22565460205078
Step 79, loss: 110.21774291992188
Step 80, loss: 110.1622085571289
Step 81, loss: 110.13780212402344
Step 82, loss: 110.03749084472656
Step 83, loss: 109.94865417480469
Step 84, loss: 109.95683288574219
Step 85, loss: 109.92646026611328
Step 86, loss: 109.8940200805664
Step 87, loss: 109.82115936279297
Step 88, loss: 109.79154968261719
Step 89, loss: 109.78386688232422
Step 90, loss: 109.74058532714844
Step 91, loss: 109.68739318847656
Step 92, loss: 109.65406036376953
Step 93, loss: 109.6512222290039
Step 94, loss: 109.55363464355469
Step 95, loss: 109.53081512451172
Step 96, loss: 109.506103515625
Step 97, loss: 109.51664733886719
Step 98, loss: 109.49114990234375
Step 99, loss: 109.43669891357422
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 109.39813995361328
Step 1, loss: 114.463623046875
Step 2, loss: 118.47991180419922
Step 3, loss: 121.81295013427734
Step 4, loss: 125.5582275390625
Step 5, loss: 129.08424377441406
Step 6, loss: 131.34451293945312
Step 7, loss: 133.32357788085938
Step 8, loss: 134.86416625976562
Step 9, loss: 136.32769775390625
Step 10, loss: 136.98851013183594
Step 11, loss: 137.05323791503906
Step 12, loss: 136.559326171875
Step 13, loss: 135.18455505371094
Step 14, loss: 133.896728515625
Step 15, loss: 132.65155029296875
Step 16, loss: 131.45230102539062
Step 17, loss: 130.35311889648438
Step 18, loss: 129.5597381591797
Step 19, loss: 128.73023986816406
Step 20, loss: 127.86009216308594
Step 21, loss: 127.08273315429688
Step 22, loss: 126.29617309570312
Step 23, loss: 125.50707244873047
Step 24, loss: 124.6923828125
Step 25, loss: 123.89649200439453
Step 26, loss: 123.22545623779297
Step 27, loss: 122.4256820678711
Step 28, loss: 121.71726989746094
Step 29, loss: 120.99281311035156
Step 30, loss: 120.33429718017578
Step 31, loss: 119.77800750732422
Step 32, loss: 119.09984588623047
Step 33, loss: 118.5423812866211
Step 34, loss: 118.02435302734375
Step 35, loss: 117.54845428466797
Step 36, loss: 117.08365631103516
Step 37, loss: 116.64845275878906
Step 38, loss: 116.21778106689453
Step 39, loss: 115.89295196533203
Step 40, loss: 115.45291137695312
Step 41, loss: 115.17682647705078
Step 42, loss: 114.86884307861328
Step 43, loss: 114.56585693359375
Step 44, loss: 114.27716827392578
Step 45, loss: 114.04810333251953
Step 46, loss: 113.77432250976562
Step 47, loss: 113.56111907958984
Step 48, loss: 113.39611053466797
Step 49, loss: 113.2081527709961
Step 50, loss: 113.00340270996094
Step 51, loss: 112.84097290039062
Step 52, loss: 112.63572692871094
Step 53, loss: 112.53115844726562
Step 54, loss: 112.3496322631836
Step 55, loss: 112.20490264892578
Step 56, loss: 112.1149673461914
Step 57, loss: 112.02013397216797
Step 58, loss: 111.82402038574219
Step 59, loss: 111.74473571777344
Step 60, loss: 111.67913818359375
Step 61, loss: 111.59294891357422
Step 62, loss: 111.47461700439453
Step 63, loss: 111.32167053222656
Step 64, loss: 111.2297592163086
Step 65, loss: 111.17547607421875
Step 66, loss: 111.11680603027344
Step 67, loss: 111.03740692138672
Step 68, loss: 110.96945190429688
Step 69, loss: 110.90755462646484
Step 70, loss: 110.81163024902344
Step 71, loss: 110.73462677001953
Step 72, loss: 110.66417694091797
Step 73, loss: 110.63229370117188
Step 74, loss: 110.55369567871094
Step 75, loss: 110.46326446533203
Step 76, loss: 110.46592712402344
Step 77, loss: 110.38158416748047
Step 78, loss: 110.34490966796875
Step 79, loss: 110.32300567626953
Step 80, loss: 110.20476531982422
Step 81, loss: 110.16508483886719
Step 82, loss: 110.17916870117188
Step 83, loss: 110.10452270507812
Step 84, loss: 110.07378387451172
Step 85, loss: 109.96438598632812
Step 86, loss: 109.97930145263672
Step 87, loss: 109.93667602539062
Step 88, loss: 109.8891372680664
Step 89, loss: 109.82359313964844
Step 90, loss: 109.80046844482422
Step 91, loss: 109.76396942138672
Step 92, loss: 109.74031829833984
Step 93, loss: 109.65027618408203
Step 94, loss: 109.66130065917969
Step 95, loss: 109.5956802368164
Step 96, loss: 109.55325317382812
Step 97, loss: 109.51542663574219
Step 98, loss: 109.52900695800781
Step 99, loss: 109.50298309326172
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 109.45228576660156
Step 1, loss: 114.4468765258789
Step 2, loss: 118.48843383789062
Step 3, loss: 121.86914825439453
Step 4, loss: 125.63925170898438
Step 5, loss: 129.362548828125
Step 6, loss: 131.80905151367188
Step 7, loss: 133.92828369140625
Step 8, loss: 135.56211853027344
Step 9, loss: 137.20982360839844
Step 10, loss: 137.86825561523438
Step 11, loss: 138.005859375
Step 12, loss: 137.48284912109375
Step 13, loss: 136.15264892578125
Step 14, loss: 134.782470703125
Step 15, loss: 133.429443359375
Step 16, loss: 132.30931091308594
Step 17, loss: 131.17164611816406
Step 18, loss: 130.26144409179688
Step 19, loss: 129.39727783203125
Step 20, loss: 128.4926300048828
Step 21, loss: 127.66659545898438
Step 22, loss: 126.79979705810547
Step 23, loss: 126.0110092163086
Step 24, loss: 125.16585540771484
Step 25, loss: 124.37899017333984
Step 26, loss: 123.57640075683594
Step 27, loss: 122.830078125
Step 28, loss: 122.1187973022461
Step 29, loss: 121.39208221435547
Step 30, loss: 120.71800231933594
Step 31, loss: 120.07550048828125
Step 32, loss: 119.49346923828125
Step 33, loss: 118.9000015258789
Step 34, loss: 118.41995239257812
Step 35, loss: 117.86453247070312
Step 36, loss: 117.36863708496094
Step 37, loss: 116.93480682373047
Step 38, loss: 116.50336456298828
Step 39, loss: 116.11514282226562
Step 40, loss: 115.7524185180664
Step 41, loss: 115.4423828125
Step 42, loss: 115.06817626953125
Step 43, loss: 114.83086395263672
Step 44, loss: 114.54112243652344
Step 45, loss: 114.31005096435547
Step 46, loss: 114.06752014160156
Step 47, loss: 113.84822845458984
Step 48, loss: 113.61666107177734
Step 49, loss: 113.47004699707031
Step 50, loss: 113.2107162475586
Step 51, loss: 113.04156494140625
Step 52, loss: 112.86236572265625
Step 53, loss: 112.68706512451172
Step 54, loss: 112.54379272460938
Step 55, loss: 112.39360046386719
Step 56, loss: 112.2987060546875
Step 57, loss: 112.18575286865234
Step 58, loss: 112.00017547607422
Step 59, loss: 111.8791275024414
Step 60, loss: 111.79656219482422
Step 61, loss: 111.71356964111328
Step 62, loss: 111.62648010253906
Step 63, loss: 111.43818664550781
Step 64, loss: 111.39415740966797
Step 65, loss: 111.27169799804688
Step 66, loss: 111.24162292480469
Step 67, loss: 111.17076110839844
Step 68, loss: 111.07737731933594
Step 69, loss: 110.94914245605469
Step 70, loss: 110.88394165039062
Step 71, loss: 110.81558990478516
Step 72, loss: 110.81867980957031
Step 73, loss: 110.71395874023438
Step 74, loss: 110.67221069335938
Step 75, loss: 110.55718994140625
Step 76, loss: 110.5186996459961
Step 77, loss: 110.44561004638672
Step 78, loss: 110.42185974121094
Step 79, loss: 110.34049224853516
Step 80, loss: 110.29305267333984
Step 81, loss: 110.22114562988281
Step 82, loss: 110.20665740966797
Step 83, loss: 110.1834487915039
Step 84, loss: 110.15840911865234
Step 85, loss: 110.05204010009766
Step 86, loss: 110.05614471435547
Step 87, loss: 109.98605346679688
Step 88, loss: 109.92914581298828
Step 89, loss: 109.9264144897461
Step 90, loss: 109.84593200683594
Step 91, loss: 109.82048034667969
Step 92, loss: 109.7948226928711
Step 93, loss: 109.74574279785156
Step 94, loss: 109.7136001586914
Step 95, loss: 109.65711212158203
Step 96, loss: 109.64708709716797
Step 97, loss: 109.61984252929688
Step 98, loss: 109.58368682861328
Step 99, loss: 109.54103088378906
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 109.41191864013672
Step 1, loss: 114.49227905273438
Step 2, loss: 118.50959777832031
Step 3, loss: 121.85176086425781
Step 4, loss: 125.48546600341797
Step 5, loss: 128.75051879882812
Step 6, loss: 130.95321655273438
Step 7, loss: 132.8828125
Step 8, loss: 134.34698486328125
Step 9, loss: 135.7285614013672
Step 10, loss: 136.3206024169922
Step 11, loss: 136.35853576660156
Step 12, loss: 135.77989196777344
Step 13, loss: 134.56924438476562
Step 14, loss: 133.2179412841797
Step 15, loss: 131.96807861328125
Step 16, loss: 130.86016845703125
Step 17, loss: 129.79507446289062
Step 18, loss: 128.99658203125
Step 19, loss: 128.17250061035156
Step 20, loss: 127.33958435058594
Step 21, loss: 126.531982421875
Step 22, loss: 125.72472381591797
Step 23, loss: 124.97627258300781
Step 24, loss: 124.19180297851562
Step 25, loss: 123.36214447021484
Step 26, loss: 122.64615631103516
Step 27, loss: 121.95928955078125
Step 28, loss: 121.31489562988281
Step 29, loss: 120.57228088378906
Step 30, loss: 119.9869613647461
Step 31, loss: 119.31522369384766
Step 32, loss: 118.74784851074219
Step 33, loss: 118.20111846923828
Step 34, loss: 117.72676849365234
Step 35, loss: 117.22423553466797
Step 36, loss: 116.76254272460938
Step 37, loss: 116.38760375976562
Step 38, loss: 115.99264526367188
Step 39, loss: 115.60242462158203
Step 40, loss: 115.29656982421875
Step 41, loss: 114.93008422851562
Step 42, loss: 114.62866973876953
Step 43, loss: 114.36457824707031
Step 44, loss: 114.09069061279297
Step 45, loss: 113.80075073242188
Step 46, loss: 113.62527465820312
Step 47, loss: 113.38634490966797
Step 48, loss: 113.18202209472656
Step 49, loss: 113.02091979980469
Step 50, loss: 112.82302856445312
Step 51, loss: 112.63050842285156
Step 52, loss: 112.52745819091797
Step 53, loss: 112.31270599365234
Step 54, loss: 112.20240783691406
Step 55, loss: 112.0804672241211
Step 56, loss: 111.89606475830078
Step 57, loss: 111.81903839111328
Step 58, loss: 111.66834259033203
Step 59, loss: 111.58431243896484
Step 60, loss: 111.4734878540039
Step 61, loss: 111.37098693847656
Step 62, loss: 111.29484558105469
Step 63, loss: 111.16661071777344
Step 64, loss: 111.09810638427734
Step 65, loss: 110.98291778564453
Step 66, loss: 110.95343017578125
Step 67, loss: 110.86563873291016
Step 68, loss: 110.74251556396484
Step 69, loss: 110.70301055908203
Step 70, loss: 110.65142822265625
Step 71, loss: 110.5838851928711
Step 72, loss: 110.50096893310547
Step 73, loss: 110.43860626220703
Step 74, loss: 110.41950988769531
Step 75, loss: 110.33989715576172
Step 76, loss: 110.28963470458984
Step 77, loss: 110.23954772949219
Step 78, loss: 110.16887664794922
Step 79, loss: 110.16447448730469
Step 80, loss: 110.08650207519531
Step 81, loss: 110.04486083984375
Step 82, loss: 109.97981262207031
Step 83, loss: 109.94426727294922
Step 84, loss: 109.90399932861328
Step 85, loss: 109.81529235839844
Step 86, loss: 109.81922149658203
Step 87, loss: 109.78530883789062
Step 88, loss: 109.76744842529297
Step 89, loss: 109.70104217529297
Step 90, loss: 109.68631744384766
Step 91, loss: 109.61986541748047
Step 92, loss: 109.60255432128906
Step 93, loss: 109.54911804199219
Step 94, loss: 109.5137939453125
Step 95, loss: 109.51758575439453
Step 96, loss: 109.51336669921875
Step 97, loss: 109.46080017089844
Step 98, loss: 109.41108703613281
Step 99, loss: 109.37374877929688
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 109.4761734008789
Step 1, loss: 114.54754638671875
Step 2, loss: 118.50419616699219
Step 3, loss: 121.75950622558594
Step 4, loss: 125.29334259033203
Step 5, loss: 128.45655822753906
Step 6, loss: 130.7967987060547
Step 7, loss: 132.72132873535156
Step 8, loss: 134.13827514648438
Step 9, loss: 135.50242614746094
Step 10, loss: 136.1060028076172
Step 11, loss: 136.18809509277344
Step 12, loss: 135.5605926513672
Step 13, loss: 134.36549377441406
Step 14, loss: 133.0284423828125
Step 15, loss: 131.71707153320312
Step 16, loss: 130.6240997314453
Step 17, loss: 129.58753967285156
Step 18, loss: 128.80604553222656
Step 19, loss: 127.95305633544922
Step 20, loss: 127.1883316040039
Step 21, loss: 126.35541534423828
Step 22, loss: 125.69022369384766
Step 23, loss: 124.87855529785156
Step 24, loss: 124.08071899414062
Step 25, loss: 123.34622955322266
Step 26, loss: 122.605224609375
Step 27, loss: 121.91725158691406
Step 28, loss: 121.22410583496094
Step 29, loss: 120.59069061279297
Step 30, loss: 119.90189361572266
Step 31, loss: 119.32138061523438
Step 32, loss: 118.76163482666016
Step 33, loss: 118.17951965332031
Step 34, loss: 117.65186309814453
Step 35, loss: 117.19501495361328
Step 36, loss: 116.71836853027344
Step 37, loss: 116.27826690673828
Step 38, loss: 115.87701416015625
Step 39, loss: 115.4701919555664
Step 40, loss: 115.08686065673828
Step 41, loss: 114.7790298461914
Step 42, loss: 114.4711685180664
Step 43, loss: 114.18123626708984
Step 44, loss: 113.95165252685547
Step 45, loss: 113.68277740478516
Step 46, loss: 113.40681457519531
Step 47, loss: 113.21288299560547
Step 48, loss: 113.00247192382812
Step 49, loss: 112.81776428222656
Step 50, loss: 112.65620422363281
Step 51, loss: 112.45352935791016
Step 52, loss: 112.34329223632812
Step 53, loss: 112.1839599609375
Step 54, loss: 112.036865234375
Step 55, loss: 111.92401123046875
Step 56, loss: 111.72283935546875
Step 57, loss: 111.65975952148438
Step 58, loss: 111.55128479003906
Step 59, loss: 111.42875671386719
Step 60, loss: 111.29849243164062
Step 61, loss: 111.17353057861328
Step 62, loss: 111.112548828125
Step 63, loss: 111.0086669921875
Step 64, loss: 110.9080581665039
Step 65, loss: 110.84327697753906
Step 66, loss: 110.76201629638672
Step 67, loss: 110.71282958984375
Step 68, loss: 110.55989074707031
Step 69, loss: 110.55206298828125
Step 70, loss: 110.4409408569336
Step 71, loss: 110.41612243652344
Step 72, loss: 110.3271484375
Step 73, loss: 110.2116470336914
Step 74, loss: 110.21574401855469
Step 75, loss: 110.16683197021484
Step 76, loss: 110.07759094238281
Step 77, loss: 110.04779052734375
Step 78, loss: 110.0002212524414
Step 79, loss: 109.950439453125
Step 80, loss: 109.8492202758789
Step 81, loss: 109.85557556152344
Step 82, loss: 109.8371810913086
Step 83, loss: 109.71416473388672
Step 84, loss: 109.7540054321289
Step 85, loss: 109.65533447265625
Step 86, loss: 109.67938232421875
Step 87, loss: 109.59033966064453
Step 88, loss: 109.53863525390625
Step 89, loss: 109.55345153808594
Step 90, loss: 109.47240447998047
Step 91, loss: 109.44798278808594
Step 92, loss: 109.44332885742188
Step 93, loss: 109.3758773803711
Step 94, loss: 109.3710708618164
Step 95, loss: 109.25568389892578
Step 96, loss: 109.29498291015625
Step 97, loss: 109.23114776611328
Step 98, loss: 109.2176513671875
Step 99, loss: 109.22627258300781
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[-0.45601626,  0.27245611, -0.84724072, -0.04145563],
       [ 0.88942142,  0.1729828 , -0.42309158,  0.70507624],
       [ 0.03128419, -0.94649069, -0.32121127,  0.15022215],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.92826254,  0.34736276,  0.13292016,  0.01299759],
       [-0.09559309,  0.56820493, -0.8173158 ,  0.288896  ],
       [-0.35943096,  0.7459774 ,  0.56064883,  0.36026766],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 0.76852175, -0.3675634 ,  0.52370933, -0.20481271],
       [-0.6353653 , -0.34196161,  0.69236781, -0.15994387],
       [-0.07540058, -0.86484646, -0.49634196,  0.52690319],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[-0.45601626,  0.27245611, -0.84724072, -0.04145563],
       [ 0.88942142,  0.1729828 , -0.42309158,  0.70507624],
       [ 0.03128419, -0.94649069, -0.32121127,  0.15022215],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[ 0.55265367,  0.31166121,  0.7729432 , -0.36156924],
       [ 0.05796198, -0.93957371,  0.33740637,  1.24053583],
       [ 0.83139324, -0.14166807, -0.53732252,  0.00928377],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False

loading data:   0%|          | 0/4 [00:00<?, ?it/s]
loading data: 100%|██████████| 4/4 [00:00<00:00, 54.59it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/67 [00:00<?, ?it/s]
loading data:  10%|█         | 7/67 [00:00<00:00, 60.06it/s]
loading data:  21%|██        | 14/67 [00:00<00:00, 61.99it/s]
loading data:  31%|███▏      | 21/67 [00:00<00:00, 62.69it/s]
loading data:  42%|████▏     | 28/67 [00:00<00:00, 63.23it/s]
loading data:  52%|█████▏    | 35/67 [00:00<00:00, 62.92it/s]
loading data:  63%|██████▎   | 42/67 [00:00<00:00, 63.48it/s]
loading data:  73%|███████▎  | 49/67 [00:00<00:00, 63.82it/s]
loading data:  84%|████████▎ | 56/67 [00:00<00:00, 63.69it/s]
loading data:  94%|█████████▍| 63/67 [00:01<00:00, 62.39it/s]
loading data: 100%|██████████| 67/67 [00:01<00:00, 63.05it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(p, device='cuda', dtype=torch.float32)
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Loss at initial point 0.7127008885145187
Loss after optimization 0.20711390674114227
logm result may be inaccurate, approximate err = 9.830155899300884e-07
Loss at initial point 0.7440736591815948
Loss after optimization 0.15991158783435822
logm result may be inaccurate, approximate err = 9.086717198399978e-07
Loss at initial point 0.8347567170858383
Loss after optimization 0.3791152536869049
logm result may be inaccurate, approximate err = 8.038638962929925e-07
Loss at initial point 0.7905804365873337
Loss after optimization 0.22133073955774307
logm result may be inaccurate, approximate err = 7.979940986957774e-07
Loss at initial point 0.6791139841079712
Loss after optimization 0.6037407666444778
logm result may be inaccurate, approximate err = 1.2044193590072962e-06
Loss at initial point 0.7356295362114906
Loss after optimization 0.14453709311783314
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Loss at initial point 0.7125649601221085
Loss after optimization 0.59969063103199
logm result may be inaccurate, approximate err = 9.830155899300884e-07
Loss at initial point 0.7439482510089874
Loss after optimization 0.39617301523685455
logm result may be inaccurate, approximate err = 9.086717198399978e-07
Loss at initial point 0.8346579670906067
Loss after optimization 0.6470158398151398
logm result may be inaccurate, approximate err = 8.038638962929925e-07
Loss at initial point 0.7907940149307251
Loss after optimization 0.41750695556402206
logm result may be inaccurate, approximate err = 7.979940986957774e-07
Loss at initial point 0.6790904998779297
Loss after optimization 0.12093100883066654
logm result may be inaccurate, approximate err = 1.2044193590072962e-06
Loss at initial point 0.7356246560811996
Loss after optimization 0.18377255648374557
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Loss at initial point 0.7125780284404755
Loss after optimization 0.1188542302697897
logm result may be inaccurate, approximate err = 9.830155899300884e-07
Loss at initial point 0.7439384907484055
Loss after optimization 0.6470157951116562
logm result may be inaccurate, approximate err = 9.086717198399978e-07
Loss at initial point 0.8346070498228073
Loss after optimization 0.6151126176118851
logm result may be inaccurate, approximate err = 8.038638962929925e-07
Loss at initial point 0.7906165271997452
Loss after optimization 0.6154579669237137
logm result may be inaccurate, approximate err = 7.979940986957774e-07
Loss at initial point 0.6789561361074448
Loss after optimization 0.1186894029378891
logm result may be inaccurate, approximate err = 1.2044193590072962e-06
Loss at initial point 0.7356675267219543
Loss after optimization 0.1183971669524908
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 476.71270751953125
Step 1, loss: 500.5183410644531
Step 2, loss: 519.359619140625
Step 3, loss: 534.6463623046875
Step 4, loss: 549.4659423828125
Step 5, loss: 563.1693115234375
Step 6, loss: 575.9703979492188
Step 7, loss: 587.839111328125
Step 8, loss: 599.0100708007812
Step 9, loss: 607.6640625
Step 10, loss: 613.21337890625
Step 11, loss: 616.3319091796875
Step 12, loss: 619.0286865234375
Step 13, loss: 617.6707153320312
Step 14, loss: 613.896728515625
Step 15, loss: 610.398681640625
Step 16, loss: 605.8970947265625
Step 17, loss: 600.0166015625
Step 18, loss: 591.9721069335938
Step 19, loss: 583.0836791992188
Step 20, loss: 575.7066040039062
Step 21, loss: 569.5308227539062
Step 22, loss: 564.46240234375
Step 23, loss: 560.3079223632812
Step 24, loss: 556.0003662109375
Step 25, loss: 552.006103515625
Step 26, loss: 548.592529296875
Step 27, loss: 544.7032470703125
Step 28, loss: 541.2060546875
Step 29, loss: 537.8487548828125
Step 30, loss: 534.2369384765625
Step 31, loss: 530.7797241210938
Step 32, loss: 527.104736328125
Step 33, loss: 523.30615234375
Step 34, loss: 519.3952026367188
Step 35, loss: 515.3055419921875
Step 36, loss: 511.4098815917969
Step 37, loss: 507.94805908203125
Step 38, loss: 504.71044921875
Step 39, loss: 501.35601806640625
Step 40, loss: 497.97357177734375
Step 41, loss: 495.4772033691406
Step 42, loss: 494.0798034667969
Step 43, loss: 492.8299255371094
Step 44, loss: 491.67120361328125
Step 45, loss: 490.6434631347656
Step 46, loss: 489.6086730957031
Step 47, loss: 488.5733642578125
Step 48, loss: 487.5816650390625
Step 49, loss: 486.65087890625
Step 50, loss: 485.709228515625
Step 51, loss: 484.84344482421875
Step 52, loss: 483.98699951171875
Step 53, loss: 483.17279052734375
Step 54, loss: 482.5108642578125
Step 55, loss: 481.7696838378906
Step 56, loss: 481.15478515625
Step 57, loss: 480.4830322265625
Step 58, loss: 479.9083557128906
Step 59, loss: 479.40008544921875
Step 60, loss: 478.9493713378906
Step 61, loss: 478.58697509765625
Step 62, loss: 478.18353271484375
Step 63, loss: 477.8888854980469
Step 64, loss: 477.5974426269531
Step 65, loss: 477.3737487792969
Step 66, loss: 477.1043701171875
Step 67, loss: 476.86370849609375
Step 68, loss: 476.62872314453125
Step 69, loss: 476.33740234375
Step 70, loss: 476.13470458984375
Step 71, loss: 475.87152099609375
Step 72, loss: 475.68658447265625
Step 73, loss: 475.48394775390625
Step 74, loss: 475.27801513671875
Step 75, loss: 475.11676025390625
Step 76, loss: 474.937744140625
Step 77, loss: 474.7940368652344
Step 78, loss: 474.655517578125
Step 79, loss: 474.468017578125
Step 80, loss: 474.3267822265625
Step 81, loss: 474.16107177734375
Step 82, loss: 474.0743408203125
Step 83, loss: 473.9788513183594
Step 84, loss: 473.910400390625
Step 85, loss: 473.82244873046875
Step 86, loss: 473.7583923339844
Step 87, loss: 473.729248046875
Step 88, loss: 473.71533203125
Step 89, loss: 473.7073974609375
Step 90, loss: 473.66290283203125
Step 91, loss: 473.66357421875
Step 92, loss: 473.64471435546875
Step 93, loss: 473.63397216796875
Step 94, loss: 473.6077880859375
Step 95, loss: 473.6156311035156
Step 96, loss: 473.56573486328125
Step 97, loss: 473.5697021484375
Step 98, loss: 473.54937744140625
Step 99, loss: 473.53424072265625
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 476.66827392578125
Step 1, loss: 500.51495361328125
Step 2, loss: 519.3653564453125
Step 3, loss: 534.6283569335938
Step 4, loss: 549.5116577148438
Step 5, loss: 563.2402954101562
Step 6, loss: 575.91357421875
Step 7, loss: 587.6634521484375
Step 8, loss: 598.7659912109375
Step 9, loss: 607.330078125
Step 10, loss: 613.1032104492188
Step 11, loss: 616.2249755859375
Step 12, loss: 619.069580078125
Step 13, loss: 617.7141723632812
Step 14, loss: 613.7573852539062
Step 15, loss: 610.0313110351562
Step 16, loss: 605.4957885742188
Step 17, loss: 599.3514404296875
Step 18, loss: 590.9480590820312
Step 19, loss: 582.1630249023438
Step 20, loss: 574.9080200195312
Step 21, loss: 568.7114868164062
Step 22, loss: 563.9207763671875
Step 23, loss: 559.5784301757812
Step 24, loss: 555.205322265625
Step 25, loss: 551.2191162109375
Step 26, loss: 547.5988159179688
Step 27, loss: 543.850341796875
Step 28, loss: 540.1907348632812
Step 29, loss: 536.9190063476562
Step 30, loss: 533.2855224609375
Step 31, loss: 529.802734375
Step 32, loss: 526.2300415039062
Step 33, loss: 522.3982543945312
Step 34, loss: 518.459228515625
Step 35, loss: 514.5125122070312
Step 36, loss: 510.7110595703125
Step 37, loss: 507.35528564453125
Step 38, loss: 504.161376953125
Step 39, loss: 500.68072509765625
Step 40, loss: 497.3840637207031
Step 41, loss: 495.17059326171875
Step 42, loss: 493.7558288574219
Step 43, loss: 492.5574035644531
Step 44, loss: 491.5068664550781
Step 45, loss: 490.36859130859375
Step 46, loss: 489.32598876953125
Step 47, loss: 488.2886962890625
Step 48, loss: 487.3763122558594
Step 49, loss: 486.3595275878906
Step 50, loss: 485.51287841796875
Step 51, loss: 484.62689208984375
Step 52, loss: 483.71405029296875
Step 53, loss: 482.9912109375
Step 54, loss: 482.3265380859375
Step 55, loss: 481.5921325683594
Step 56, loss: 480.9483642578125
Step 57, loss: 480.29266357421875
Step 58, loss: 479.7393798828125
Step 59, loss: 479.25775146484375
Step 60, loss: 478.81182861328125
Step 61, loss: 478.42364501953125
Step 62, loss: 478.10858154296875
Step 63, loss: 477.7657775878906
Step 64, loss: 477.54364013671875
Step 65, loss: 477.2805480957031
Step 66, loss: 477.0667724609375
Step 67, loss: 476.8315124511719
Step 68, loss: 476.5997619628906
Step 69, loss: 476.3727722167969
Step 70, loss: 476.118408203125
Step 71, loss: 475.90777587890625
Step 72, loss: 475.69720458984375
Step 73, loss: 475.4750671386719
Step 74, loss: 475.2560729980469
Step 75, loss: 475.09027099609375
Step 76, loss: 474.9342041015625
Step 77, loss: 474.8047790527344
Step 78, loss: 474.630615234375
Step 79, loss: 474.5159912109375
Step 80, loss: 474.31268310546875
Step 81, loss: 474.2019348144531
Step 82, loss: 474.06451416015625
Step 83, loss: 474.01092529296875
Step 84, loss: 473.91644287109375
Step 85, loss: 473.81512451171875
Step 86, loss: 473.76165771484375
Step 87, loss: 473.7063293457031
Step 88, loss: 473.6932373046875
Step 89, loss: 473.67779541015625
Step 90, loss: 473.6741638183594
Step 91, loss: 473.62872314453125
Step 92, loss: 473.61785888671875
Step 93, loss: 473.5848083496094
Step 94, loss: 473.5839538574219
Step 95, loss: 473.5477294921875
Step 96, loss: 473.52374267578125
Step 97, loss: 473.5556640625
Step 98, loss: 473.52001953125
Step 99, loss: 473.51641845703125
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 476.7843322753906
Step 1, loss: 500.49212646484375
Step 2, loss: 519.4995727539062
Step 3, loss: 534.688720703125
Step 4, loss: 549.5224609375
Step 5, loss: 563.1092529296875
Step 6, loss: 575.615966796875
Step 7, loss: 587.1942138671875
Step 8, loss: 598.0140380859375
Step 9, loss: 606.43603515625
Step 10, loss: 612.4688110351562
Step 11, loss: 616.0045776367188
Step 12, loss: 618.9417724609375
Step 13, loss: 617.78076171875
Step 14, loss: 613.79638671875
Step 15, loss: 610.09912109375
Step 16, loss: 605.43115234375
Step 17, loss: 599.7099609375
Step 18, loss: 591.2132568359375
Step 19, loss: 582.509521484375
Step 20, loss: 575.1489868164062
Step 21, loss: 569.0384521484375
Step 22, loss: 564.1712646484375
Step 23, loss: 559.7919921875
Step 24, loss: 555.5684814453125
Step 25, loss: 551.6314697265625
Step 26, loss: 548.1204833984375
Step 27, loss: 544.31787109375
Step 28, loss: 540.8079833984375
Step 29, loss: 537.2720947265625
Step 30, loss: 533.9219360351562
Step 31, loss: 530.2476196289062
Step 32, loss: 526.6419067382812
Step 33, loss: 522.877685546875
Step 34, loss: 519.0332641601562
Step 35, loss: 514.95556640625
Step 36, loss: 511.22869873046875
Step 37, loss: 507.7492370605469
Step 38, loss: 504.5208740234375
Step 39, loss: 500.8747863769531
Step 40, loss: 497.48455810546875
Step 41, loss: 495.1717529296875
Step 42, loss: 493.756103515625
Step 43, loss: 492.447998046875
Step 44, loss: 491.3746337890625
Step 45, loss: 490.26446533203125
Step 46, loss: 489.12469482421875
Step 47, loss: 488.1316223144531
Step 48, loss: 487.07330322265625
Step 49, loss: 486.12664794921875
Step 50, loss: 485.16619873046875
Step 51, loss: 484.330322265625
Step 52, loss: 483.50933837890625
Step 53, loss: 482.76220703125
Step 54, loss: 482.0394287109375
Step 55, loss: 481.36187744140625
Step 56, loss: 480.66912841796875
Step 57, loss: 480.05682373046875
Step 58, loss: 479.4886474609375
Step 59, loss: 479.013916015625
Step 60, loss: 478.59130859375
Step 61, loss: 478.26153564453125
Step 62, loss: 477.9270935058594
Step 63, loss: 477.65625
Step 64, loss: 477.35064697265625
Step 65, loss: 477.0921630859375
Step 66, loss: 476.77777099609375
Step 67, loss: 476.558349609375
Step 68, loss: 476.27923583984375
Step 69, loss: 476.06829833984375
Step 70, loss: 475.80535888671875
Step 71, loss: 475.57305908203125
Step 72, loss: 475.34808349609375
Step 73, loss: 475.1687927246094
Step 74, loss: 474.9715881347656
Step 75, loss: 474.800048828125
Step 76, loss: 474.60736083984375
Step 77, loss: 474.395751953125
Step 78, loss: 474.28558349609375
Step 79, loss: 474.15850830078125
Step 80, loss: 474.06646728515625
Step 81, loss: 473.9804992675781
Step 82, loss: 473.889404296875
Step 83, loss: 473.82305908203125
Step 84, loss: 473.8008117675781
Step 85, loss: 473.7798156738281
Step 86, loss: 473.7777099609375
Step 87, loss: 473.7626953125
Step 88, loss: 473.738525390625
Step 89, loss: 473.7481689453125
Step 90, loss: 473.7373046875
Step 91, loss: 473.698974609375
Step 92, loss: 473.7069091796875
Step 93, loss: 473.6615905761719
Step 94, loss: 473.659423828125
Step 95, loss: 473.62384033203125
Step 96, loss: 473.59271240234375
Step 97, loss: 473.5791320800781
Step 98, loss: 473.56707763671875
Step 99, loss: 473.54278564453125
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 476.79315185546875
Step 1, loss: 500.48663330078125
Step 2, loss: 519.2694091796875
Step 3, loss: 534.6368408203125
Step 4, loss: 549.6161499023438
Step 5, loss: 563.1886596679688
Step 6, loss: 575.830322265625
Step 7, loss: 587.282958984375
Step 8, loss: 598.595703125
Step 9, loss: 606.9464111328125
Step 10, loss: 612.7821044921875
Step 11, loss: 616.0541381835938
Step 12, loss: 619.0560302734375
Step 13, loss: 617.7913818359375
Step 14, loss: 613.8291625976562
Step 15, loss: 610.0402221679688
Step 16, loss: 605.8348388671875
Step 17, loss: 599.9257202148438
Step 18, loss: 591.7548217773438
Step 19, loss: 582.9219970703125
Step 20, loss: 575.71337890625
Step 21, loss: 569.5004272460938
Step 22, loss: 564.49267578125
Step 23, loss: 560.5177612304688
Step 24, loss: 556.2620849609375
Step 25, loss: 552.21923828125
Step 26, loss: 548.862548828125
Step 27, loss: 545.1089477539062
Step 28, loss: 541.638916015625
Step 29, loss: 538.3202514648438
Step 30, loss: 534.7275390625
Step 31, loss: 531.1246337890625
Step 32, loss: 527.55712890625
Step 33, loss: 523.6624755859375
Step 34, loss: 519.7518310546875
Step 35, loss: 515.6309814453125
Step 36, loss: 511.764404296875
Step 37, loss: 508.1015930175781
Step 38, loss: 504.8692932128906
Step 39, loss: 501.44158935546875
Step 40, loss: 498.01031494140625
Step 41, loss: 495.3478088378906
Step 42, loss: 494.001708984375
Step 43, loss: 492.656982421875
Step 44, loss: 491.60186767578125
Step 45, loss: 490.5675048828125
Step 46, loss: 489.4500427246094
Step 47, loss: 488.39031982421875
Step 48, loss: 487.4493103027344
Step 49, loss: 486.50531005859375
Step 50, loss: 485.6478271484375
Step 51, loss: 484.6715087890625
Step 52, loss: 483.87078857421875
Step 53, loss: 483.12030029296875
Step 54, loss: 482.4013671875
Step 55, loss: 481.72650146484375
Step 56, loss: 481.0514221191406
Step 57, loss: 480.46881103515625
Step 58, loss: 479.8477783203125
Step 59, loss: 479.3216552734375
Step 60, loss: 478.91650390625
Step 61, loss: 478.49951171875
Step 62, loss: 478.1534423828125
Step 63, loss: 477.847412109375
Step 64, loss: 477.58624267578125
Step 65, loss: 477.306640625
Step 66, loss: 477.085205078125
Step 67, loss: 476.827392578125
Step 68, loss: 476.5948181152344
Step 69, loss: 476.41925048828125
Step 70, loss: 476.1510925292969
Step 71, loss: 475.92095947265625
Step 72, loss: 475.74163818359375
Step 73, loss: 475.5294494628906
Step 74, loss: 475.322265625
Step 75, loss: 475.1461486816406
Step 76, loss: 474.9559326171875
Step 77, loss: 474.8096923828125
Step 78, loss: 474.6693115234375
Step 79, loss: 474.5314025878906
Step 80, loss: 474.400634765625
Step 81, loss: 474.2379455566406
Step 82, loss: 474.069580078125
Step 83, loss: 473.97283935546875
Step 84, loss: 473.9227294921875
Step 85, loss: 473.85565185546875
Step 86, loss: 473.75799560546875
Step 87, loss: 473.71697998046875
Step 88, loss: 473.68060302734375
Step 89, loss: 473.66070556640625
Step 90, loss: 473.62103271484375
Step 91, loss: 473.64617919921875
Step 92, loss: 473.64801025390625
Step 93, loss: 473.5976867675781
Step 94, loss: 473.5885925292969
Step 95, loss: 473.5567626953125
Step 96, loss: 473.54107666015625
Step 97, loss: 473.5114440917969
Step 98, loss: 473.51068115234375
Step 99, loss: 473.512451171875
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 476.7495422363281
Step 1, loss: 500.4479675292969
Step 2, loss: 519.4359130859375
Step 3, loss: 534.630126953125
Step 4, loss: 549.56201171875
Step 5, loss: 563.2549438476562
Step 6, loss: 575.9210205078125
Step 7, loss: 587.7501220703125
Step 8, loss: 599.0191040039062
Step 9, loss: 607.6925048828125
Step 10, loss: 613.6311645507812
Step 11, loss: 616.841064453125
Step 12, loss: 619.3375244140625
Step 13, loss: 617.5509033203125
Step 14, loss: 613.5493774414062
Step 15, loss: 609.822265625
Step 16, loss: 605.425048828125
Step 17, loss: 599.698974609375
Step 18, loss: 591.637451171875
Step 19, loss: 582.9346923828125
Step 20, loss: 575.42138671875
Step 21, loss: 569.3106079101562
Step 22, loss: 564.3812255859375
Step 23, loss: 559.9278564453125
Step 24, loss: 555.5869140625
Step 25, loss: 551.6358642578125
Step 26, loss: 548.0440673828125
Step 27, loss: 544.2489624023438
Step 28, loss: 540.603271484375
Step 29, loss: 537.1394653320312
Step 30, loss: 533.6171875
Step 31, loss: 530.0106201171875
Step 32, loss: 526.3072509765625
Step 33, loss: 522.4375
Step 34, loss: 518.5482177734375
Step 35, loss: 514.5243530273438
Step 36, loss: 510.8164367675781
Step 37, loss: 507.4750061035156
Step 38, loss: 504.13775634765625
Step 39, loss: 500.51654052734375
Step 40, loss: 497.211181640625
Step 41, loss: 495.0564880371094
Step 42, loss: 493.72088623046875
Step 43, loss: 492.4528503417969
Step 44, loss: 491.3951416015625
Step 45, loss: 490.2982177734375
Step 46, loss: 489.16387939453125
Step 47, loss: 488.19061279296875
Step 48, loss: 487.18902587890625
Step 49, loss: 486.17486572265625
Step 50, loss: 485.2148742675781
Step 51, loss: 484.3770751953125
Step 52, loss: 483.4591064453125
Step 53, loss: 482.75653076171875
Step 54, loss: 482.0576171875
Step 55, loss: 481.30670166015625
Step 56, loss: 480.6641845703125
Step 57, loss: 480.0501403808594
Step 58, loss: 479.4655456542969
Step 59, loss: 478.9970703125
Step 60, loss: 478.57940673828125
Step 61, loss: 478.189453125
Step 62, loss: 477.8984375
Step 63, loss: 477.59686279296875
Step 64, loss: 477.3477783203125
Step 65, loss: 477.0745544433594
Step 66, loss: 476.83355712890625
Step 67, loss: 476.5400390625
Step 68, loss: 476.2841491699219
Step 69, loss: 476.0576477050781
Step 70, loss: 475.76806640625
Step 71, loss: 475.56048583984375
Step 72, loss: 475.31414794921875
Step 73, loss: 475.11785888671875
Step 74, loss: 474.96368408203125
Step 75, loss: 474.7780456542969
Step 76, loss: 474.5892639160156
Step 77, loss: 474.44793701171875
Step 78, loss: 474.2698974609375
Step 79, loss: 474.1528625488281
Step 80, loss: 473.9977722167969
Step 81, loss: 473.94061279296875
Step 82, loss: 473.8564453125
Step 83, loss: 473.81805419921875
Step 84, loss: 473.7689208984375
Step 85, loss: 473.76263427734375
Step 86, loss: 473.75189208984375
Step 87, loss: 473.7621765136719
Step 88, loss: 473.6929931640625
Step 89, loss: 473.6624755859375
Step 90, loss: 473.65423583984375
Step 91, loss: 473.6493835449219
Step 92, loss: 473.6166687011719
Step 93, loss: 473.6131591796875
Step 94, loss: 473.6158447265625
Step 95, loss: 473.59283447265625
Step 96, loss: 473.56005859375
Step 97, loss: 473.55938720703125
Step 98, loss: 473.5404357910156
Step 99, loss: 473.54791259765625
logm result may be inaccurate, approximate err = 9.160323819369986e-07
Step 0, loss: 476.6531982421875
Step 1, loss: 500.5801086425781
Step 2, loss: 519.408203125
Step 3, loss: 534.6785278320312
Step 4, loss: 549.6483154296875
Step 5, loss: 563.261962890625
Step 6, loss: 575.8211669921875
Step 7, loss: 587.4994506835938
Step 8, loss: 598.3555908203125
Step 9, loss: 606.6209716796875
Step 10, loss: 612.4356689453125
Step 11, loss: 615.6935424804688
Step 12, loss: 618.5352783203125
Step 13, loss: 617.9617919921875
Step 14, loss: 614.0341186523438
Step 15, loss: 610.2687377929688
Step 16, loss: 605.7454833984375
Step 17, loss: 599.84912109375
Step 18, loss: 591.622314453125
Step 19, loss: 582.779541015625
Step 20, loss: 575.45458984375
Step 21, loss: 569.1123046875
Step 22, loss: 564.226318359375
Step 23, loss: 560.0260009765625
Step 24, loss: 555.4390258789062
Step 25, loss: 551.4298095703125
Step 26, loss: 548.0469970703125
Step 27, loss: 544.0228271484375
Step 28, loss: 540.46923828125
Step 29, loss: 537.22265625
Step 30, loss: 533.5533447265625
Step 31, loss: 530.0479736328125
Step 32, loss: 526.3927612304688
Step 33, loss: 522.5056762695312
Step 34, loss: 518.5414428710938
Step 35, loss: 514.4786376953125
Step 36, loss: 510.65618896484375
Step 37, loss: 507.34912109375
Step 38, loss: 504.12249755859375
Step 39, loss: 500.52862548828125
Step 40, loss: 497.291015625
Step 41, loss: 495.17169189453125
Step 42, loss: 493.7949523925781
Step 43, loss: 492.6119384765625
Step 44, loss: 491.483642578125
Step 45, loss: 490.39715576171875
Step 46, loss: 489.3506774902344
Step 47, loss: 488.3165283203125
Step 48, loss: 487.38787841796875
Step 49, loss: 486.4444580078125
Step 50, loss: 485.4699401855469
Step 51, loss: 484.58544921875
Step 52, loss: 483.7489013671875
Step 53, loss: 482.9653015136719
Step 54, loss: 482.3093566894531
Step 55, loss: 481.6427001953125
Step 56, loss: 481.00927734375
Step 57, loss: 480.3731384277344
Step 58, loss: 479.7464294433594
Step 59, loss: 479.28131103515625
Step 60, loss: 478.80731201171875
Step 61, loss: 478.45074462890625
Step 62, loss: 478.1470947265625
Step 63, loss: 477.834716796875
Step 64, loss: 477.5267333984375
Step 65, loss: 477.30242919921875
Step 66, loss: 477.08453369140625
Step 67, loss: 476.78814697265625
Step 68, loss: 476.5483093261719
Step 69, loss: 476.2288513183594
Step 70, loss: 476.0218505859375
Step 71, loss: 475.79840087890625
Step 72, loss: 475.63531494140625
Step 73, loss: 475.3883056640625
Step 74, loss: 475.2532958984375
Step 75, loss: 475.0517578125
Step 76, loss: 474.8570556640625
Step 77, loss: 474.6823425292969
Step 78, loss: 474.5039367675781
Step 79, loss: 474.3538818359375
Step 80, loss: 474.221923828125
Step 81, loss: 474.1234130859375
Step 82, loss: 474.0013427734375
Step 83, loss: 473.96075439453125
Step 84, loss: 473.8287353515625
Step 85, loss: 473.8018798828125
Step 86, loss: 473.7756042480469
Step 87, loss: 473.801025390625
Step 88, loss: 473.76654052734375
Step 89, loss: 473.7410888671875
Step 90, loss: 473.7088623046875
Step 91, loss: 473.71807861328125
Step 92, loss: 473.71136474609375
Step 93, loss: 473.68603515625
Step 94, loss: 473.6488037109375
Step 95, loss: 473.62127685546875
Step 96, loss: 473.6236572265625
Step 97, loss: 473.60443115234375
Step 98, loss: 473.5574951171875
Step 99, loss: 473.56243896484375
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 0.53641294,  0.04322484,  0.84284801,  0.23164379],
       [-0.83596269, -0.10990907,  0.53766753,  0.76628266],
       [ 0.11587724, -0.99300131, -0.02282229,  0.06789392],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.53775012, -0.00350973,  0.84309697,  0.22871523],
       [-0.83971372, -0.09182152,  0.53520994,  0.76541598],
       [ 0.07553601, -0.9957693 , -0.0523242 ,  0.06115146],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 0.54482693, -0.01278715,  0.83845102,  0.04729437],
       [-0.83592811, -0.08726726,  0.54185663,  0.75986435],
       [ 0.06624052, -0.99610286, -0.05823469,  0.16246331],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[ 0.54482693, -0.01278715,  0.83845102,  0.04729437],
       [-0.83592811, -0.08726726,  0.54185663,  0.75986435],
       [ 0.06624052, -0.99610286, -0.05823469,  0.16246331],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[-0.37099099,  0.31505424,  0.87355995, -0.12511655],
       [-0.5057987 ,  0.7203576 , -0.47460759,  0.54353721],
       [-0.77880239, -0.61792099, -0.10789169,  0.50689956],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True

loading data:   0%|          | 0/5 [00:00<?, ?it/s]
loading data: 100%|██████████| 5/5 [00:00<00:00, 49.76it/s]
loading data: 100%|██████████| 5/5 [00:00<00:00, 49.62it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/70 [00:00<?, ?it/s]
loading data:   9%|▊         | 6/70 [00:00<00:01, 55.86it/s]
loading data:  17%|█▋        | 12/70 [00:00<00:01, 56.77it/s]
loading data:  26%|██▌       | 18/70 [00:00<00:00, 58.06it/s]
loading data:  34%|███▍      | 24/70 [00:00<00:00, 55.74it/s]
loading data:  43%|████▎     | 30/70 [00:00<00:00, 56.79it/s]
loading data:  51%|█████▏    | 36/70 [00:00<00:00, 56.04it/s]
loading data:  60%|██████    | 42/70 [00:00<00:00, 56.46it/s]
loading data:  69%|██████▊   | 48/70 [00:00<00:00, 57.04it/s]
loading data:  77%|███████▋  | 54/70 [00:00<00:00, 55.77it/s]
loading data:  86%|████████▌ | 60/70 [00:01<00:00, 56.51it/s]
loading data:  94%|█████████▍| 66/70 [00:01<00:00, 56.75it/s]
loading data: 100%|██████████| 70/70 [00:01<00:00, 56.52it/s]
[ WARN:0@5.353] global loadsave.cpp:248 findDecoder imread_('/home/saptarshi/dev/CustomComposer/workdir/basket_5_2_val4_noflip_2/inerf_data_iter_23/post_flip_image1.png'): can't open/read file: check file path/integrity
155.46740898662756
Traceback (most recent call last):
  File "/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py", line 146, in <module>
    target_images_np.append(cv2.resize(cv2.imread(f'{args.input}/post_flip_image{idx+1}.png', cv2.IMREAD_UNCHANGED), (W, H), interpolation=cv2.INTER_AREA))
cv2.error: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'

['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True

loading data:   0%|          | 0/5 [00:00<?, ?it/s]
loading data: 100%|██████████| 5/5 [00:00<00:00, 54.09it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/70 [00:00<?, ?it/s]
loading data:   9%|▊         | 6/70 [00:00<00:01, 57.34it/s]
loading data:  19%|█▊        | 13/70 [00:00<00:00, 60.27it/s]
loading data:  29%|██▊       | 20/70 [00:00<00:00, 61.89it/s]
loading data:  39%|███▊      | 27/70 [00:00<00:00, 60.29it/s]
loading data:  49%|████▊     | 34/70 [00:00<00:00, 57.31it/s]
loading data:  59%|█████▊    | 41/70 [00:00<00:00, 58.41it/s]
loading data:  69%|██████▊   | 48/70 [00:00<00:00, 58.48it/s]
loading data:  77%|███████▋  | 54/70 [00:00<00:00, 57.47it/s]
loading data:  87%|████████▋ | 61/70 [00:01<00:00, 58.22it/s]
loading data:  97%|█████████▋| 68/70 [00:01<00:00, 59.35it/s]
loading data: 100%|██████████| 70/70 [00:01<00:00, 59.03it/s]
[ WARN:0@5.549] global loadsave.cpp:248 findDecoder imread_('/home/saptarshi/dev/CustomComposer/workdir/basket_5_2_val4_noflip_2/inerf_data_iter_23/post_flip_image1.png'): can't open/read file: check file path/integrity
155.46740898662756
Traceback (most recent call last):
  File "/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py", line 146, in <module>
    target_images_np.append(cv2.resize(cv2.imread(f'{args.input}/post_flip_image{idx+1}.png', cv2.IMREAD_UNCHANGED), (W, H), interpolation=cv2.INTER_AREA))
cv2.error: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'

['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True

loading data:   0%|          | 0/4 [00:00<?, ?it/s]
loading data: 100%|██████████| 4/4 [00:00<00:00, 57.60it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/68 [00:00<?, ?it/s]
loading data:  10%|█         | 7/68 [00:00<00:00, 65.63it/s]
loading data:  21%|██        | 14/68 [00:00<00:00, 66.21it/s]
loading data:  31%|███       | 21/68 [00:00<00:00, 65.18it/s]
loading data:  41%|████      | 28/68 [00:00<00:00, 66.76it/s]
loading data:  51%|█████▏    | 35/68 [00:00<00:00, 67.56it/s]
loading data:  62%|██████▏   | 42/68 [00:00<00:00, 68.12it/s]
loading data:  72%|███████▏  | 49/68 [00:00<00:00, 68.06it/s]
loading data:  82%|████████▏ | 56/68 [00:00<00:00, 66.85it/s]
loading data:  93%|█████████▎| 63/68 [00:00<00:00, 67.12it/s]
loading data: 100%|██████████| 68/68 [00:01<00:00, 67.25it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Loss at initial point 0.04461987316608429
Loss after optimization 0.022129451856017113
logm result may be inaccurate, approximate err = 2.889533899282208e-07
Loss at initial point 0.05897308886051178
Loss after optimization 0.023545492440462112
logm result may be inaccurate, approximate err = 7.409076943619857e-07
Loss at initial point 0.06427372992038727
Loss after optimization 0.023545492440462112
logm result may be inaccurate, approximate err = 9.714574524536884e-07
Loss at initial point 0.04296359047293663
Loss after optimization 0.023545492440462112
logm result may be inaccurate, approximate err = 1.1730723980192872e-06
Loss at initial point 0.027272121980786324
Loss after optimization 0.006008951459079981
logm result may be inaccurate, approximate err = 3.777805079213297e-07
Loss at initial point 0.022649426013231277
Loss after optimization 0.0032076421193778515
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Loss at initial point 0.04466612637042999
Loss after optimization 0.0235456395894289
logm result may be inaccurate, approximate err = 2.889533899282208e-07
Loss at initial point 0.05899641290307045
Loss after optimization 0.02354556694626808
logm result may be inaccurate, approximate err = 7.409076943619857e-07
Loss at initial point 0.06428574025630951
Loss after optimization 0.023545561358332634
logm result may be inaccurate, approximate err = 9.714574524536884e-07
Loss at initial point 0.04290963336825371
Loss after optimization 0.023545484989881516
logm result may be inaccurate, approximate err = 1.1730723980192872e-06
Loss at initial point 0.02728360705077648
Loss after optimization 0.010099494829773903
logm result may be inaccurate, approximate err = 3.777805079213297e-07
Loss at initial point 0.022617295384407043
Loss after optimization 0.01160925067961216
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Loss at initial point 0.04470770061016083
Loss after optimization 0.023545492440462112
logm result may be inaccurate, approximate err = 2.889533899282208e-07
Loss at initial point 0.05898085981607437
Loss after optimization 0.023545492440462112
logm result may be inaccurate, approximate err = 7.409076943619857e-07
Loss at initial point 0.06423871219158173
Loss after optimization 0.023545492440462112
logm result may be inaccurate, approximate err = 9.714574524536884e-07
Loss at initial point 0.042938362807035446
Loss after optimization 0.023545492440462112
logm result may be inaccurate, approximate err = 1.1730723980192872e-06
Loss at initial point 0.0272358525544405
Loss after optimization 0.023545492440462112
logm result may be inaccurate, approximate err = 3.777805079213297e-07
Loss at initial point 0.022654090076684952
Loss after optimization 0.0029643182642757893
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 63.101505279541016
Step 1, loss: 59.41897964477539
Step 2, loss: 55.31474304199219
Step 3, loss: 50.7744140625
Step 4, loss: 45.654808044433594
Step 5, loss: 40.92707443237305
Step 6, loss: 37.07997512817383
Step 7, loss: 35.031654357910156
Step 8, loss: 32.79719543457031
Step 9, loss: 31.398359298706055
Step 10, loss: 31.89404296875
Step 11, loss: 33.8403205871582
Step 12, loss: 35.800254821777344
Step 13, loss: 37.633384704589844
Step 14, loss: 39.341217041015625
Step 15, loss: 40.621456146240234
Step 16, loss: 41.72315979003906
Step 17, loss: 42.83550262451172
Step 18, loss: 43.82341003417969
Step 19, loss: 44.70197677612305
Step 20, loss: 45.55305099487305
Step 21, loss: 46.34575653076172
Step 22, loss: 47.26719284057617
Step 23, loss: 48.00052261352539
Step 24, loss: 48.75704574584961
Step 25, loss: 49.50706100463867
Step 26, loss: 50.262298583984375
Step 27, loss: 50.92528533935547
Step 28, loss: 51.61415100097656
Step 29, loss: 52.23662567138672
Step 30, loss: 52.85259246826172
Step 31, loss: 53.45990753173828
Step 32, loss: 53.93872833251953
Step 33, loss: 54.39399719238281
Step 34, loss: 54.71666717529297
Step 35, loss: 55.059104919433594
Step 36, loss: 55.19055938720703
Step 37, loss: 55.10676956176758
Step 38, loss: 54.987483978271484
Step 39, loss: 54.856971740722656
Step 40, loss: 54.71175003051758
Step 41, loss: 54.682647705078125
Step 42, loss: 54.47956085205078
Step 43, loss: 54.589263916015625
Step 44, loss: 54.38545227050781
Step 45, loss: 54.41062545776367
Step 46, loss: 54.328006744384766
Step 47, loss: 54.34269714355469
Step 48, loss: 54.218299865722656
Step 49, loss: 54.22915267944336
Step 50, loss: 54.24570083618164
Step 51, loss: 54.21485900878906
Step 52, loss: 54.16154861450195
Step 53, loss: 54.21714782714844
Step 54, loss: 54.19123840332031
Step 55, loss: 54.212867736816406
Step 56, loss: 54.2136116027832
Step 57, loss: 54.26490020751953
Step 58, loss: 54.30497741699219
Step 59, loss: 54.33509826660156
Step 60, loss: 54.41355895996094
Step 61, loss: 54.41070556640625
Step 62, loss: 54.499359130859375
Step 63, loss: 54.537601470947266
Step 64, loss: 54.61489486694336
Step 65, loss: 54.63954162597656
Step 66, loss: 54.789703369140625
Step 67, loss: 54.50581741333008
Step 68, loss: 53.963356018066406
Step 69, loss: 53.18656539916992
Step 70, loss: 52.345916748046875
Step 71, loss: 51.46983337402344
Step 72, loss: 50.59864044189453
Step 73, loss: 49.741844177246094
Step 74, loss: 48.87046813964844
Step 75, loss: 48.02257537841797
Step 76, loss: 47.275028228759766
Step 77, loss: 46.70463180541992
Step 78, loss: 46.32109069824219
Step 79, loss: 46.09092330932617
Step 80, loss: 46.078826904296875
Step 81, loss: 46.078826904296875
Step 82, loss: 46.078826904296875
Step 83, loss: 46.078826904296875
Step 84, loss: 46.078826904296875
Step 85, loss: 46.078826904296875
Step 86, loss: 46.078826904296875
Step 87, loss: 46.078826904296875
Step 88, loss: 46.078826904296875
Step 89, loss: 46.078826904296875
Step 90, loss: 46.078826904296875
Step 91, loss: 46.078826904296875
Step 92, loss: 46.078826904296875
Step 93, loss: 46.078826904296875
Step 94, loss: 46.078826904296875
Step 95, loss: 46.078826904296875
Step 96, loss: 46.078826904296875
Step 97, loss: 46.078826904296875
Step 98, loss: 46.078826904296875
Step 99, loss: 46.078826904296875
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 63.09540939331055
Step 1, loss: 59.378440856933594
Step 2, loss: 55.272010803222656
Step 3, loss: 50.79543685913086
Step 4, loss: 45.698551177978516
Step 5, loss: 41.00795364379883
Step 6, loss: 37.21623229980469
Step 7, loss: 35.18008041381836
Step 8, loss: 32.85144805908203
Step 9, loss: 31.244661331176758
Step 10, loss: 31.262819290161133
Step 11, loss: 33.17218017578125
Step 12, loss: 35.12939453125
Step 13, loss: 36.8017578125
Step 14, loss: 38.35244369506836
Step 15, loss: 39.7170524597168
Step 16, loss: 40.987979888916016
Step 17, loss: 41.94996643066406
Step 18, loss: 42.892539978027344
Step 19, loss: 43.760189056396484
Step 20, loss: 44.644317626953125
Step 21, loss: 45.47052001953125
Step 22, loss: 46.20002365112305
Step 23, loss: 46.929134368896484
Step 24, loss: 47.71929168701172
Step 25, loss: 48.446739196777344
Step 26, loss: 49.308433532714844
Step 27, loss: 50.007972717285156
Step 28, loss: 50.72872543334961
Step 29, loss: 51.38799285888672
Step 30, loss: 51.99712371826172
Step 31, loss: 52.65193557739258
Step 32, loss: 53.239444732666016
Step 33, loss: 53.73258590698242
Step 34, loss: 54.204837799072266
Step 35, loss: 54.52067565917969
Step 36, loss: 54.95191192626953
Step 37, loss: 55.1561393737793
Step 38, loss: 55.212337493896484
Step 39, loss: 55.07650375366211
Step 40, loss: 54.95035171508789
Step 41, loss: 54.8469123840332
Step 42, loss: 54.71074676513672
Step 43, loss: 54.7269401550293
Step 44, loss: 54.5801887512207
Step 45, loss: 54.582435607910156
Step 46, loss: 54.484615325927734
Step 47, loss: 54.481224060058594
Step 48, loss: 54.386783599853516
Step 49, loss: 54.41413879394531
Step 50, loss: 54.330257415771484
Step 51, loss: 54.37470245361328
Step 52, loss: 54.33934783935547
Step 53, loss: 54.369117736816406
Step 54, loss: 54.34711456298828
Step 55, loss: 54.361061096191406
Step 56, loss: 54.39234161376953
Step 57, loss: 54.389217376708984
Step 58, loss: 54.41868591308594
Step 59, loss: 54.41695022583008
Step 60, loss: 54.490478515625
Step 61, loss: 54.53170394897461
Step 62, loss: 54.52815628051758
Step 63, loss: 54.60311508178711
Step 64, loss: 54.64886474609375
Step 65, loss: 54.765628814697266
Step 66, loss: 54.82320022583008
Step 67, loss: 54.9324836730957
Step 68, loss: 54.73240280151367
Step 69, loss: 54.14054489135742
Step 70, loss: 53.34259796142578
Step 71, loss: 52.388118743896484
Step 72, loss: 51.4965705871582
Step 73, loss: 50.58579635620117
Step 74, loss: 49.65692901611328
Step 75, loss: 48.667762756347656
Step 76, loss: 47.87465286254883
Step 77, loss: 47.19528579711914
Step 78, loss: 46.67708969116211
Step 79, loss: 46.339786529541016
Step 80, loss: 46.13825607299805
Step 81, loss: 46.078826904296875
Step 82, loss: 46.078826904296875
Step 83, loss: 46.078826904296875
Step 84, loss: 46.078826904296875
Step 85, loss: 46.078826904296875
Step 86, loss: 46.078826904296875
Step 87, loss: 46.078826904296875
Step 88, loss: 46.078826904296875
Step 89, loss: 46.078826904296875
Step 90, loss: 46.078826904296875
Step 91, loss: 46.078826904296875
Step 92, loss: 46.078826904296875
Step 93, loss: 46.078826904296875
Step 94, loss: 46.078834533691406
Step 95, loss: 46.078826904296875
Step 96, loss: 46.078826904296875
Step 97, loss: 46.078826904296875
Step 98, loss: 46.078826904296875
Step 99, loss: 46.078826904296875
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 63.06716537475586
Step 1, loss: 59.36813735961914
Step 2, loss: 55.281890869140625
Step 3, loss: 50.79198455810547
Step 4, loss: 45.64398956298828
Step 5, loss: 40.92620086669922
Step 6, loss: 37.10800552368164
Step 7, loss: 34.9384765625
Step 8, loss: 32.56385803222656
Step 9, loss: 31.01270866394043
Step 10, loss: 31.079607009887695
Step 11, loss: 32.94123077392578
Step 12, loss: 34.77334976196289
Step 13, loss: 36.555274963378906
Step 14, loss: 38.35341262817383
Step 15, loss: 39.59847640991211
Step 16, loss: 40.791385650634766
Step 17, loss: 41.954872131347656
Step 18, loss: 42.927425384521484
Step 19, loss: 43.922645568847656
Step 20, loss: 44.87893295288086
Step 21, loss: 45.8175163269043
Step 22, loss: 46.61334991455078
Step 23, loss: 47.50989532470703
Step 24, loss: 48.36302185058594
Step 25, loss: 49.16129684448242
Step 26, loss: 49.93154525756836
Step 27, loss: 50.62896728515625
Step 28, loss: 51.225807189941406
Step 29, loss: 51.860599517822266
Step 30, loss: 52.47174835205078
Step 31, loss: 53.090396881103516
Step 32, loss: 53.604251861572266
Step 33, loss: 54.12452697753906
Step 34, loss: 54.52351379394531
Step 35, loss: 54.92689514160156
Step 36, loss: 55.18870162963867
Step 37, loss: 55.19382858276367
Step 38, loss: 55.06233215332031
Step 39, loss: 54.95286560058594
Step 40, loss: 54.892982482910156
Step 41, loss: 54.71346664428711
Step 42, loss: 54.71477127075195
Step 43, loss: 54.53189468383789
Step 44, loss: 54.57208251953125
Step 45, loss: 54.43779754638672
Step 46, loss: 54.39393997192383
Step 47, loss: 54.37372589111328
Step 48, loss: 54.31257247924805
Step 49, loss: 54.32660675048828
Step 50, loss: 54.22172164916992
Step 51, loss: 54.25520706176758
Step 52, loss: 54.22663116455078
Step 53, loss: 54.22834777832031
Step 54, loss: 54.21437454223633
Step 55, loss: 54.206260681152344
Step 56, loss: 54.210899353027344
Step 57, loss: 54.22373962402344
Step 58, loss: 54.25494384765625
Step 59, loss: 54.28135681152344
Step 60, loss: 54.3161506652832
Step 61, loss: 54.412532806396484
Step 62, loss: 54.368019104003906
Step 63, loss: 54.47616195678711
Step 64, loss: 54.49652099609375
Step 65, loss: 54.592010498046875
Step 66, loss: 54.622459411621094
Step 67, loss: 54.73280334472656
Step 68, loss: 54.47968673706055
Step 69, loss: 54.12507629394531
Step 70, loss: 53.679473876953125
Step 71, loss: 52.93370819091797
Step 72, loss: 52.093990325927734
Step 73, loss: 51.20981216430664
Step 74, loss: 50.305419921875
Step 75, loss: 49.42284393310547
Step 76, loss: 48.647457122802734
Step 77, loss: 47.84854507446289
Step 78, loss: 47.100101470947266
Step 79, loss: 46.52183532714844
Step 80, loss: 46.14884567260742
Step 81, loss: 46.078826904296875
Step 82, loss: 46.078826904296875
Step 83, loss: 46.078826904296875
Step 84, loss: 46.078826904296875
Step 85, loss: 46.078826904296875
Step 86, loss: 46.078826904296875
Step 87, loss: 46.078826904296875
Step 88, loss: 46.078826904296875
Step 89, loss: 46.078826904296875
Step 90, loss: 46.078826904296875
Step 91, loss: 46.078826904296875
Step 92, loss: 46.078826904296875
Step 93, loss: 46.078826904296875
Step 94, loss: 46.078826904296875
Step 95, loss: 46.078826904296875
Step 96, loss: 46.078826904296875
Step 97, loss: 46.078826904296875
Step 98, loss: 46.078826904296875
Step 99, loss: 46.078826904296875
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 63.0905876159668
Step 1, loss: 59.393409729003906
Step 2, loss: 55.26973342895508
Step 3, loss: 50.75739669799805
Step 4, loss: 45.621341705322266
Step 5, loss: 40.945438385009766
Step 6, loss: 37.12114715576172
Step 7, loss: 35.145408630371094
Step 8, loss: 32.773841857910156
Step 9, loss: 31.189571380615234
Step 10, loss: 31.48089599609375
Step 11, loss: 33.43444061279297
Step 12, loss: 35.390689849853516
Step 13, loss: 37.17425537109375
Step 14, loss: 38.711395263671875
Step 15, loss: 40.15321731567383
Step 16, loss: 41.45991134643555
Step 17, loss: 42.54149627685547
Step 18, loss: 43.55935287475586
Step 19, loss: 44.49837112426758
Step 20, loss: 45.314552307128906
Step 21, loss: 46.204402923583984
Step 22, loss: 46.985591888427734
Step 23, loss: 47.906761169433594
Step 24, loss: 48.55409240722656
Step 25, loss: 49.28483200073242
Step 26, loss: 50.02703857421875
Step 27, loss: 50.678749084472656
Step 28, loss: 51.32289123535156
Step 29, loss: 51.94383239746094
Step 30, loss: 52.49492263793945
Step 31, loss: 53.046566009521484
Step 32, loss: 53.54060363769531
Step 33, loss: 54.060691833496094
Step 34, loss: 54.46262741088867
Step 35, loss: 54.814510345458984
Step 36, loss: 55.13739776611328
Step 37, loss: 55.24711608886719
Step 38, loss: 55.11275100708008
Step 39, loss: 55.02544403076172
Step 40, loss: 54.86798858642578
Step 41, loss: 54.84090042114258
Step 42, loss: 54.689857482910156
Step 43, loss: 54.605003356933594
Step 44, loss: 54.609561920166016
Step 45, loss: 54.474830627441406
Step 46, loss: 54.50847625732422
Step 47, loss: 54.36524963378906
Step 48, loss: 54.37415313720703
Step 49, loss: 54.3375358581543
Step 50, loss: 54.32093811035156
Step 51, loss: 54.27086639404297
Step 52, loss: 54.231422424316406
Step 53, loss: 54.25684356689453
Step 54, loss: 54.271331787109375
Step 55, loss: 54.255821228027344
Step 56, loss: 54.229732513427734
Step 57, loss: 54.28734588623047
Step 58, loss: 54.31519317626953
Step 59, loss: 54.36918258666992
Step 60, loss: 54.42213439941406
Step 61, loss: 54.48271942138672
Step 62, loss: 54.474056243896484
Step 63, loss: 54.58474349975586
Step 64, loss: 54.652957916259766
Step 65, loss: 54.699947357177734
Step 66, loss: 54.764923095703125
Step 67, loss: 54.8846549987793
Step 68, loss: 54.58884811401367
Step 69, loss: 54.03599548339844
Step 70, loss: 53.22706985473633
Step 71, loss: 52.317806243896484
Step 72, loss: 51.46805191040039
Step 73, loss: 50.50904846191406
Step 74, loss: 49.611305236816406
Step 75, loss: 48.68976974487305
Step 76, loss: 47.8486328125
Step 77, loss: 47.14258575439453
Step 78, loss: 46.61874771118164
Step 79, loss: 46.24620819091797
Step 80, loss: 46.078826904296875
Step 81, loss: 46.078826904296875
Step 82, loss: 46.078826904296875
Step 83, loss: 46.078826904296875
Step 84, loss: 46.078826904296875
Step 85, loss: 46.078826904296875
Step 86, loss: 46.078826904296875
Step 87, loss: 46.078826904296875
Step 88, loss: 46.078826904296875
Step 89, loss: 46.078826904296875
Step 90, loss: 46.078826904296875
Step 91, loss: 46.078826904296875
Step 92, loss: 46.078826904296875
Step 93, loss: 46.078826904296875
Step 94, loss: 46.078826904296875
Step 95, loss: 46.078826904296875
Step 96, loss: 46.078826904296875
Step 97, loss: 46.078826904296875
Step 98, loss: 46.078826904296875
Step 99, loss: 46.078826904296875
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 63.100914001464844
Step 1, loss: 59.395790100097656
Step 2, loss: 55.29177474975586
Step 3, loss: 50.75316619873047
Step 4, loss: 45.640750885009766
Step 5, loss: 40.94036102294922
Step 6, loss: 37.084285736083984
Step 7, loss: 34.77030944824219
Step 8, loss: 32.40593338012695
Step 9, loss: 30.78583526611328
Step 10, loss: 30.909629821777344
Step 11, loss: 32.935035705566406
Step 12, loss: 34.80775833129883
Step 13, loss: 36.75099182128906
Step 14, loss: 38.45153045654297
Step 15, loss: 39.68696975708008
Step 16, loss: 40.887168884277344
Step 17, loss: 41.9597282409668
Step 18, loss: 43.01115798950195
Step 19, loss: 43.913543701171875
Step 20, loss: 44.73085021972656
Step 21, loss: 45.62845230102539
Step 22, loss: 46.44974899291992
Step 23, loss: 47.28700637817383
Step 24, loss: 48.118656158447266
Step 25, loss: 48.860992431640625
Step 26, loss: 49.649208068847656
Step 27, loss: 50.38170623779297
Step 28, loss: 51.001487731933594
Step 29, loss: 51.660911560058594
Step 30, loss: 52.33098220825195
Step 31, loss: 52.885658264160156
Step 32, loss: 53.515830993652344
Step 33, loss: 53.95420455932617
Step 34, loss: 54.38605499267578
Step 35, loss: 54.782958984375
Step 36, loss: 55.10464859008789
Step 37, loss: 55.231056213378906
Step 38, loss: 55.09352493286133
Step 39, loss: 54.984825134277344
Step 40, loss: 54.92509078979492
Step 41, loss: 54.80499267578125
Step 42, loss: 54.737606048583984
Step 43, loss: 54.62749099731445
Step 44, loss: 54.599761962890625
Step 45, loss: 54.50369644165039
Step 46, loss: 54.54593276977539
Step 47, loss: 54.455772399902344
Step 48, loss: 54.39967727661133
Step 49, loss: 54.332767486572266
Step 50, loss: 54.2714729309082
Step 51, loss: 54.34535217285156
Step 52, loss: 54.269554138183594
Step 53, loss: 54.301212310791016
Step 54, loss: 54.324256896972656
Step 55, loss: 54.28541564941406
Step 56, loss: 54.26679229736328
Step 57, loss: 54.35835647583008
Step 58, loss: 54.33338928222656
Step 59, loss: 54.42186737060547
Step 60, loss: 54.382530212402344
Step 61, loss: 54.41862106323242
Step 62, loss: 54.4865837097168
Step 63, loss: 54.505252838134766
Step 64, loss: 54.60368728637695
Step 65, loss: 54.63010025024414
Step 66, loss: 54.72764205932617
Step 67, loss: 54.86212921142578
Step 68, loss: 54.60750961303711
Step 69, loss: 54.12853240966797
Step 70, loss: 53.369205474853516
Step 71, loss: 52.53028869628906
Step 72, loss: 51.6641731262207
Step 73, loss: 50.790435791015625
Step 74, loss: 49.90537643432617
Step 75, loss: 49.04397201538086
Step 76, loss: 48.182796478271484
Step 77, loss: 47.41311264038086
Step 78, loss: 46.824554443359375
Step 79, loss: 46.38332748413086
Step 80, loss: 46.110382080078125
Step 81, loss: 46.078826904296875
Step 82, loss: 46.078826904296875
Step 83, loss: 46.078826904296875
Step 84, loss: 46.078826904296875
Step 85, loss: 46.078826904296875
Step 86, loss: 46.078826904296875
Step 87, loss: 46.078826904296875
Step 88, loss: 46.078826904296875
Step 89, loss: 46.078826904296875
Step 90, loss: 46.078826904296875
Step 91, loss: 46.078826904296875
Step 92, loss: 46.078826904296875
Step 93, loss: 46.078826904296875
Step 94, loss: 46.078826904296875
Step 95, loss: 46.078826904296875
Step 96, loss: 46.078826904296875
Step 97, loss: 46.078826904296875
Step 98, loss: 46.078826904296875
Step 99, loss: 46.078826904296875
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 63.08097457885742
Step 1, loss: 59.36190414428711
Step 2, loss: 55.248565673828125
Step 3, loss: 50.72209167480469
Step 4, loss: 45.613868713378906
Step 5, loss: 40.94126892089844
Step 6, loss: 37.11518096923828
Step 7, loss: 35.1467399597168
Step 8, loss: 32.85380935668945
Step 9, loss: 31.431640625
Step 10, loss: 31.94808578491211
Step 11, loss: 34.10190200805664
Step 12, loss: 36.250186920166016
Step 13, loss: 38.188838958740234
Step 14, loss: 39.85057067871094
Step 15, loss: 41.3173713684082
Step 16, loss: 42.52824783325195
Step 17, loss: 43.546348571777344
Step 18, loss: 44.5825309753418
Step 19, loss: 45.5860481262207
Step 20, loss: 46.51606750488281
Step 21, loss: 47.42460250854492
Step 22, loss: 48.23957061767578
Step 23, loss: 49.02744674682617
Step 24, loss: 49.93254089355469
Step 25, loss: 50.723472595214844
Step 26, loss: 51.42182922363281
Step 27, loss: 52.16133499145508
Step 28, loss: 52.78495407104492
Step 29, loss: 53.38688278198242
Step 30, loss: 53.866783142089844
Step 31, loss: 54.43690490722656
Step 32, loss: 54.856109619140625
Step 33, loss: 55.18342208862305
Step 34, loss: 55.54812240600586
Step 35, loss: 55.51256561279297
Step 36, loss: 55.469520568847656
Step 37, loss: 55.369136810302734
Step 38, loss: 55.25178146362305
Step 39, loss: 55.19260787963867
Step 40, loss: 55.0148811340332
Step 41, loss: 55.02845764160156
Step 42, loss: 54.83954620361328
Step 43, loss: 54.849334716796875
Step 44, loss: 54.743927001953125
Step 45, loss: 54.76350021362305
Step 46, loss: 54.72386169433594
Step 47, loss: 54.66789245605469
Step 48, loss: 54.61429977416992
Step 49, loss: 54.630126953125
Step 50, loss: 54.58036422729492
Step 51, loss: 54.59894943237305
Step 52, loss: 54.62685775756836
Step 53, loss: 54.56171417236328
Step 54, loss: 54.61941146850586
Step 55, loss: 54.65436553955078
Step 56, loss: 54.716304779052734
Step 57, loss: 54.739315032958984
Step 58, loss: 54.77963638305664
Step 59, loss: 54.886260986328125
Step 60, loss: 54.89420700073242
Step 61, loss: 54.94879913330078
Step 62, loss: 55.01462936401367
Step 63, loss: 55.09608840942383
Step 64, loss: 55.18551254272461
Step 65, loss: 55.00590515136719
Step 66, loss: 54.42476272583008
Step 67, loss: 53.6225471496582
Step 68, loss: 52.660423278808594
Step 69, loss: 51.833736419677734
Step 70, loss: 50.9340934753418
Step 71, loss: 49.98707580566406
Step 72, loss: 48.993682861328125
Step 73, loss: 48.054500579833984
Step 74, loss: 47.313873291015625
Step 75, loss: 46.722530364990234
Step 76, loss: 46.338138580322266
Step 77, loss: 46.12527847290039
Step 78, loss: 46.078826904296875
Step 79, loss: 46.078826904296875
Step 80, loss: 46.078826904296875
Step 81, loss: 46.078834533691406
Step 82, loss: 46.078826904296875
Step 83, loss: 46.078826904296875
Step 84, loss: 46.078826904296875
Step 85, loss: 46.078826904296875
Step 86, loss: 46.078826904296875
Step 87, loss: 46.078826904296875
Step 88, loss: 46.078826904296875
Step 89, loss: 46.078826904296875
Step 90, loss: 46.078826904296875
Step 91, loss: 46.078826904296875
Step 92, loss: 46.078826904296875
Step 93, loss: 46.078826904296875
Step 94, loss: 46.078826904296875
Step 95, loss: 46.078826904296875
Step 96, loss: 46.078826904296875
Step 97, loss: 46.078826904296875
Step 98, loss: 46.078826904296875
Step 99, loss: 46.078826904296875
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 0.52214256,  0.01931436,  0.85263949,  0.21758836],
       [-0.85281894,  0.00222869,  0.52220197,  0.76334882],
       [ 0.00818573, -0.99981098,  0.01763535,  0.0868555 ],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.4050137 ,  0.49974378,  0.76565009,  0.22002702],
       [-0.82523842,  0.56032862,  0.07080535,  0.66591518],
       [-0.39363112, -0.66052101,  0.63934853,  0.2838686 ],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 0.5080671 ,  0.07649664,  0.8579138 ,  0.04548399],
       [-0.86108636,  0.0220373 ,  0.50798095,  0.75814334],
       [ 0.01995273, -0.99682627,  0.07706664,  0.17140364],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[ 0.5080671 ,  0.07649664,  0.8579138 ,  0.04548399],
       [-0.86108636,  0.0220373 ,  0.50798095,  0.75814334],
       [ 0.01995273, -0.99682627,  0.07706664,  0.17140364],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[ 0.40099084, -0.45544603,  0.79484224,  0.44085556],
       [-0.48167187, -0.84285903, -0.23995975,  1.02454673],
       [ 0.77922887, -0.28663179, -0.55735511, -0.30683253],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False

loading data:   0%|          | 0/4 [00:00<?, ?it/s]
loading data: 100%|██████████| 4/4 [00:00<00:00, 53.77it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/68 [00:00<?, ?it/s]
loading data:  10%|█         | 7/68 [00:00<00:00, 66.47it/s]
loading data:  21%|██        | 14/68 [00:00<00:00, 66.86it/s]
loading data:  31%|███       | 21/68 [00:00<00:00, 67.53it/s]
loading data:  41%|████      | 28/68 [00:00<00:00, 67.72it/s]
loading data:  53%|█████▎    | 36/68 [00:00<00:00, 68.75it/s]
loading data:  63%|██████▎   | 43/68 [00:00<00:00, 60.83it/s]
loading data:  74%|███████▎  | 50/68 [00:00<00:00, 59.30it/s]
loading data:  85%|████████▌ | 58/68 [00:00<00:00, 62.86it/s]
loading data:  97%|█████████▋| 66/68 [00:01<00:00, 65.71it/s]
loading data: 100%|██████████| 68/68 [00:01<00:00, 65.00it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(p, device='cuda', dtype=torch.float32)
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Loss at initial point 0.1935468725860119
Loss after optimization 0.05213171150535345
logm result may be inaccurate, approximate err = 3.572278853216968e-07
Loss at initial point 0.2215748056769371
Loss after optimization 0.036901913583278656
logm result may be inaccurate, approximate err = 5.89503235995473e-07
Loss at initial point 0.10323935374617577
Loss after optimization 0.018878417555242777
logm result may be inaccurate, approximate err = 8.576643970835055e-07
Loss at initial point 0.14439266361296177
Loss after optimization 0.020391436526551843
logm result may be inaccurate, approximate err = 4.435257111943495e-07
Loss at initial point 0.22929973155260086
Loss after optimization 0.20573309063911438
logm result may be inaccurate, approximate err = 5.042083854764364e-07
Loss at initial point 0.2174774929881096
Loss after optimization 0.08483034744858742
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Loss at initial point 0.19354108721017838
Loss after optimization 0.1070901695638895
logm result may be inaccurate, approximate err = 3.572278853216968e-07
Loss at initial point 0.22160357981920242
Loss after optimization 0.10334143228828907
logm result may be inaccurate, approximate err = 5.89503235995473e-07
Loss at initial point 0.10323657654225826
Loss after optimization 0.034683057107031345
logm result may be inaccurate, approximate err = 8.576643970835055e-07
Loss at initial point 0.1443207673728466
Loss after optimization 0.10476633533835411
logm result may be inaccurate, approximate err = 4.435257111943495e-07
Loss at initial point 0.2292533703148365
Loss after optimization 0.1077624149620533
logm result may be inaccurate, approximate err = 5.042083854764364e-07
Loss at initial point 0.21753985807299614
Loss after optimization 0.10772679001092911
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Loss at initial point 0.1935088038444519
Loss after optimization 0.1070901695638895
logm result may be inaccurate, approximate err = 3.572278853216968e-07
Loss at initial point 0.22154204547405243
Loss after optimization 0.10148761048913002
logm result may be inaccurate, approximate err = 5.89503235995473e-07
Loss at initial point 0.1031784787774086
Loss after optimization 0.01884427131153643
logm result may be inaccurate, approximate err = 8.576643970835055e-07
Loss at initial point 0.1443363018333912
Loss after optimization 0.1070901695638895
logm result may be inaccurate, approximate err = 4.435257111943495e-07
Loss at initial point 0.22929874435067177
Loss after optimization 0.1070901695638895
logm result may be inaccurate, approximate err = 5.042083854764364e-07
Loss at initial point 0.2174665667116642
Loss after optimization 0.1070901695638895
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 257.1153259277344
Step 1, loss: 247.37890625
Step 2, loss: 236.615478515625
Step 3, loss: 222.68304443359375
Step 4, loss: 205.38067626953125
Step 5, loss: 186.33200073242188
Step 6, loss: 165.09739685058594
Step 7, loss: 145.45103454589844
Step 8, loss: 125.07662200927734
Step 9, loss: 110.65538787841797
Step 10, loss: 112.2759017944336
Step 11, loss: 128.2050323486328
Step 12, loss: 145.56134033203125
Step 13, loss: 159.92066955566406
Step 14, loss: 171.27630615234375
Step 15, loss: 179.64723205566406
Step 16, loss: 186.13491821289062
Step 17, loss: 191.33059692382812
Step 18, loss: 195.29180908203125
Step 19, loss: 198.6612091064453
Step 20, loss: 201.76669311523438
Step 21, loss: 204.12802124023438
Step 22, loss: 206.23233032226562
Step 23, loss: 208.19874572753906
Step 24, loss: 209.86807250976562
Step 25, loss: 211.24819946289062
Step 26, loss: 212.25985717773438
Step 27, loss: 213.26519775390625
Step 28, loss: 214.3719482421875
Step 29, loss: 215.4261016845703
Step 30, loss: 216.7206573486328
Step 31, loss: 217.7283172607422
Step 32, loss: 219.06158447265625
Step 33, loss: 220.56773376464844
Step 34, loss: 222.13690185546875
Step 35, loss: 224.0635528564453
Step 36, loss: 225.78036499023438
Step 37, loss: 227.93568420410156
Step 38, loss: 230.3556365966797
Step 39, loss: 233.10076904296875
Step 40, loss: 236.44094848632812
Step 41, loss: 240.4117431640625
Step 42, loss: 244.71913146972656
Step 43, loss: 249.35903930664062
Step 44, loss: 254.05477905273438
Step 45, loss: 258.604736328125
Step 46, loss: 262.73565673828125
Step 47, loss: 264.87799072265625
Step 48, loss: 266.09228515625
Step 49, loss: 265.9691162109375
Step 50, loss: 263.6434326171875
Step 51, loss: 258.7716064453125
Step 52, loss: 251.28990173339844
Step 53, loss: 243.2897491455078
Step 54, loss: 236.23207092285156
Step 55, loss: 229.17990112304688
Step 56, loss: 222.1247100830078
Step 57, loss: 215.73471069335938
Step 58, loss: 210.59698486328125
Step 59, loss: 208.88372802734375
Step 60, loss: 208.13729858398438
Step 61, loss: 207.37586975097656
Step 62, loss: 206.6790313720703
Step 63, loss: 205.9703369140625
Step 64, loss: 205.35935974121094
Step 65, loss: 204.6793212890625
Step 66, loss: 204.0055389404297
Step 67, loss: 203.43287658691406
Step 68, loss: 202.87705993652344
Step 69, loss: 202.50433349609375
Step 70, loss: 201.9552764892578
Step 71, loss: 201.55184936523438
Step 72, loss: 201.1659698486328
Step 73, loss: 200.70420837402344
Step 74, loss: 200.39036560058594
Step 75, loss: 200.087646484375
Step 76, loss: 199.8495635986328
Step 77, loss: 199.48802185058594
Step 78, loss: 199.13844299316406
Step 79, loss: 198.97300720214844
Step 80, loss: 198.63111877441406
Step 81, loss: 198.40757751464844
Step 82, loss: 198.21682739257812
Step 83, loss: 198.01617431640625
Step 84, loss: 197.89187622070312
Step 85, loss: 197.7654571533203
Step 86, loss: 197.56991577148438
Step 87, loss: 197.3626708984375
Step 88, loss: 197.21217346191406
Step 89, loss: 197.0886993408203
Step 90, loss: 196.88644409179688
Step 91, loss: 196.8267822265625
Step 92, loss: 196.56744384765625
Step 93, loss: 196.50108337402344
Step 94, loss: 196.3140869140625
Step 95, loss: 196.20779418945312
Step 96, loss: 196.14990234375
Step 97, loss: 196.03717041015625
Step 98, loss: 195.90994262695312
Step 99, loss: 195.90103149414062
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 256.9465637207031
Step 1, loss: 247.3908233642578
Step 2, loss: 236.77732849121094
Step 3, loss: 222.95542907714844
Step 4, loss: 205.48875427246094
Step 5, loss: 186.39642333984375
Step 6, loss: 165.58094787597656
Step 7, loss: 145.56240844726562
Step 8, loss: 124.79022216796875
Step 9, loss: 109.7161865234375
Step 10, loss: 112.12385559082031
Step 11, loss: 129.16427612304688
Step 12, loss: 146.92361450195312
Step 13, loss: 161.66802978515625
Step 14, loss: 173.08792114257812
Step 15, loss: 181.74371337890625
Step 16, loss: 188.33963012695312
Step 17, loss: 193.66668701171875
Step 18, loss: 198.0536651611328
Step 19, loss: 201.4346923828125
Step 20, loss: 204.67709350585938
Step 21, loss: 207.302734375
Step 22, loss: 209.7382354736328
Step 23, loss: 212.17233276367188
Step 24, loss: 214.06675720214844
Step 25, loss: 216.25912475585938
Step 26, loss: 218.41561889648438
Step 27, loss: 220.64796447753906
Step 28, loss: 223.15647888183594
Step 29, loss: 225.50711059570312
Step 30, loss: 228.46617126464844
Step 31, loss: 231.24305725097656
Step 32, loss: 234.54135131835938
Step 33, loss: 238.68560791015625
Step 34, loss: 243.58831787109375
Step 35, loss: 248.48934936523438
Step 36, loss: 253.21734619140625
Step 37, loss: 257.6322937011719
Step 38, loss: 261.7475280761719
Step 39, loss: 264.8494567871094
Step 40, loss: 267.01617431640625
Step 41, loss: 268.0333557128906
Step 42, loss: 268.0626525878906
Step 43, loss: 266.1195068359375
Step 44, loss: 261.7403869628906
Step 45, loss: 254.80734252929688
Step 46, loss: 245.3694305419922
Step 47, loss: 236.7843475341797
Step 48, loss: 231.64898681640625
Step 49, loss: 227.3146209716797
Step 50, loss: 222.9781036376953
Step 51, loss: 218.38406372070312
Step 52, loss: 214.44325256347656
Step 53, loss: 212.6302490234375
Step 54, loss: 211.97068786621094
Step 55, loss: 211.29347229003906
Step 56, loss: 210.6304473876953
Step 57, loss: 210.13980102539062
Step 58, loss: 209.5611114501953
Step 59, loss: 209.0264129638672
Step 60, loss: 208.49398803710938
Step 61, loss: 207.91622924804688
Step 62, loss: 207.34161376953125
Step 63, loss: 206.8350067138672
Step 64, loss: 206.296875
Step 65, loss: 205.860107421875
Step 66, loss: 205.3811492919922
Step 67, loss: 204.9029083251953
Step 68, loss: 204.5570526123047
Step 69, loss: 204.10350036621094
Step 70, loss: 203.7957763671875
Step 71, loss: 203.49496459960938
Step 72, loss: 203.2073516845703
Step 73, loss: 202.85804748535156
Step 74, loss: 202.6038818359375
Step 75, loss: 202.17625427246094
Step 76, loss: 201.91598510742188
Step 77, loss: 201.67633056640625
Step 78, loss: 201.44384765625
Step 79, loss: 201.1508026123047
Step 80, loss: 200.92079162597656
Step 81, loss: 200.61830139160156
Step 82, loss: 200.56163024902344
Step 83, loss: 200.29518127441406
Step 84, loss: 200.10264587402344
Step 85, loss: 199.87759399414062
Step 86, loss: 199.7045135498047
Step 87, loss: 199.53114318847656
Step 88, loss: 199.34689331054688
Step 89, loss: 199.1985321044922
Step 90, loss: 199.0602264404297
Step 91, loss: 198.80416870117188
Step 92, loss: 198.66661071777344
Step 93, loss: 198.47398376464844
Step 94, loss: 198.40768432617188
Step 95, loss: 198.22938537597656
Step 96, loss: 198.1067352294922
Step 97, loss: 197.9771270751953
Step 98, loss: 197.77723693847656
Step 99, loss: 197.65589904785156
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 257.1312255859375
Step 1, loss: 247.43746948242188
Step 2, loss: 236.72036743164062
Step 3, loss: 223.64251708984375
Step 4, loss: 207.0331573486328
Step 5, loss: 188.98843383789062
Step 6, loss: 168.72943115234375
Step 7, loss: 149.29327392578125
Step 8, loss: 129.49716186523438
Step 9, loss: 113.64923095703125
Step 10, loss: 114.49512481689453
Step 11, loss: 130.9464874267578
Step 12, loss: 146.502197265625
Step 13, loss: 160.8758544921875
Step 14, loss: 172.73992919921875
Step 15, loss: 182.40835571289062
Step 16, loss: 190.59776306152344
Step 17, loss: 197.30120849609375
Step 18, loss: 202.8885955810547
Step 19, loss: 207.2454071044922
Step 20, loss: 211.4739227294922
Step 21, loss: 214.5992431640625
Step 22, loss: 217.59014892578125
Step 23, loss: 220.17300415039062
Step 24, loss: 222.6454620361328
Step 25, loss: 224.53472900390625
Step 26, loss: 226.54208374023438
Step 27, loss: 228.6433868408203
Step 28, loss: 231.31454467773438
Step 29, loss: 234.29638671875
Step 30, loss: 238.0574493408203
Step 31, loss: 242.571533203125
Step 32, loss: 247.809814453125
Step 33, loss: 252.84005737304688
Step 34, loss: 257.62420654296875
Step 35, loss: 261.989501953125
Step 36, loss: 265.5390319824219
Step 37, loss: 267.7194519042969
Step 38, loss: 268.8324279785156
Step 39, loss: 269.0609436035156
Step 40, loss: 267.42437744140625
Step 41, loss: 263.3149108886719
Step 42, loss: 256.49200439453125
Step 43, loss: 247.25958251953125
Step 44, loss: 236.65501403808594
Step 45, loss: 230.41087341308594
Step 46, loss: 225.3909454345703
Step 47, loss: 220.6365203857422
Step 48, loss: 216.03042602539062
Step 49, loss: 212.433837890625
Step 50, loss: 211.08807373046875
Step 51, loss: 210.3363800048828
Step 52, loss: 209.559814453125
Step 53, loss: 208.95924377441406
Step 54, loss: 208.18853759765625
Step 55, loss: 207.6320343017578
Step 56, loss: 207.07574462890625
Step 57, loss: 206.61849975585938
Step 58, loss: 206.0414276123047
Step 59, loss: 205.67921447753906
Step 60, loss: 205.21913146972656
Step 61, loss: 204.72535705566406
Step 62, loss: 204.29718017578125
Step 63, loss: 203.94679260253906
Step 64, loss: 203.5675506591797
Step 65, loss: 203.15135192871094
Step 66, loss: 202.7978057861328
Step 67, loss: 202.4438934326172
Step 68, loss: 202.09063720703125
Step 69, loss: 201.74766540527344
Step 70, loss: 201.43455505371094
Step 71, loss: 201.16558837890625
Step 72, loss: 200.79747009277344
Step 73, loss: 200.52529907226562
Step 74, loss: 200.20298767089844
Step 75, loss: 199.86741638183594
Step 76, loss: 199.5914764404297
Step 77, loss: 199.33897399902344
Step 78, loss: 198.97593688964844
Step 79, loss: 198.8159637451172
Step 80, loss: 198.5373992919922
Step 81, loss: 198.28067016601562
Step 82, loss: 197.99183654785156
Step 83, loss: 197.6386260986328
Step 84, loss: 197.5239715576172
Step 85, loss: 197.20068359375
Step 86, loss: 197.10243225097656
Step 87, loss: 196.79652404785156
Step 88, loss: 196.67323303222656
Step 89, loss: 196.42662048339844
Step 90, loss: 196.3065948486328
Step 91, loss: 196.04965209960938
Step 92, loss: 195.92893981933594
Step 93, loss: 195.83758544921875
Step 94, loss: 195.62718200683594
Step 95, loss: 195.56300354003906
Step 96, loss: 195.41897583007812
Step 97, loss: 195.33619689941406
Step 98, loss: 195.2488555908203
Step 99, loss: 195.0886993408203
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 257.0721740722656
Step 1, loss: 247.55343627929688
Step 2, loss: 236.65675354003906
Step 3, loss: 221.82980346679688
Step 4, loss: 203.28762817382812
Step 5, loss: 182.6297607421875
Step 6, loss: 160.64215087890625
Step 7, loss: 138.3241729736328
Step 8, loss: 116.93607330322266
Step 9, loss: 103.53565216064453
Step 10, loss: 110.11773681640625
Step 11, loss: 129.24073791503906
Step 12, loss: 147.5783233642578
Step 13, loss: 161.6239471435547
Step 14, loss: 171.73373413085938
Step 15, loss: 178.91079711914062
Step 16, loss: 184.40737915039062
Step 17, loss: 188.93524169921875
Step 18, loss: 192.43466186523438
Step 19, loss: 195.2530059814453
Step 20, loss: 197.919189453125
Step 21, loss: 200.20248413085938
Step 22, loss: 202.34046936035156
Step 23, loss: 204.5931854248047
Step 24, loss: 206.78570556640625
Step 25, loss: 208.78262329101562
Step 26, loss: 210.84915161132812
Step 27, loss: 212.56576538085938
Step 28, loss: 214.04664611816406
Step 29, loss: 214.9298858642578
Step 30, loss: 215.38111877441406
Step 31, loss: 215.4844512939453
Step 32, loss: 215.0909881591797
Step 33, loss: 214.5701446533203
Step 34, loss: 213.27029418945312
Step 35, loss: 211.9681396484375
Step 36, loss: 210.4927215576172
Step 37, loss: 208.96746826171875
Step 38, loss: 207.53179931640625
Step 39, loss: 206.55368041992188
Step 40, loss: 205.91949462890625
Step 41, loss: 205.48753356933594
Step 42, loss: 205.5771026611328
Step 43, loss: 205.69107055664062
Step 44, loss: 206.00039672851562
Step 45, loss: 206.16455078125
Step 46, loss: 205.93572998046875
Step 47, loss: 205.51007080078125
Step 48, loss: 205.63572692871094
Step 49, loss: 205.7139892578125
Step 50, loss: 206.23170471191406
Step 51, loss: 206.85260009765625
Step 52, loss: 207.63272094726562
Step 53, loss: 208.74612426757812
Step 54, loss: 209.98622131347656
Step 55, loss: 211.6030731201172
Step 56, loss: 214.03756713867188
Step 57, loss: 216.45248413085938
Step 58, loss: 219.09307861328125
Step 59, loss: 222.80836486816406
Step 60, loss: 226.86752319335938
Step 61, loss: 231.6363067626953
Step 62, loss: 237.19122314453125
Step 63, loss: 243.37252807617188
Step 64, loss: 250.27297973632812
Step 65, loss: 257.9484558105469
Step 66, loss: 263.86883544921875
Step 67, loss: 266.1846618652344
Step 68, loss: 264.75323486328125
Step 69, loss: 259.460205078125
Step 70, loss: 250.27940368652344
Step 71, loss: 236.33192443847656
Step 72, loss: 220.06201171875
Step 73, loss: 209.24081420898438
Step 74, loss: 208.0381317138672
Step 75, loss: 207.8175048828125
Step 76, loss: 207.25588989257812
Step 77, loss: 206.62551879882812
Step 78, loss: 205.89854431152344
Step 79, loss: 205.2597198486328
Step 80, loss: 204.7665252685547
Step 81, loss: 204.1409149169922
Step 82, loss: 203.66905212402344
Step 83, loss: 203.15679931640625
Step 84, loss: 202.7369842529297
Step 85, loss: 202.2899169921875
Step 86, loss: 201.9440460205078
Step 87, loss: 201.5800323486328
Step 88, loss: 201.1396942138672
Step 89, loss: 200.8351287841797
Step 90, loss: 200.53701782226562
Step 91, loss: 200.1503448486328
Step 92, loss: 199.8693084716797
Step 93, loss: 199.65457153320312
Step 94, loss: 199.2368621826172
Step 95, loss: 199.06837463378906
Step 96, loss: 198.8449249267578
Step 97, loss: 198.50465393066406
Step 98, loss: 198.36593627929688
Step 99, loss: 198.13429260253906
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 257.019287109375
Step 1, loss: 247.45968627929688
Step 2, loss: 236.73802185058594
Step 3, loss: 223.3290252685547
Step 4, loss: 205.9989013671875
Step 5, loss: 186.84376525878906
Step 6, loss: 166.10641479492188
Step 7, loss: 146.76400756835938
Step 8, loss: 126.39083099365234
Step 9, loss: 112.55944061279297
Step 10, loss: 114.00032806396484
Step 11, loss: 129.5486602783203
Step 12, loss: 146.50775146484375
Step 13, loss: 160.87725830078125
Step 14, loss: 172.16278076171875
Step 15, loss: 181.09283447265625
Step 16, loss: 188.28948974609375
Step 17, loss: 193.6631622314453
Step 18, loss: 197.89517211914062
Step 19, loss: 201.4790496826172
Step 20, loss: 204.34817504882812
Step 21, loss: 206.59011840820312
Step 22, loss: 208.4469451904297
Step 23, loss: 210.47511291503906
Step 24, loss: 211.9323272705078
Step 25, loss: 213.08419799804688
Step 26, loss: 214.24679565429688
Step 27, loss: 215.229736328125
Step 28, loss: 216.20358276367188
Step 29, loss: 217.5859375
Step 30, loss: 218.75897216796875
Step 31, loss: 220.66030883789062
Step 32, loss: 222.81472778320312
Step 33, loss: 225.037109375
Step 34, loss: 227.57565307617188
Step 35, loss: 230.31668090820312
Step 36, loss: 233.33883666992188
Step 37, loss: 236.56149291992188
Step 38, loss: 240.23001098632812
Step 39, loss: 244.07046508789062
Step 40, loss: 248.42625427246094
Step 41, loss: 252.84356689453125
Step 42, loss: 257.04827880859375
Step 43, loss: 260.9378967285156
Step 44, loss: 264.091796875
Step 45, loss: 265.8788146972656
Step 46, loss: 266.9869689941406
Step 47, loss: 266.9058837890625
Step 48, loss: 265.0874938964844
Step 49, loss: 260.3445129394531
Step 50, loss: 253.23358154296875
Step 51, loss: 242.58535766601562
Step 52, loss: 236.1272430419922
Step 53, loss: 230.99313354492188
Step 54, loss: 226.28717041015625
Step 55, loss: 221.50717163085938
Step 56, loss: 216.61697387695312
Step 57, loss: 213.28689575195312
Step 58, loss: 212.27261352539062
Step 59, loss: 211.53555297851562
Step 60, loss: 210.88433837890625
Step 61, loss: 210.21717834472656
Step 62, loss: 209.77194213867188
Step 63, loss: 209.2981719970703
Step 64, loss: 208.7209930419922
Step 65, loss: 208.2522735595703
Step 66, loss: 207.5442657470703
Step 67, loss: 207.07118225097656
Step 68, loss: 206.51809692382812
Step 69, loss: 206.0632781982422
Step 70, loss: 205.49375915527344
Step 71, loss: 205.0792236328125
Step 72, loss: 204.6230926513672
Step 73, loss: 204.2378387451172
Step 74, loss: 203.87440490722656
Step 75, loss: 203.4296112060547
Step 76, loss: 203.0535888671875
Step 77, loss: 202.72991943359375
Step 78, loss: 202.412109375
Step 79, loss: 202.1962127685547
Step 80, loss: 201.81703186035156
Step 81, loss: 201.56581115722656
Step 82, loss: 201.3329315185547
Step 83, loss: 201.0714569091797
Step 84, loss: 200.8550262451172
Step 85, loss: 200.58877563476562
Step 86, loss: 200.4209442138672
Step 87, loss: 200.18875122070312
Step 88, loss: 199.99391174316406
Step 89, loss: 199.7886505126953
Step 90, loss: 199.60577392578125
Step 91, loss: 199.45945739746094
Step 92, loss: 199.19984436035156
Step 93, loss: 199.06414794921875
Step 94, loss: 198.85520935058594
Step 95, loss: 198.77438354492188
Step 96, loss: 198.59962463378906
Step 97, loss: 198.38648986816406
Step 98, loss: 198.25680541992188
Step 99, loss: 198.18946838378906
logm result may be inaccurate, approximate err = 3.511603431029489e-07
Step 0, loss: 257.10418701171875
Step 1, loss: 247.35507202148438
Step 2, loss: 236.14248657226562
Step 3, loss: 221.7208709716797
Step 4, loss: 203.16934204101562
Step 5, loss: 182.69769287109375
Step 6, loss: 160.5210723876953
Step 7, loss: 138.792724609375
Step 8, loss: 118.04850006103516
Step 9, loss: 104.5418472290039
Step 10, loss: 111.84725952148438
Step 11, loss: 130.55743408203125
Step 12, loss: 148.45599365234375
Step 13, loss: 162.57614135742188
Step 14, loss: 173.0110321044922
Step 15, loss: 180.873291015625
Step 16, loss: 187.2963409423828
Step 17, loss: 192.01748657226562
Step 18, loss: 195.73239135742188
Step 19, loss: 198.99935913085938
Step 20, loss: 202.05027770996094
Step 21, loss: 204.4258270263672
Step 22, loss: 206.86767578125
Step 23, loss: 209.28880310058594
Step 24, loss: 211.47103881835938
Step 25, loss: 213.5729217529297
Step 26, loss: 215.53573608398438
Step 27, loss: 217.14450073242188
Step 28, loss: 218.69908142089844
Step 29, loss: 220.24807739257812
Step 30, loss: 221.7789306640625
Step 31, loss: 223.39198303222656
Step 32, loss: 224.8848876953125
Step 33, loss: 226.5763397216797
Step 34, loss: 228.46243286132812
Step 35, loss: 230.70455932617188
Step 36, loss: 233.16152954101562
Step 37, loss: 235.89430236816406
Step 38, loss: 239.29010009765625
Step 39, loss: 242.9391326904297
Step 40, loss: 247.1060791015625
Step 41, loss: 251.52334594726562
Step 42, loss: 256.14593505859375
Step 43, loss: 260.16046142578125
Step 44, loss: 263.3525390625
Step 45, loss: 265.65081787109375
Step 46, loss: 266.8549499511719
Step 47, loss: 267.10418701171875
Step 48, loss: 265.2027587890625
Step 49, loss: 260.8373718261719
Step 50, loss: 253.93563842773438
Step 51, loss: 244.00955200195312
Step 52, loss: 237.88339233398438
Step 53, loss: 233.2013702392578
Step 54, loss: 228.97772216796875
Step 55, loss: 224.49880981445312
Step 56, loss: 219.7165069580078
Step 57, loss: 215.13461303710938
Step 58, loss: 211.78733825683594
Step 59, loss: 210.65078735351562
Step 60, loss: 209.54844665527344
Step 61, loss: 208.506591796875
Step 62, loss: 207.3935546875
Step 63, loss: 206.38253784179688
Step 64, loss: 205.41311645507812
Step 65, loss: 204.35418701171875
Step 66, loss: 203.47462463378906
Step 67, loss: 202.60520935058594
Step 68, loss: 201.81553649902344
Step 69, loss: 200.8603057861328
Step 70, loss: 200.05332946777344
Step 71, loss: 199.35702514648438
Step 72, loss: 198.576904296875
Step 73, loss: 197.87606811523438
Step 74, loss: 197.18270874023438
Step 75, loss: 196.59127807617188
Step 76, loss: 196.06175231933594
Step 77, loss: 195.529052734375
Step 78, loss: 195.05392456054688
Step 79, loss: 194.6848602294922
Step 80, loss: 194.32835388183594
Step 81, loss: 194.05006408691406
Step 82, loss: 193.76040649414062
Step 83, loss: 193.6576690673828
Step 84, loss: 193.47711181640625
Step 85, loss: 193.41551208496094
Step 86, loss: 193.3182830810547
Step 87, loss: 193.1533966064453
Step 88, loss: 193.17044067382812
Step 89, loss: 193.0409698486328
Step 90, loss: 192.97201538085938
Step 91, loss: 192.9093017578125
Step 92, loss: 192.84132385253906
Step 93, loss: 192.81170654296875
Step 94, loss: 192.76443481445312
Step 95, loss: 192.7322540283203
Step 96, loss: 192.6651153564453
Step 97, loss: 192.60678100585938
Step 98, loss: 192.5071258544922
Step 99, loss: 192.48028564453125
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 0.49936282,  0.00424738,  0.86638255,  0.217434  ],
       [-0.86638195, -0.00259362,  0.49937519,  0.75507158],
       [ 0.00436811, -0.99998762,  0.00238469,  0.08463335],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.63883011,  0.20391115,  0.74183309,  0.18755001],
       [-0.15119811,  0.97870772, -0.13881764,  0.60471533],
       [-0.75434423, -0.02348287,  0.65605894,  0.31869697],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 0.5193521 ,  0.04208905,  0.85352323,  0.0446804 ],
       [-0.85358425, -0.02217416,  0.52048269,  0.75821329],
       [ 0.04083279, -0.99886777,  0.02441036,  0.16871203],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[ 0.5193521 ,  0.04208905,  0.85352323,  0.0446804 ],
       [-0.85358425, -0.02217416,  0.52048269,  0.75821329],
       [ 0.04083279, -0.99886777,  0.02441036,  0.16871203],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[ 0.33295441, -0.01763654,  0.94277811,  0.01784633],
       [-0.15471078,  0.98525405,  0.07306957,  1.27083788],
       [-0.93016452, -0.170187  ,  0.32531619, -0.17834928],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False

loading data:   0%|          | 0/5 [00:00<?, ?it/s]
loading data: 100%|██████████| 5/5 [00:00<00:00, 60.10it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/69 [00:00<?, ?it/s]
loading data:  10%|█         | 7/69 [00:00<00:00, 63.74it/s]
loading data:  20%|██        | 14/69 [00:00<00:00, 65.36it/s]
loading data:  30%|███       | 21/69 [00:00<00:00, 64.88it/s]
loading data:  41%|████      | 28/69 [00:00<00:00, 63.01it/s]
loading data:  51%|█████     | 35/69 [00:00<00:00, 63.36it/s]
loading data:  61%|██████    | 42/69 [00:00<00:00, 63.52it/s]
loading data:  71%|███████   | 49/69 [00:00<00:00, 63.34it/s]
loading data:  81%|████████  | 56/69 [00:00<00:00, 63.59it/s]
loading data:  91%|█████████▏| 63/69 [00:00<00:00, 64.22it/s]
loading data: 100%|██████████| 69/69 [00:01<00:00, 64.07it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.13619443774223328
Loss after optimization 0.10783272981643677
logm result may be inaccurate, approximate err = 8.940919001628595e-07
Loss at initial point 0.18436791002750397
Loss after optimization 0.14172014594078064
logm result may be inaccurate, approximate err = 5.273362902940592e-07
Loss at initial point 0.2576182782649994
Loss after optimization 0.037221360951662064
logm result may be inaccurate, approximate err = 6.088832324020161e-07
Loss at initial point 0.1373407244682312
Loss after optimization 0.10365132242441177
logm result may be inaccurate, approximate err = 3.846214078550909e-07
Loss at initial point 0.1907200813293457
Loss after optimization 0.05008348077535629
logm result may be inaccurate, approximate err = 5.182130227321769e-07
Loss at initial point 0.09968125820159912
Loss after optimization 0.047231629490852356
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.13611187040805817
Loss after optimization 0.04076775908470154
logm result may be inaccurate, approximate err = 8.940919001628595e-07
Loss at initial point 0.18429076671600342
Loss after optimization 0.15142720937728882
logm result may be inaccurate, approximate err = 5.273362902940592e-07
Loss at initial point 0.25762656331062317
Loss after optimization 0.1514272689819336
logm result may be inaccurate, approximate err = 6.088832324020161e-07
Loss at initial point 0.13730452954769135
Loss after optimization 0.035701122134923935
logm result may be inaccurate, approximate err = 3.846214078550909e-07
Loss at initial point 0.1907106190919876
Loss after optimization 0.1514272540807724
logm result may be inaccurate, approximate err = 5.182130227321769e-07
Loss at initial point 0.09965955466032028
Loss after optimization 0.03169981762766838
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.13617825508117676
Loss after optimization 0.011373051442205906
logm result may be inaccurate, approximate err = 8.940919001628595e-07
Loss at initial point 0.18436086177825928
Loss after optimization 0.1514272689819336
logm result may be inaccurate, approximate err = 5.273362902940592e-07
Loss at initial point 0.25753214955329895
Loss after optimization 0.1514272689819336
logm result may be inaccurate, approximate err = 6.088832324020161e-07
Loss at initial point 0.13728557527065277
Loss after optimization 0.011297715827822685
logm result may be inaccurate, approximate err = 3.846214078550909e-07
Loss at initial point 0.1907626986503601
Loss after optimization 0.1514272689819336
logm result may be inaccurate, approximate err = 5.182130227321769e-07
Loss at initial point 0.09962809085845947
Loss after optimization 0.015191309154033661
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 116.59620666503906
Step 1, loss: 108.26628875732422
Step 2, loss: 99.35358428955078
Step 3, loss: 89.20291900634766
Step 4, loss: 77.92212677001953
Step 5, loss: 75.4018325805664
Step 6, loss: 79.5741958618164
Step 7, loss: 81.84371185302734
Step 8, loss: 84.58319854736328
Step 9, loss: 86.4873275756836
Step 10, loss: 87.36314392089844
Step 11, loss: 88.62036895751953
Step 12, loss: 89.16221618652344
Step 13, loss: 89.38085174560547
Step 14, loss: 88.59077453613281
Step 15, loss: 87.68634796142578
Step 16, loss: 86.73416900634766
Step 17, loss: 86.37860870361328
Step 18, loss: 86.47462463378906
Step 19, loss: 87.25886535644531
Step 20, loss: 87.77098846435547
Step 21, loss: 87.8622055053711
Step 22, loss: 87.73907470703125
Step 23, loss: 87.45486450195312
Step 24, loss: 87.01258850097656
Step 25, loss: 86.82339477539062
Step 26, loss: 87.02323150634766
Step 27, loss: 87.17160034179688
Step 28, loss: 87.43067932128906
Step 29, loss: 87.4574203491211
Step 30, loss: 87.2364273071289
Step 31, loss: 86.81927490234375
Step 32, loss: 86.3379898071289
Step 33, loss: 85.98799896240234
Step 34, loss: 85.93302154541016
Step 35, loss: 86.0567855834961
Step 36, loss: 86.21122741699219
Step 37, loss: 86.36455535888672
Step 38, loss: 86.26716613769531
Step 39, loss: 85.93187713623047
Step 40, loss: 85.55597686767578
Step 41, loss: 85.0234603881836
Step 42, loss: 84.77214050292969
Step 43, loss: 84.38607025146484
Step 44, loss: 83.96013641357422
Step 45, loss: 83.49980163574219
Step 46, loss: 83.10393524169922
Step 47, loss: 82.69271850585938
Step 48, loss: 82.42578125
Step 49, loss: 82.3619384765625
Step 50, loss: 82.215576171875
Step 51, loss: 82.1253662109375
Step 52, loss: 82.0543212890625
Step 53, loss: 82.0345230102539
Step 54, loss: 81.90774536132812
Step 55, loss: 81.7274169921875
Step 56, loss: 81.25798034667969
Step 57, loss: 80.95586395263672
Step 58, loss: 80.67066192626953
Step 59, loss: 80.39161682128906
Step 60, loss: 80.30485534667969
Step 61, loss: 80.19688415527344
Step 62, loss: 80.03365325927734
Step 63, loss: 79.538330078125
Step 64, loss: 79.07591247558594
Step 65, loss: 78.61540985107422
Step 66, loss: 78.12985229492188
Step 67, loss: 77.71898651123047
Step 68, loss: 77.55268859863281
Step 69, loss: 77.49522399902344
Step 70, loss: 77.32598876953125
Step 71, loss: 77.05844116210938
Step 72, loss: 76.46176147460938
Step 73, loss: 76.00407409667969
Step 74, loss: 75.70230102539062
Step 75, loss: 75.66114807128906
Step 76, loss: 75.75447082519531
Step 77, loss: 75.58848571777344
Step 78, loss: 75.0915298461914
Step 79, loss: 74.67147827148438
Step 80, loss: 74.45213317871094
Step 81, loss: 74.31356811523438
Step 82, loss: 73.90916442871094
Step 83, loss: 73.4162368774414
Step 84, loss: 73.46833038330078
Step 85, loss: 72.98467254638672
Step 86, loss: 71.3094253540039
Step 87, loss: 69.30084991455078
Step 88, loss: 72.39730834960938
Step 89, loss: 71.5924301147461
Step 90, loss: 69.60564422607422
Step 91, loss: 67.91328430175781
Step 92, loss: 66.8539810180664
Step 93, loss: 66.52951049804688
Step 94, loss: 66.87026977539062
Step 95, loss: 66.91856384277344
Step 96, loss: 65.66975402832031
Step 97, loss: 64.35662078857422
Step 98, loss: 63.77342224121094
Step 99, loss: 63.548072814941406
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 116.60031127929688
Step 1, loss: 108.23206329345703
Step 2, loss: 99.344482421875
Step 3, loss: 89.20419311523438
Step 4, loss: 77.96473693847656
Step 5, loss: 75.33866882324219
Step 6, loss: 79.57300567626953
Step 7, loss: 81.7884750366211
Step 8, loss: 84.5652084350586
Step 9, loss: 86.52122497558594
Step 10, loss: 87.42900848388672
Step 11, loss: 88.70153045654297
Step 12, loss: 89.27978515625
Step 13, loss: 89.4170913696289
Step 14, loss: 88.65813446044922
Step 15, loss: 87.66375732421875
Step 16, loss: 86.79818725585938
Step 17, loss: 86.48007202148438
Step 18, loss: 86.6396255493164
Step 19, loss: 87.2608413696289
Step 20, loss: 87.81095886230469
Step 21, loss: 87.92244720458984
Step 22, loss: 87.83299255371094
Step 23, loss: 87.5169677734375
Step 24, loss: 87.0946273803711
Step 25, loss: 86.88188171386719
Step 26, loss: 86.9395523071289
Step 27, loss: 87.1618423461914
Step 28, loss: 87.41131591796875
Step 29, loss: 87.3772201538086
Step 30, loss: 87.06593322753906
Step 31, loss: 86.8124008178711
Step 32, loss: 86.27462005615234
Step 33, loss: 85.87158966064453
Step 34, loss: 85.89796447753906
Step 35, loss: 86.10404205322266
Step 36, loss: 86.28997802734375
Step 37, loss: 86.432861328125
Step 38, loss: 86.50469207763672
Step 39, loss: 86.34745788574219
Step 40, loss: 85.86431884765625
Step 41, loss: 85.38622283935547
Step 42, loss: 84.94025421142578
Step 43, loss: 84.53382873535156
Step 44, loss: 84.10782623291016
Step 45, loss: 83.6800537109375
Step 46, loss: 83.30667877197266
Step 47, loss: 82.94761657714844
Step 48, loss: 82.7327651977539
Step 49, loss: 82.53931427001953
Step 50, loss: 82.38409423828125
Step 51, loss: 82.24261474609375
Step 52, loss: 82.14400482177734
Step 53, loss: 82.01091003417969
Step 54, loss: 81.90913391113281
Step 55, loss: 81.81700897216797
Step 56, loss: 81.40491485595703
Step 57, loss: 81.03096008300781
Step 58, loss: 80.78721618652344
Step 59, loss: 80.54802703857422
Step 60, loss: 80.42036437988281
Step 61, loss: 80.26347351074219
Step 62, loss: 80.13627624511719
Step 63, loss: 79.79472351074219
Step 64, loss: 79.36243438720703
Step 65, loss: 78.9580307006836
Step 66, loss: 78.63336944580078
Step 67, loss: 78.37979888916016
Step 68, loss: 78.23546600341797
Step 69, loss: 78.10326385498047
Step 70, loss: 77.85281372070312
Step 71, loss: 77.54790496826172
Step 72, loss: 76.9166259765625
Step 73, loss: 76.47124481201172
Step 74, loss: 76.15887451171875
Step 75, loss: 75.87552642822266
Step 76, loss: 75.73939514160156
Step 77, loss: 75.38520812988281
Step 78, loss: 75.00796508789062
Step 79, loss: 74.53522491455078
Step 80, loss: 73.95720672607422
Step 81, loss: 73.50599670410156
Step 82, loss: 73.15794372558594
Step 83, loss: 72.82366943359375
Step 84, loss: 72.41416931152344
Step 85, loss: 71.77214813232422
Step 86, loss: 71.04972839355469
Step 87, loss: 70.55500030517578
Step 88, loss: 70.22134399414062
Step 89, loss: 69.80708312988281
Step 90, loss: 69.49234008789062
Step 91, loss: 68.85079956054688
Step 92, loss: 68.02957916259766
Step 93, loss: 67.12635803222656
Step 94, loss: 66.3465347290039
Step 95, loss: 65.6839828491211
Step 96, loss: 65.17872619628906
Step 97, loss: 64.71784973144531
Step 98, loss: 63.90415954589844
Step 99, loss: 62.6659049987793
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 116.5828857421875
Step 1, loss: 108.24996948242188
Step 2, loss: 99.36576843261719
Step 3, loss: 89.1185531616211
Step 4, loss: 77.8909912109375
Step 5, loss: 75.32586669921875
Step 6, loss: 79.59123229980469
Step 7, loss: 81.81002044677734
Step 8, loss: 84.58419799804688
Step 9, loss: 86.52035522460938
Step 10, loss: 87.39149475097656
Step 11, loss: 88.64282989501953
Step 12, loss: 89.16879272460938
Step 13, loss: 89.35942077636719
Step 14, loss: 88.67131805419922
Step 15, loss: 87.78600311279297
Step 16, loss: 86.8189697265625
Step 17, loss: 86.4817123413086
Step 18, loss: 86.5023422241211
Step 19, loss: 87.24463653564453
Step 20, loss: 87.69681549072266
Step 21, loss: 87.85611724853516
Step 22, loss: 87.7678451538086
Step 23, loss: 87.51284790039062
Step 24, loss: 87.01250457763672
Step 25, loss: 86.8226547241211
Step 26, loss: 86.99420166015625
Step 27, loss: 87.14017486572266
Step 28, loss: 87.419189453125
Step 29, loss: 87.39044952392578
Step 30, loss: 87.1722183227539
Step 31, loss: 86.7758560180664
Step 32, loss: 86.33668518066406
Step 33, loss: 85.95160675048828
Step 34, loss: 85.90083312988281
Step 35, loss: 85.97691345214844
Step 36, loss: 86.1183090209961
Step 37, loss: 86.14442443847656
Step 38, loss: 86.0719223022461
Step 39, loss: 85.84492492675781
Step 40, loss: 85.45565032958984
Step 41, loss: 84.98416900634766
Step 42, loss: 84.60392761230469
Step 43, loss: 84.21682739257812
Step 44, loss: 83.60926055908203
Step 45, loss: 83.12814331054688
Step 46, loss: 82.76102447509766
Step 47, loss: 82.4145278930664
Step 48, loss: 82.21865844726562
Step 49, loss: 82.16559600830078
Step 50, loss: 82.0078353881836
Step 51, loss: 81.96348571777344
Step 52, loss: 81.8948745727539
Step 53, loss: 81.83081817626953
Step 54, loss: 81.63211059570312
Step 55, loss: 81.25140380859375
Step 56, loss: 80.904296875
Step 57, loss: 80.6250228881836
Step 58, loss: 80.24626159667969
Step 59, loss: 79.97332763671875
Step 60, loss: 79.77519226074219
Step 61, loss: 79.70377349853516
Step 62, loss: 79.50028228759766
Step 63, loss: 79.05485534667969
Step 64, loss: 78.55982208251953
Step 65, loss: 78.0696792602539
Step 66, loss: 77.60014343261719
Step 67, loss: 77.26729583740234
Step 68, loss: 77.08052825927734
Step 69, loss: 77.03924560546875
Step 70, loss: 76.82051086425781
Step 71, loss: 76.51570892333984
Step 72, loss: 76.27770233154297
Step 73, loss: 76.1505126953125
Step 74, loss: 75.94549560546875
Step 75, loss: 75.77623748779297
Step 76, loss: 75.97527313232422
Step 77, loss: 76.24378204345703
Step 78, loss: 76.1626968383789
Step 79, loss: 75.84210968017578
Step 80, loss: 75.65406036376953
Step 81, loss: 75.67825317382812
Step 82, loss: 75.9184341430664
Step 83, loss: 76.31085205078125
Step 84, loss: 76.37443542480469
Step 85, loss: 76.14824676513672
Step 86, loss: 75.99226379394531
Step 87, loss: 75.83938598632812
Step 88, loss: 75.65286254882812
Step 89, loss: 75.51657104492188
Step 90, loss: 75.34544372558594
Step 91, loss: 75.17577362060547
Step 92, loss: 74.95819091796875
Step 93, loss: 74.6093521118164
Step 94, loss: 74.19127655029297
Step 95, loss: 73.8376235961914
Step 96, loss: 73.2526626586914
Step 97, loss: 72.84581756591797
Step 98, loss: 71.29621887207031
Step 99, loss: 69.53852081298828
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 116.62201690673828
Step 1, loss: 108.27523803710938
Step 2, loss: 99.3551254272461
Step 3, loss: 89.16268157958984
Step 4, loss: 77.94212341308594
Step 5, loss: 75.34394073486328
Step 6, loss: 79.61186981201172
Step 7, loss: 81.7780990600586
Step 8, loss: 84.56336975097656
Step 9, loss: 86.4940185546875
Step 10, loss: 87.45662689208984
Step 11, loss: 88.59074401855469
Step 12, loss: 89.13174438476562
Step 13, loss: 89.29209899902344
Step 14, loss: 88.52141571044922
Step 15, loss: 87.71382904052734
Step 16, loss: 86.75438690185547
Step 17, loss: 86.4806900024414
Step 18, loss: 86.49925231933594
Step 19, loss: 87.42507934570312
Step 20, loss: 87.84780883789062
Step 21, loss: 88.06564331054688
Step 22, loss: 87.8912124633789
Step 23, loss: 87.64862060546875
Step 24, loss: 87.2436752319336
Step 25, loss: 86.99579620361328
Step 26, loss: 87.0558853149414
Step 27, loss: 87.22272491455078
Step 28, loss: 87.50464630126953
Step 29, loss: 87.54630279541016
Step 30, loss: 87.28684997558594
Step 31, loss: 86.95651245117188
Step 32, loss: 86.40206146240234
Step 33, loss: 85.93671417236328
Step 34, loss: 85.78679656982422
Step 35, loss: 85.88887023925781
Step 36, loss: 86.18382263183594
Step 37, loss: 86.40970611572266
Step 38, loss: 86.49981689453125
Step 39, loss: 86.40261840820312
Step 40, loss: 86.09284973144531
Step 41, loss: 85.64226531982422
Step 42, loss: 85.05782318115234
Step 43, loss: 84.77737426757812
Step 44, loss: 84.23173522949219
Step 45, loss: 83.69519805908203
Step 46, loss: 83.22660827636719
Step 47, loss: 82.8839340209961
Step 48, loss: 82.62515258789062
Step 49, loss: 82.46597290039062
Step 50, loss: 82.32403564453125
Step 51, loss: 82.14861297607422
Step 52, loss: 82.06814575195312
Step 53, loss: 81.98348236083984
Step 54, loss: 81.84549713134766
Step 55, loss: 81.74226379394531
Step 56, loss: 81.31969451904297
Step 57, loss: 80.89937591552734
Step 58, loss: 80.68917083740234
Step 59, loss: 80.4566650390625
Step 60, loss: 80.25650787353516
Step 61, loss: 80.22674560546875
Step 62, loss: 80.08210754394531
Step 63, loss: 79.7261962890625
Step 64, loss: 79.25271606445312
Step 65, loss: 78.83268737792969
Step 66, loss: 78.41802978515625
Step 67, loss: 78.23019409179688
Step 68, loss: 78.09577941894531
Step 69, loss: 77.92018127441406
Step 70, loss: 77.80779266357422
Step 71, loss: 77.45845794677734
Step 72, loss: 76.9424819946289
Step 73, loss: 76.5606460571289
Step 74, loss: 76.29255676269531
Step 75, loss: 76.10039520263672
Step 76, loss: 75.94332885742188
Step 77, loss: 75.83226013183594
Step 78, loss: 75.63457489013672
Step 79, loss: 75.29875946044922
Step 80, loss: 74.80178833007812
Step 81, loss: 74.47852325439453
Step 82, loss: 74.06507873535156
Step 83, loss: 73.6462173461914
Step 84, loss: 73.31298828125
Step 85, loss: 72.79196166992188
Step 86, loss: 72.40013885498047
Step 87, loss: 71.85633850097656
Step 88, loss: 71.41531372070312
Step 89, loss: 70.98253631591797
Step 90, loss: 70.53990936279297
Step 91, loss: 70.00796508789062
Step 92, loss: 69.43898010253906
Step 93, loss: 68.90020751953125
Step 94, loss: 68.55294799804688
Step 95, loss: 67.98192596435547
Step 96, loss: 67.41056060791016
Step 97, loss: 66.58544921875
Step 98, loss: 66.09317016601562
Step 99, loss: 65.64802551269531
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 116.61734771728516
Step 1, loss: 108.24818420410156
Step 2, loss: 99.36445617675781
Step 3, loss: 89.12814331054688
Step 4, loss: 77.93199157714844
Step 5, loss: 75.42105865478516
Step 6, loss: 79.5709457397461
Step 7, loss: 81.81661224365234
Step 8, loss: 84.58094024658203
Step 9, loss: 86.49115753173828
Step 10, loss: 87.39889526367188
Step 11, loss: 88.6484146118164
Step 12, loss: 89.2746353149414
Step 13, loss: 89.41287231445312
Step 14, loss: 88.66758728027344
Step 15, loss: 87.66038513183594
Step 16, loss: 86.70646667480469
Step 17, loss: 86.38837432861328
Step 18, loss: 86.44071960449219
Step 19, loss: 87.30310821533203
Step 20, loss: 87.74757385253906
Step 21, loss: 87.96778106689453
Step 22, loss: 87.84722137451172
Step 23, loss: 87.54186248779297
Step 24, loss: 87.12882232666016
Step 25, loss: 86.95146942138672
Step 26, loss: 87.0584487915039
Step 27, loss: 87.2607421875
Step 28, loss: 87.51390838623047
Step 29, loss: 87.4174575805664
Step 30, loss: 87.17347717285156
Step 31, loss: 86.85550689697266
Step 32, loss: 86.3493881225586
Step 33, loss: 85.90399169921875
Step 34, loss: 85.8685302734375
Step 35, loss: 86.03641510009766
Step 36, loss: 86.28284454345703
Step 37, loss: 86.45562744140625
Step 38, loss: 86.50959777832031
Step 39, loss: 86.39010620117188
Step 40, loss: 85.93827056884766
Step 41, loss: 85.43061065673828
Step 42, loss: 84.97210693359375
Step 43, loss: 84.5794677734375
Step 44, loss: 84.12195587158203
Step 45, loss: 83.64964294433594
Step 46, loss: 83.29934692382812
Step 47, loss: 82.9656753540039
Step 48, loss: 82.68855285644531
Step 49, loss: 82.51888275146484
Step 50, loss: 82.38705444335938
Step 51, loss: 82.21698760986328
Step 52, loss: 82.14221954345703
Step 53, loss: 82.05500793457031
Step 54, loss: 81.87911224365234
Step 55, loss: 81.75251770019531
Step 56, loss: 81.42086029052734
Step 57, loss: 80.98233032226562
Step 58, loss: 80.7344970703125
Step 59, loss: 80.5133285522461
Step 60, loss: 80.29130554199219
Step 61, loss: 80.16635131835938
Step 62, loss: 80.0252914428711
Step 63, loss: 79.72045135498047
Step 64, loss: 79.3343734741211
Step 65, loss: 78.84425354003906
Step 66, loss: 78.40084075927734
Step 67, loss: 78.12667846679688
Step 68, loss: 77.81624603271484
Step 69, loss: 77.70243835449219
Step 70, loss: 77.38359069824219
Step 71, loss: 77.06390380859375
Step 72, loss: 76.63259887695312
Step 73, loss: 76.28775024414062
Step 74, loss: 75.97366333007812
Step 75, loss: 75.7553939819336
Step 76, loss: 75.69803619384766
Step 77, loss: 75.58921813964844
Step 78, loss: 75.20723724365234
Step 79, loss: 74.7128677368164
Step 80, loss: 74.29161071777344
Step 81, loss: 73.87675476074219
Step 82, loss: 73.53313446044922
Step 83, loss: 73.01558685302734
Step 84, loss: 72.71282196044922
Step 85, loss: 72.51631927490234
Step 86, loss: 72.12986755371094
Step 87, loss: 71.72926330566406
Step 88, loss: 71.28594970703125
Step 89, loss: 70.80940246582031
Step 90, loss: 70.35343170166016
Step 91, loss: 69.9438247680664
Step 92, loss: 69.55485534667969
Step 93, loss: 69.11119842529297
Step 94, loss: 68.7697525024414
Step 95, loss: 68.13957214355469
Step 96, loss: 67.31825256347656
Step 97, loss: 66.45960235595703
Step 98, loss: 65.71873474121094
Step 99, loss: 65.2636489868164
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 116.5771255493164
Step 1, loss: 108.29193878173828
Step 2, loss: 99.35784149169922
Step 3, loss: 89.12815856933594
Step 4, loss: 77.84330749511719
Step 5, loss: 75.22115325927734
Step 6, loss: 79.48178100585938
Step 7, loss: 81.64102172851562
Step 8, loss: 84.46260833740234
Step 9, loss: 86.42745971679688
Step 10, loss: 87.30364990234375
Step 11, loss: 88.45759582519531
Step 12, loss: 89.0096206665039
Step 13, loss: 89.09205627441406
Step 14, loss: 88.37535858154297
Step 15, loss: 87.38499450683594
Step 16, loss: 86.54550170898438
Step 17, loss: 86.14013671875
Step 18, loss: 86.40814208984375
Step 19, loss: 87.08100128173828
Step 20, loss: 87.70606994628906
Step 21, loss: 87.73522186279297
Step 22, loss: 87.74079895019531
Step 23, loss: 87.47593688964844
Step 24, loss: 86.9868392944336
Step 25, loss: 86.67176055908203
Step 26, loss: 86.74376678466797
Step 27, loss: 86.97052764892578
Step 28, loss: 87.22835540771484
Step 29, loss: 87.3457260131836
Step 30, loss: 87.1717529296875
Step 31, loss: 86.9075698852539
Step 32, loss: 86.449462890625
Step 33, loss: 85.95454406738281
Step 34, loss: 85.7999496459961
Step 35, loss: 85.83506774902344
Step 36, loss: 85.92918395996094
Step 37, loss: 86.03502655029297
Step 38, loss: 86.0092544555664
Step 39, loss: 85.76453399658203
Step 40, loss: 85.46266174316406
Step 41, loss: 84.98529052734375
Step 42, loss: 84.66887664794922
Step 43, loss: 84.34965515136719
Step 44, loss: 83.90556335449219
Step 45, loss: 83.43408966064453
Step 46, loss: 83.0183334350586
Step 47, loss: 82.59429168701172
Step 48, loss: 82.34528350830078
Step 49, loss: 82.23690032958984
Step 50, loss: 82.12858581542969
Step 51, loss: 82.0632553100586
Step 52, loss: 82.01431274414062
Step 53, loss: 81.95001983642578
Step 54, loss: 81.88465881347656
Step 55, loss: 81.520263671875
Step 56, loss: 81.03923797607422
Step 57, loss: 80.74984741210938
Step 58, loss: 80.43010711669922
Step 59, loss: 80.14537048339844
Step 60, loss: 79.97647094726562
Step 61, loss: 79.9476318359375
Step 62, loss: 79.80361938476562
Step 63, loss: 79.44844818115234
Step 64, loss: 79.04864501953125
Step 65, loss: 78.65084075927734
Step 66, loss: 78.1884765625
Step 67, loss: 77.90906524658203
Step 68, loss: 77.79366302490234
Step 69, loss: 77.87702178955078
Step 70, loss: 77.81674194335938
Step 71, loss: 77.6216812133789
Step 72, loss: 77.15105438232422
Step 73, loss: 76.67804718017578
Step 74, loss: 76.3863525390625
Step 75, loss: 76.39190673828125
Step 76, loss: 76.52804565429688
Step 77, loss: 76.58255767822266
Step 78, loss: 76.28351593017578
Step 79, loss: 75.86825561523438
Step 80, loss: 75.8203353881836
Step 81, loss: 75.77615356445312
Step 82, loss: 75.86661529541016
Step 83, loss: 75.84728240966797
Step 84, loss: 75.70442962646484
Step 85, loss: 75.55538177490234
Step 86, loss: 75.61671447753906
Step 87, loss: 75.69795227050781
Step 88, loss: 75.69227600097656
Step 89, loss: 75.40943908691406
Step 90, loss: 75.03019714355469
Step 91, loss: 74.701171875
Step 92, loss: 74.72061157226562
Step 93, loss: 74.78473663330078
Step 94, loss: 74.2529525756836
Step 95, loss: 73.6235580444336
Step 96, loss: 73.39917755126953
Step 97, loss: 73.2186508178711
Step 98, loss: 71.76649475097656
Step 99, loss: 69.86853790283203
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 9.99796825e-01, -2.01529994e-02,  4.05883631e-04,
        -3.60268044e-02],
       [-3.49461375e-04, -3.74628632e-02, -9.99297959e-01,
         3.46036345e-01],
       [ 2.01540567e-02,  9.99094786e-01, -3.74622943e-02,
         1.39376348e-01],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'COBYLA': array([[ 0.96821221, -0.08239805,  0.23616876,  0.02247801],
       [-0.19883523,  0.31929973,  0.92655935,  0.82406734],
       [-0.15175531, -0.94406475,  0.29276626,  0.1888551 ],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 0.6144903 , -0.03802644,  0.7880074 ,  0.03588443],
       [-0.78579286,  0.05940625,  0.61563014,  0.76161689],
       [-0.07022279, -0.99750934,  0.00662366,  0.17206194],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[ 0.6144903 , -0.03802644,  0.7880074 ,  0.03588443],
       [-0.78579286,  0.05940625,  0.61563014,  0.76161689],
       [-0.07022279, -0.99750934,  0.00662366,  0.17206194],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[ 0.46181887, -0.12153208,  0.87860835,  0.03569011],
       [-0.88428789, -0.14012352,  0.44542167,  0.86484955],
       [ 0.06898072, -0.98264706, -0.1721811 ,  0.09206118],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False

loading data:   0%|          | 0/5 [00:00<?, ?it/s]
loading data: 100%|██████████| 5/5 [00:00<00:00, 55.60it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/69 [00:00<?, ?it/s]
loading data:  10%|█         | 7/69 [00:00<00:00, 62.54it/s]
loading data:  20%|██        | 14/69 [00:00<00:00, 65.25it/s]
loading data:  30%|███       | 21/69 [00:00<00:00, 66.46it/s]
loading data:  41%|████      | 28/69 [00:00<00:00, 65.61it/s]
loading data:  51%|█████     | 35/69 [00:00<00:00, 64.74it/s]
loading data:  61%|██████    | 42/69 [00:00<00:00, 64.76it/s]
loading data:  71%|███████   | 49/69 [00:00<00:00, 64.43it/s]
loading data:  81%|████████  | 56/69 [00:00<00:00, 64.52it/s]
loading data:  91%|█████████▏| 63/69 [00:00<00:00, 65.31it/s]
loading data: 100%|██████████| 69/69 [00:01<00:00, 65.07it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(p, device='cuda', dtype=torch.float32)
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.46098559722304344
Loss after optimization 0.38788386806845665
logm result may be inaccurate, approximate err = 5.512988965006467e-07
Loss at initial point 0.5149700343608856
Loss after optimization 0.1976999007165432
logm result may be inaccurate, approximate err = 7.569166025103958e-07
Loss at initial point 0.4180767387151718
Loss after optimization 0.09620143473148346
logm result may be inaccurate, approximate err = 5.822843093851032e-07
Loss at initial point 1.1089305132627487
Loss after optimization 0.14408345893025398
logm result may be inaccurate, approximate err = 5.511813149174307e-07
Loss at initial point 0.4949596896767616
Loss after optimization 0.43793370202183723
logm result may be inaccurate, approximate err = 1.0307275388563472e-06
Loss at initial point 0.30675259232521057
Loss after optimization 0.11936094239354134
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.4609505645930767
Loss after optimization 0.0952109806239605
logm result may be inaccurate, approximate err = 5.512988965006467e-07
Loss at initial point 0.514999732375145
Loss after optimization 0.11964979767799377
logm result may be inaccurate, approximate err = 7.569166025103958e-07
Loss at initial point 0.4181699603796005
Loss after optimization 0.12214955501258373
logm result may be inaccurate, approximate err = 5.822843093851032e-07
Loss at initial point 1.1090152859687805
Loss after optimization 0.518452413380146
logm result may be inaccurate, approximate err = 5.511813149174307e-07
Loss at initial point 0.49503474682569504
Loss after optimization 0.09684671275317669
logm result may be inaccurate, approximate err = 1.0307275388563472e-06
Loss at initial point 0.30684253945946693
Loss after optimization 0.0976096885278821
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.4609505720436573
Loss after optimization 0.09276963956654072
logm result may be inaccurate, approximate err = 5.512988965006467e-07
Loss at initial point 0.5147687718272209
Loss after optimization 0.09321041218936443
logm result may be inaccurate, approximate err = 7.569166025103958e-07
Loss at initial point 0.41808805614709854
Loss after optimization 0.09275235049426556
logm result may be inaccurate, approximate err = 5.822843093851032e-07
Loss at initial point 1.1090695559978485
Loss after optimization 0.518452413380146
logm result may be inaccurate, approximate err = 5.511813149174307e-07
Loss at initial point 0.4947848543524742
Loss after optimization 0.0937572680413723
logm result may be inaccurate, approximate err = 1.0307275388563472e-06
Loss at initial point 0.306726798415184
Loss after optimization 0.09268399141728878
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 408.65155029296875
Step 1, loss: 360.5270080566406
Step 2, loss: 310.75958251953125
Step 3, loss: 253.191650390625
Step 4, loss: 196.1541748046875
Step 5, loss: 153.07936096191406
Step 6, loss: 198.65240478515625
Step 7, loss: 237.32418823242188
Step 8, loss: 262.17657470703125
Step 9, loss: 276.4324951171875
Step 10, loss: 284.3260192871094
Step 11, loss: 287.3766784667969
Step 12, loss: 286.15911865234375
Step 13, loss: 280.81585693359375
Step 14, loss: 273.91748046875
Step 15, loss: 265.6717224121094
Step 16, loss: 257.15008544921875
Step 17, loss: 245.9019012451172
Step 18, loss: 232.14950561523438
Step 19, loss: 214.50462341308594
Step 20, loss: 196.77883911132812
Step 21, loss: 188.74563598632812
Step 22, loss: 189.185302734375
Step 23, loss: 189.12136840820312
Step 24, loss: 186.44601440429688
Step 25, loss: 182.35440063476562
Step 26, loss: 179.0133514404297
Step 27, loss: 181.4300994873047
Step 28, loss: 188.20423889160156
Step 29, loss: 196.42111206054688
Step 30, loss: 204.76397705078125
Step 31, loss: 212.62252807617188
Step 32, loss: 218.60874938964844
Step 33, loss: 222.22793579101562
Step 34, loss: 223.76686096191406
Step 35, loss: 222.21292114257812
Step 36, loss: 217.99388122558594
Step 37, loss: 210.71360778808594
Step 38, loss: 201.32421875
Step 39, loss: 191.9532928466797
Step 40, loss: 179.7997589111328
Step 41, loss: 167.77969360351562
Step 42, loss: 161.0017547607422
Step 43, loss: 160.3391876220703
Step 44, loss: 159.2436065673828
Step 45, loss: 155.0694122314453
Step 46, loss: 149.859130859375
Step 47, loss: 151.2437286376953
Step 48, loss: 158.29556274414062
Step 49, loss: 168.09274291992188
Step 50, loss: 176.75851440429688
Step 51, loss: 181.2041778564453
Step 52, loss: 178.07659912109375
Step 53, loss: 167.00250244140625
Step 54, loss: 153.15997314453125
Step 55, loss: 146.8896942138672
Step 56, loss: 145.13302612304688
Step 57, loss: 144.52488708496094
Step 58, loss: 145.00953674316406
Step 59, loss: 146.84608459472656
Step 60, loss: 150.13995361328125
Step 61, loss: 152.5053253173828
Step 62, loss: 152.98605346679688
Step 63, loss: 150.54849243164062
Step 64, loss: 145.9153594970703
Step 65, loss: 142.1003875732422
Step 66, loss: 139.93505859375
Step 67, loss: 143.2489013671875
Step 68, loss: 149.57977294921875
Step 69, loss: 154.73146057128906
Step 70, loss: 154.6228790283203
Step 71, loss: 151.18942260742188
Step 72, loss: 146.89028930664062
Step 73, loss: 143.3472442626953
Step 74, loss: 143.32757568359375
Step 75, loss: 145.7287139892578
Step 76, loss: 146.6513671875
Step 77, loss: 145.69041442871094
Step 78, loss: 143.96661376953125
Step 79, loss: 142.45162963867188
Step 80, loss: 143.29513549804688
Step 81, loss: 146.26712036132812
Step 82, loss: 148.86306762695312
Step 83, loss: 150.56005859375
Step 84, loss: 150.0401611328125
Step 85, loss: 148.0869140625
Step 86, loss: 146.33834838867188
Step 87, loss: 146.07589721679688
Step 88, loss: 146.50193786621094
Step 89, loss: 147.60018920898438
Step 90, loss: 147.2046356201172
Step 91, loss: 145.9729461669922
Step 92, loss: 145.59841918945312
Step 93, loss: 145.68057250976562
Step 94, loss: 146.55299377441406
Step 95, loss: 147.15414428710938
Step 96, loss: 147.2792205810547
Step 97, loss: 147.40171813964844
Step 98, loss: 147.56846618652344
Step 99, loss: 147.83370971679688
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 408.7186279296875
Step 1, loss: 360.497314453125
Step 2, loss: 310.993896484375
Step 3, loss: 253.21231079101562
Step 4, loss: 196.5595245361328
Step 5, loss: 153.35768127441406
Step 6, loss: 198.91299438476562
Step 7, loss: 237.7154541015625
Step 8, loss: 262.5978088378906
Step 9, loss: 276.6709289550781
Step 10, loss: 284.4658203125
Step 11, loss: 287.593017578125
Step 12, loss: 286.1143493652344
Step 13, loss: 280.78729248046875
Step 14, loss: 274.07867431640625
Step 15, loss: 265.9980773925781
Step 16, loss: 257.8072814941406
Step 17, loss: 246.93934631347656
Step 18, loss: 233.52871704101562
Step 19, loss: 216.5958251953125
Step 20, loss: 198.6427001953125
Step 21, loss: 190.01058959960938
Step 22, loss: 189.86000061035156
Step 23, loss: 190.26475524902344
Step 24, loss: 188.151123046875
Step 25, loss: 184.39584350585938
Step 26, loss: 181.1242218017578
Step 27, loss: 182.42578125
Step 28, loss: 188.91302490234375
Step 29, loss: 197.01943969726562
Step 30, loss: 204.49928283691406
Step 31, loss: 211.8137664794922
Step 32, loss: 217.5667724609375
Step 33, loss: 221.422119140625
Step 34, loss: 222.72018432617188
Step 35, loss: 221.20590209960938
Step 36, loss: 217.42703247070312
Step 37, loss: 210.6859130859375
Step 38, loss: 201.499755859375
Step 39, loss: 191.7720489501953
Step 40, loss: 180.24424743652344
Step 41, loss: 168.88255310058594
Step 42, loss: 161.35086059570312
Step 43, loss: 159.39349365234375
Step 44, loss: 158.40797424316406
Step 45, loss: 154.51742553710938
Step 46, loss: 149.84414672851562
Step 47, loss: 151.14669799804688
Step 48, loss: 156.82598876953125
Step 49, loss: 165.98623657226562
Step 50, loss: 174.51708984375
Step 51, loss: 178.7952880859375
Step 52, loss: 177.06082153320312
Step 53, loss: 169.19918823242188
Step 54, loss: 156.49974060058594
Step 55, loss: 149.7086944580078
Step 56, loss: 148.62086486816406
Step 57, loss: 149.159423828125
Step 58, loss: 151.12298583984375
Step 59, loss: 153.06704711914062
Step 60, loss: 152.69369506835938
Step 61, loss: 149.46485900878906
Step 62, loss: 146.1458740234375
Step 63, loss: 143.43173217773438
Step 64, loss: 144.29293823242188
Step 65, loss: 147.19012451171875
Step 66, loss: 150.44577026367188
Step 67, loss: 150.28700256347656
Step 68, loss: 147.29559326171875
Step 69, loss: 143.76361083984375
Step 70, loss: 142.40432739257812
Step 71, loss: 143.10650634765625
Step 72, loss: 146.0105438232422
Step 73, loss: 146.5347900390625
Step 74, loss: 143.83685302734375
Step 75, loss: 140.44313049316406
Step 76, loss: 139.6985626220703
Step 77, loss: 141.70150756835938
Step 78, loss: 145.57606506347656
Step 79, loss: 149.416259765625
Step 80, loss: 149.040283203125
Step 81, loss: 145.90890502929688
Step 82, loss: 143.42881774902344
Step 83, loss: 142.7489471435547
Step 84, loss: 143.1326904296875
Step 85, loss: 144.32957458496094
Step 86, loss: 146.72732543945312
Step 87, loss: 147.90695190429688
Step 88, loss: 145.99429321289062
Step 89, loss: 144.86166381835938
Step 90, loss: 144.55047607421875
Step 91, loss: 145.23291015625
Step 92, loss: 146.42129516601562
Step 93, loss: 148.3411865234375
Step 94, loss: 149.8098602294922
Step 95, loss: 148.31353759765625
Step 96, loss: 145.4653778076172
Step 97, loss: 143.20346069335938
Step 98, loss: 143.3800811767578
Step 99, loss: 145.4514617919922
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 408.6468505859375
Step 1, loss: 360.62054443359375
Step 2, loss: 310.7921142578125
Step 3, loss: 253.4807586669922
Step 4, loss: 196.67892456054688
Step 5, loss: 153.17282104492188
Step 6, loss: 199.1016387939453
Step 7, loss: 237.66586303710938
Step 8, loss: 262.312255859375
Step 9, loss: 276.54736328125
Step 10, loss: 284.0334167480469
Step 11, loss: 287.56158447265625
Step 12, loss: 286.1197509765625
Step 13, loss: 281.0403747558594
Step 14, loss: 274.3271484375
Step 15, loss: 266.205810546875
Step 16, loss: 257.5478210449219
Step 17, loss: 246.97361755371094
Step 18, loss: 233.88523864746094
Step 19, loss: 217.1636962890625
Step 20, loss: 198.87619018554688
Step 21, loss: 189.9085235595703
Step 22, loss: 190.1714324951172
Step 23, loss: 190.51089477539062
Step 24, loss: 188.138427734375
Step 25, loss: 183.24317932128906
Step 26, loss: 178.77505493164062
Step 27, loss: 179.80039978027344
Step 28, loss: 186.8379364013672
Step 29, loss: 195.9856414794922
Step 30, loss: 204.43405151367188
Step 31, loss: 212.74664306640625
Step 32, loss: 218.7404327392578
Step 33, loss: 222.722412109375
Step 34, loss: 224.22752380371094
Step 35, loss: 222.4873046875
Step 36, loss: 218.59463500976562
Step 37, loss: 211.73910522460938
Step 38, loss: 202.05609130859375
Step 39, loss: 191.4264678955078
Step 40, loss: 179.086181640625
Step 41, loss: 165.67935180664062
Step 42, loss: 156.62496948242188
Step 43, loss: 155.2041473388672
Step 44, loss: 156.4776153564453
Step 45, loss: 154.6998291015625
Step 46, loss: 151.5538787841797
Step 47, loss: 154.2465057373047
Step 48, loss: 161.38333129882812
Step 49, loss: 170.6442108154297
Step 50, loss: 177.04959106445312
Step 51, loss: 178.65267944335938
Step 52, loss: 174.16734313964844
Step 53, loss: 162.8900604248047
Step 54, loss: 151.70162963867188
Step 55, loss: 147.65130615234375
Step 56, loss: 145.8255615234375
Step 57, loss: 144.58177185058594
Step 58, loss: 144.81332397460938
Step 59, loss: 147.233642578125
Step 60, loss: 150.0430908203125
Step 61, loss: 153.51095581054688
Step 62, loss: 152.81114196777344
Step 63, loss: 148.16738891601562
Step 64, loss: 142.37838745117188
Step 65, loss: 140.34408569335938
Step 66, loss: 143.46051025390625
Step 67, loss: 147.4173583984375
Step 68, loss: 149.34019470214844
Step 69, loss: 148.9425506591797
Step 70, loss: 145.8603057861328
Step 71, loss: 143.30062866210938
Step 72, loss: 141.7647705078125
Step 73, loss: 142.42132568359375
Step 74, loss: 144.17640686035156
Step 75, loss: 144.84129333496094
Step 76, loss: 144.06625366210938
Step 77, loss: 141.7694854736328
Step 78, loss: 141.5128173828125
Step 79, loss: 143.4987335205078
Step 80, loss: 146.29644775390625
Step 81, loss: 148.79400634765625
Step 82, loss: 148.70016479492188
Step 83, loss: 146.81777954101562
Step 84, loss: 145.4375
Step 85, loss: 144.93272399902344
Step 86, loss: 146.36831665039062
Step 87, loss: 147.60928344726562
Step 88, loss: 147.93785095214844
Step 89, loss: 147.7622528076172
Step 90, loss: 146.49661254882812
Step 91, loss: 145.50489807128906
Step 92, loss: 145.88255310058594
Step 93, loss: 146.6926727294922
Step 94, loss: 147.90121459960938
Step 95, loss: 148.89633178710938
Step 96, loss: 148.64382934570312
Step 97, loss: 147.34780883789062
Step 98, loss: 146.48129272460938
Step 99, loss: 146.27679443359375
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 408.6943054199219
Step 1, loss: 360.56512451171875
Step 2, loss: 310.781005859375
Step 3, loss: 253.14859008789062
Step 4, loss: 196.15396118164062
Step 5, loss: 153.01998901367188
Step 6, loss: 198.6552734375
Step 7, loss: 237.17715454101562
Step 8, loss: 262.1747131347656
Step 9, loss: 276.57440185546875
Step 10, loss: 284.6547546386719
Step 11, loss: 287.8565368652344
Step 12, loss: 286.61346435546875
Step 13, loss: 281.57958984375
Step 14, loss: 274.9212646484375
Step 15, loss: 267.03729248046875
Step 16, loss: 258.5813903808594
Step 17, loss: 247.9150848388672
Step 18, loss: 234.6064910888672
Step 19, loss: 217.54849243164062
Step 20, loss: 199.09848022460938
Step 21, loss: 188.95179748535156
Step 22, loss: 188.557373046875
Step 23, loss: 188.65196228027344
Step 24, loss: 186.318115234375
Step 25, loss: 183.06141662597656
Step 26, loss: 180.0306396484375
Step 27, loss: 181.7207794189453
Step 28, loss: 188.34359741210938
Step 29, loss: 196.5977020263672
Step 30, loss: 204.53585815429688
Step 31, loss: 211.7900390625
Step 32, loss: 217.47434997558594
Step 33, loss: 220.6996307373047
Step 34, loss: 221.55162048339844
Step 35, loss: 219.88294982910156
Step 36, loss: 215.98736572265625
Step 37, loss: 208.68780517578125
Step 38, loss: 199.70794677734375
Step 39, loss: 190.48252868652344
Step 40, loss: 178.7450714111328
Step 41, loss: 168.3115234375
Step 42, loss: 162.1466064453125
Step 43, loss: 161.14291381835938
Step 44, loss: 159.79296875
Step 45, loss: 155.42547607421875
Step 46, loss: 152.05711364746094
Step 47, loss: 155.2839813232422
Step 48, loss: 160.9359130859375
Step 49, loss: 169.80856323242188
Step 50, loss: 177.9456024169922
Step 51, loss: 181.50563049316406
Step 52, loss: 178.85960388183594
Step 53, loss: 168.09532165527344
Step 54, loss: 153.600830078125
Step 55, loss: 145.8372802734375
Step 56, loss: 144.3973388671875
Step 57, loss: 144.7709197998047
Step 58, loss: 146.95938110351562
Step 59, loss: 150.406494140625
Step 60, loss: 154.2323760986328
Step 61, loss: 155.10997009277344
Step 62, loss: 153.20565795898438
Step 63, loss: 148.6895751953125
Step 64, loss: 144.30223083496094
Step 65, loss: 142.6979217529297
Step 66, loss: 143.9608612060547
Step 67, loss: 146.42977905273438
Step 68, loss: 148.2880096435547
Step 69, loss: 147.23475646972656
Step 70, loss: 144.79031372070312
Step 71, loss: 142.95069885253906
Step 72, loss: 142.6733856201172
Step 73, loss: 144.45521545410156
Step 74, loss: 146.1954803466797
Step 75, loss: 145.6725616455078
Step 76, loss: 143.15882873535156
Step 77, loss: 141.53549194335938
Step 78, loss: 140.72012329101562
Step 79, loss: 142.9459228515625
Step 80, loss: 145.9000244140625
Step 81, loss: 148.0057373046875
Step 82, loss: 148.52261352539062
Step 83, loss: 147.27008056640625
Step 84, loss: 145.734375
Step 85, loss: 145.25390625
Step 86, loss: 146.20663452148438
Step 87, loss: 146.8134765625
Step 88, loss: 146.8575897216797
Step 89, loss: 145.33749389648438
Step 90, loss: 144.75985717773438
Step 91, loss: 145.40054321289062
Step 92, loss: 146.94659423828125
Step 93, loss: 148.184326171875
Step 94, loss: 148.2314453125
Step 95, loss: 147.80575561523438
Step 96, loss: 147.54959106445312
Step 97, loss: 147.02406311035156
Step 98, loss: 147.06752014160156
Step 99, loss: 147.21665954589844
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 408.78497314453125
Step 1, loss: 360.5205078125
Step 2, loss: 310.65972900390625
Step 3, loss: 253.23643493652344
Step 4, loss: 196.3087158203125
Step 5, loss: 153.0150146484375
Step 6, loss: 198.44076538085938
Step 7, loss: 237.03579711914062
Step 8, loss: 261.7059631347656
Step 9, loss: 276.2851257324219
Step 10, loss: 284.359130859375
Step 11, loss: 287.4344482421875
Step 12, loss: 285.8424072265625
Step 13, loss: 280.7191467285156
Step 14, loss: 273.55267333984375
Step 15, loss: 265.6521911621094
Step 16, loss: 257.0573425292969
Step 17, loss: 246.06993103027344
Step 18, loss: 232.43804931640625
Step 19, loss: 215.17369079589844
Step 20, loss: 197.1617431640625
Step 21, loss: 189.00460815429688
Step 22, loss: 189.18309020996094
Step 23, loss: 189.05088806152344
Step 24, loss: 186.57427978515625
Step 25, loss: 182.8940887451172
Step 26, loss: 179.9363250732422
Step 27, loss: 182.1202392578125
Step 28, loss: 188.64566040039062
Step 29, loss: 196.8017120361328
Step 30, loss: 204.84909057617188
Step 31, loss: 212.42730712890625
Step 32, loss: 218.12344360351562
Step 33, loss: 221.1240234375
Step 34, loss: 222.05218505859375
Step 35, loss: 219.91781616210938
Step 36, loss: 214.8060302734375
Step 37, loss: 206.94032287597656
Step 38, loss: 197.74917602539062
Step 39, loss: 188.24761962890625
Step 40, loss: 175.80526733398438
Step 41, loss: 165.58969116210938
Step 42, loss: 160.5311737060547
Step 43, loss: 159.60610961914062
Step 44, loss: 157.61123657226562
Step 45, loss: 152.9829559326172
Step 46, loss: 150.8499755859375
Step 47, loss: 155.20314025878906
Step 48, loss: 162.81593322753906
Step 49, loss: 172.4249267578125
Step 50, loss: 180.7117919921875
Step 51, loss: 182.58506774902344
Step 52, loss: 177.08444213867188
Step 53, loss: 163.2211456298828
Step 54, loss: 149.50814819335938
Step 55, loss: 144.72003173828125
Step 56, loss: 144.08065795898438
Step 57, loss: 144.9522247314453
Step 58, loss: 147.3743133544922
Step 59, loss: 150.87716674804688
Step 60, loss: 154.06015014648438
Step 61, loss: 154.84661865234375
Step 62, loss: 152.43824768066406
Step 63, loss: 147.46395874023438
Step 64, loss: 143.04559326171875
Step 65, loss: 142.3814697265625
Step 66, loss: 145.1771240234375
Step 67, loss: 148.31704711914062
Step 68, loss: 148.91908264160156
Step 69, loss: 146.3563995361328
Step 70, loss: 144.6585693359375
Step 71, loss: 144.63917541503906
Step 72, loss: 146.22335815429688
Step 73, loss: 147.91836547851562
Step 74, loss: 147.3240966796875
Step 75, loss: 143.6192169189453
Step 76, loss: 141.02108764648438
Step 77, loss: 139.75732421875
Step 78, loss: 140.15261840820312
Step 79, loss: 142.5579833984375
Step 80, loss: 146.10450744628906
Step 81, loss: 149.1605682373047
Step 82, loss: 149.47227478027344
Step 83, loss: 148.33444213867188
Step 84, loss: 147.13661193847656
Step 85, loss: 146.6162109375
Step 86, loss: 145.66244506835938
Step 87, loss: 144.96646118164062
Step 88, loss: 144.7382049560547
Step 89, loss: 145.1002655029297
Step 90, loss: 144.96124267578125
Step 91, loss: 145.8624267578125
Step 92, loss: 147.7777099609375
Step 93, loss: 148.83847045898438
Step 94, loss: 150.19139099121094
Step 95, loss: 149.8013153076172
Step 96, loss: 148.70773315429688
Step 97, loss: 147.72509765625
Step 98, loss: 146.689208984375
Step 99, loss: 145.95179748535156
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 408.7171630859375
Step 1, loss: 360.5296325683594
Step 2, loss: 310.7393798828125
Step 3, loss: 253.38294982910156
Step 4, loss: 196.410400390625
Step 5, loss: 152.92819213867188
Step 6, loss: 198.83270263671875
Step 7, loss: 237.68214416503906
Step 8, loss: 262.51861572265625
Step 9, loss: 276.7771911621094
Step 10, loss: 284.4234313964844
Step 11, loss: 287.4744567871094
Step 12, loss: 285.8987121582031
Step 13, loss: 280.6570129394531
Step 14, loss: 273.6589660644531
Step 15, loss: 265.6219482421875
Step 16, loss: 257.011474609375
Step 17, loss: 246.36671447753906
Step 18, loss: 232.95228576660156
Step 19, loss: 216.4800262451172
Step 20, loss: 198.51608276367188
Step 21, loss: 189.5966796875
Step 22, loss: 189.57290649414062
Step 23, loss: 189.7520751953125
Step 24, loss: 186.7811737060547
Step 25, loss: 182.611328125
Step 26, loss: 178.74734497070312
Step 27, loss: 180.53173828125
Step 28, loss: 187.95750427246094
Step 29, loss: 197.11807250976562
Step 30, loss: 205.96485900878906
Step 31, loss: 213.874267578125
Step 32, loss: 219.71633911132812
Step 33, loss: 223.06149291992188
Step 34, loss: 223.70663452148438
Step 35, loss: 221.72207641601562
Step 36, loss: 217.52847290039062
Step 37, loss: 210.69961547851562
Step 38, loss: 201.8936767578125
Step 39, loss: 191.93873596191406
Step 40, loss: 180.05545043945312
Step 41, loss: 167.9071044921875
Step 42, loss: 159.47500610351562
Step 43, loss: 156.4852752685547
Step 44, loss: 156.0292510986328
Step 45, loss: 153.5919647216797
Step 46, loss: 150.09829711914062
Step 47, loss: 152.37355041503906
Step 48, loss: 159.59475708007812
Step 49, loss: 169.14999389648438
Step 50, loss: 178.50897216796875
Step 51, loss: 183.2327880859375
Step 52, loss: 181.1666259765625
Step 53, loss: 171.9295196533203
Step 54, loss: 156.61907958984375
Step 55, loss: 146.905029296875
Step 56, loss: 144.77749633789062
Step 57, loss: 144.9886474609375
Step 58, loss: 146.71160888671875
Step 59, loss: 150.11376953125
Step 60, loss: 154.27243041992188
Step 61, loss: 155.63255310058594
Step 62, loss: 154.181396484375
Step 63, loss: 149.14129638671875
Step 64, loss: 144.07489013671875
Step 65, loss: 142.1131591796875
Step 66, loss: 143.21372985839844
Step 67, loss: 145.85653686523438
Step 68, loss: 147.42832946777344
Step 69, loss: 146.3008575439453
Step 70, loss: 145.1104736328125
Step 71, loss: 144.7178955078125
Step 72, loss: 144.44662475585938
Step 73, loss: 145.0362091064453
Step 74, loss: 145.2800750732422
Step 75, loss: 143.30490112304688
Step 76, loss: 140.31396484375
Step 77, loss: 139.708740234375
Step 78, loss: 141.62008666992188
Step 79, loss: 144.92514038085938
Step 80, loss: 147.3052520751953
Step 81, loss: 148.5229949951172
Step 82, loss: 147.7422332763672
Step 83, loss: 147.15028381347656
Step 84, loss: 146.77926635742188
Step 85, loss: 147.1468048095703
Step 86, loss: 146.86940002441406
Step 87, loss: 145.94732666015625
Step 88, loss: 145.2475128173828
Step 89, loss: 144.48704528808594
Step 90, loss: 144.5972137451172
Step 91, loss: 144.7473602294922
Step 92, loss: 146.8110809326172
Step 93, loss: 147.98489379882812
Step 94, loss: 148.36593627929688
Step 95, loss: 147.87274169921875
Step 96, loss: 147.71775817871094
Step 97, loss: 147.41624450683594
Step 98, loss: 147.06607055664062
Step 99, loss: 147.35116577148438
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 0.59024219, -0.01490675,  0.80708856,  0.17495499],
       [-0.80647278,  0.03229834,  0.5903884 ,  0.76909466],
       [-0.0348684 , -0.9993671 ,  0.00704195,  0.10279714],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.61590281, -0.05340053,  0.78601025,  0.16870851],
       [-0.78242098,  0.07516572,  0.61819699,  0.77673272],
       [-0.09209307, -0.99574018,  0.00451309,  0.10649023],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 0.59404608, -0.03900129,  0.80348501,  0.036257  ],
       [-0.8015457 ,  0.05582342,  0.59532196,  0.76055281],
       [-0.0680716 , -0.99767863,  0.00190036,  0.17157875],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[ 0.59404608, -0.03900129,  0.80348501,  0.036257  ],
       [-0.8015457 ,  0.05582342,  0.59532196,  0.76055281],
       [-0.0680716 , -0.99767863,  0.00190036,  0.17157875],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[ 0.71364218, -0.06820843,  0.69718182,  0.03350588],
       [-0.70003521, -0.03277459,  0.71335614,  0.84049068],
       [-0.02580703, -0.99713272, -0.07113761,  0.0982724 ],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True

loading data:   0%|          | 0/5 [00:00<?, ?it/s]
loading data: 100%|██████████| 5/5 [00:00<00:00, 57.97it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/66 [00:00<?, ?it/s]
loading data:  11%|█         | 7/66 [00:00<00:00, 66.59it/s]
loading data:  21%|██        | 14/66 [00:00<00:00, 67.56it/s]
loading data:  33%|███▎      | 22/66 [00:00<00:00, 69.46it/s]
loading data:  44%|████▍     | 29/66 [00:00<00:00, 67.81it/s]
loading data:  55%|█████▍    | 36/66 [00:00<00:00, 67.50it/s]
loading data:  65%|██████▌   | 43/66 [00:00<00:00, 66.46it/s]
loading data:  76%|███████▌  | 50/66 [00:00<00:00, 65.95it/s]
loading data:  86%|████████▋ | 57/66 [00:00<00:00, 66.26it/s]
loading data:  97%|█████████▋| 64/66 [00:00<00:00, 66.13it/s]
loading data: 100%|██████████| 66/66 [00:00<00:00, 66.81it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.2058088183403015
Loss after optimization 0.06933369487524033
logm result may be inaccurate, approximate err = 6.93134198997013e-07
Loss at initial point 0.16627226769924164
Loss after optimization 0.06438469141721725
logm result may be inaccurate, approximate err = 6.369998691289772e-07
Loss at initial point 0.21562911570072174
Loss after optimization 0.06766537576913834
logm result may be inaccurate, approximate err = 8.135960059004636e-07
Loss at initial point 0.16799238324165344
Loss after optimization 0.01601901836693287
logm result may be inaccurate, approximate err = 4.144464687651012e-07
Loss at initial point 0.2711436450481415
Loss after optimization 0.06592253595590591
logm result may be inaccurate, approximate err = 8.006130097881131e-07
Loss at initial point 0.3954591155052185
Loss after optimization 0.06367617845535278
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.20575863122940063
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 6.93134198997013e-07
Loss at initial point 0.166251078248024
Loss after optimization 0.11624012142419815
logm result may be inaccurate, approximate err = 6.369998691289772e-07
Loss at initial point 0.21576045453548431
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 8.135960059004636e-07
Loss at initial point 0.1680377721786499
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 4.144464687651012e-07
Loss at initial point 0.2711952328681946
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 8.006130097881131e-07
Loss at initial point 0.39537909626960754
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.20578405261039734
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 6.93134198997013e-07
Loss at initial point 0.16617724299430847
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 6.369998691289772e-07
Loss at initial point 0.2157324105501175
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 8.135960059004636e-07
Loss at initial point 0.1680547595024109
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 4.144464687651012e-07
Loss at initial point 0.27121788263320923
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 8.006130097881131e-07
Loss at initial point 0.39542245864868164
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 138.78750610351562
Step 1, loss: 130.3610382080078
Step 2, loss: 122.91828155517578
Step 3, loss: 119.48683166503906
Step 4, loss: 117.2433090209961
Step 5, loss: 115.50889587402344
Step 6, loss: 114.05685424804688
Step 7, loss: 112.60587310791016
Step 8, loss: 111.80811309814453
Step 9, loss: 111.41455841064453
Step 10, loss: 110.9605712890625
Step 11, loss: 110.43128967285156
Step 12, loss: 110.11688995361328
Step 13, loss: 110.08670043945312
Step 14, loss: 110.06102752685547
Step 15, loss: 109.99008178710938
Step 16, loss: 109.7194595336914
Step 17, loss: 109.34540557861328
Step 18, loss: 108.97972106933594
Step 19, loss: 108.5448989868164
Step 20, loss: 108.08812713623047
Step 21, loss: 107.53192138671875
Step 22, loss: 106.9622573852539
Step 23, loss: 106.33970642089844
Step 24, loss: 105.82247924804688
Step 25, loss: 105.31062316894531
Step 26, loss: 104.88174438476562
Step 27, loss: 104.51092529296875
Step 28, loss: 104.12214660644531
Step 29, loss: 103.852783203125
Step 30, loss: 103.56292724609375
Step 31, loss: 103.29969787597656
Step 32, loss: 103.0498046875
Step 33, loss: 102.80292510986328
Step 34, loss: 102.55812072753906
Step 35, loss: 102.36626434326172
Step 36, loss: 102.10599517822266
Step 37, loss: 101.85570526123047
Step 38, loss: 101.53317260742188
Step 39, loss: 101.20559692382812
Step 40, loss: 100.86691284179688
Step 41, loss: 100.48639678955078
Step 42, loss: 100.10549926757812
Step 43, loss: 99.65320587158203
Step 44, loss: 99.19114685058594
Step 45, loss: 98.72134399414062
Step 46, loss: 98.24522399902344
Step 47, loss: 97.71471405029297
Step 48, loss: 97.2163314819336
Step 49, loss: 96.77975463867188
Step 50, loss: 96.37947082519531
Step 51, loss: 95.97377014160156
Step 52, loss: 95.57176208496094
Step 53, loss: 95.20146942138672
Step 54, loss: 94.8428726196289
Step 55, loss: 94.34367370605469
Step 56, loss: 93.74601745605469
Step 57, loss: 93.10646057128906
Step 58, loss: 92.346435546875
Step 59, loss: 91.40205383300781
Step 60, loss: 90.3517074584961
Step 61, loss: 89.26634216308594
Step 62, loss: 88.35060119628906
Step 63, loss: 87.6712417602539
Step 64, loss: 87.39067077636719
Step 65, loss: 87.4432601928711
Step 66, loss: 87.62059783935547
Step 67, loss: 87.71926879882812
Step 68, loss: 87.87410736083984
Step 69, loss: 87.66096496582031
Step 70, loss: 87.33661651611328
Step 71, loss: 86.57280731201172
Step 72, loss: 85.86756134033203
Step 73, loss: 85.13689422607422
Step 74, loss: 84.58236694335938
Step 75, loss: 84.13066864013672
Step 76, loss: 83.57605743408203
Step 77, loss: 83.2032470703125
Step 78, loss: 83.10448455810547
Step 79, loss: 83.29621124267578
Step 80, loss: 83.47499084472656
Step 81, loss: 83.93892669677734
Step 82, loss: 84.5680923461914
Step 83, loss: 85.03260040283203
Step 84, loss: 85.29476928710938
Step 85, loss: 85.51513671875
Step 86, loss: 85.81283569335938
Step 87, loss: 86.21003723144531
Step 88, loss: 86.70818328857422
Step 89, loss: 87.1837387084961
Step 90, loss: 87.59053039550781
Step 91, loss: 87.86019897460938
Step 92, loss: 87.90422058105469
Step 93, loss: 87.8651123046875
Step 94, loss: 87.73851013183594
Step 95, loss: 87.43106842041016
Step 96, loss: 87.17053985595703
Step 97, loss: 86.89693450927734
Step 98, loss: 86.63066864013672
Step 99, loss: 86.3963623046875
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 138.78123474121094
Step 1, loss: 130.38719177246094
Step 2, loss: 122.88450622558594
Step 3, loss: 119.39063262939453
Step 4, loss: 117.12061309814453
Step 5, loss: 115.4274673461914
Step 6, loss: 114.05046844482422
Step 7, loss: 112.72024536132812
Step 8, loss: 112.0453872680664
Step 9, loss: 111.64129638671875
Step 10, loss: 111.26123809814453
Step 11, loss: 110.8257827758789
Step 12, loss: 110.5748062133789
Step 13, loss: 110.65531158447266
Step 14, loss: 110.81146240234375
Step 15, loss: 110.60564422607422
Step 16, loss: 110.44300842285156
Step 17, loss: 110.07923889160156
Step 18, loss: 109.63090515136719
Step 19, loss: 109.1153335571289
Step 20, loss: 108.58221435546875
Step 21, loss: 108.02324676513672
Step 22, loss: 107.36123657226562
Step 23, loss: 106.72294616699219
Step 24, loss: 106.18601989746094
Step 25, loss: 105.69939422607422
Step 26, loss: 105.34703063964844
Step 27, loss: 105.02010345458984
Step 28, loss: 104.7007064819336
Step 29, loss: 104.43525695800781
Step 30, loss: 104.2225570678711
Step 31, loss: 103.97417449951172
Step 32, loss: 103.76614379882812
Step 33, loss: 103.59587860107422
Step 34, loss: 103.42665100097656
Step 35, loss: 103.2553939819336
Step 36, loss: 103.06790161132812
Step 37, loss: 102.85882568359375
Step 38, loss: 102.61369323730469
Step 39, loss: 102.31430053710938
Step 40, loss: 101.99176788330078
Step 41, loss: 101.5999526977539
Step 42, loss: 101.18370056152344
Step 43, loss: 100.74971008300781
Step 44, loss: 100.30625915527344
Step 45, loss: 99.87632751464844
Step 46, loss: 99.37062072753906
Step 47, loss: 98.86712646484375
Step 48, loss: 98.40298461914062
Step 49, loss: 97.89263153076172
Step 50, loss: 97.44235229492188
Step 51, loss: 97.00081634521484
Step 52, loss: 96.6081771850586
Step 53, loss: 96.21528625488281
Step 54, loss: 95.84033966064453
Step 55, loss: 95.51332092285156
Step 56, loss: 95.16769409179688
Step 57, loss: 94.68508911132812
Step 58, loss: 94.16362762451172
Step 59, loss: 93.53436279296875
Step 60, loss: 92.80992126464844
Step 61, loss: 91.95059967041016
Step 62, loss: 90.96495056152344
Step 63, loss: 89.98929595947266
Step 64, loss: 89.18348693847656
Step 65, loss: 88.63676452636719
Step 66, loss: 88.27072143554688
Step 67, loss: 88.13040161132812
Step 68, loss: 88.20188903808594
Step 69, loss: 88.3716812133789
Step 70, loss: 88.4254150390625
Step 71, loss: 88.24671173095703
Step 72, loss: 87.98301696777344
Step 73, loss: 87.28414916992188
Step 74, loss: 86.33741760253906
Step 75, loss: 85.27408599853516
Step 76, loss: 84.43509674072266
Step 77, loss: 83.7530746459961
Step 78, loss: 83.19819641113281
Step 79, loss: 82.7342758178711
Step 80, loss: 82.43431091308594
Step 81, loss: 82.3138198852539
Step 82, loss: 82.2930679321289
Step 83, loss: 82.39979553222656
Step 84, loss: 82.46025085449219
Step 85, loss: 82.652099609375
Step 86, loss: 82.7921371459961
Step 87, loss: 83.00772857666016
Step 88, loss: 83.41075134277344
Step 89, loss: 83.98450469970703
Step 90, loss: 84.5611343383789
Step 91, loss: 85.03318786621094
Step 92, loss: 85.39433288574219
Step 93, loss: 85.72454833984375
Step 94, loss: 85.97261810302734
Step 95, loss: 86.06024932861328
Step 96, loss: 86.08394622802734
Step 97, loss: 86.05500793457031
Step 98, loss: 86.02018737792969
Step 99, loss: 85.89967346191406
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 138.8230743408203
Step 1, loss: 130.41812133789062
Step 2, loss: 122.90453338623047
Step 3, loss: 119.33745574951172
Step 4, loss: 117.09664916992188
Step 5, loss: 115.45779418945312
Step 6, loss: 113.94744873046875
Step 7, loss: 112.4663314819336
Step 8, loss: 111.64822387695312
Step 9, loss: 111.1299057006836
Step 10, loss: 110.67857360839844
Step 11, loss: 110.15267181396484
Step 12, loss: 109.75833129882812
Step 13, loss: 109.45940399169922
Step 14, loss: 109.43954467773438
Step 15, loss: 109.5321273803711
Step 16, loss: 109.47895050048828
Step 17, loss: 109.26893615722656
Step 18, loss: 109.01278686523438
Step 19, loss: 108.61465454101562
Step 20, loss: 108.13880157470703
Step 21, loss: 107.59799194335938
Step 22, loss: 107.10411071777344
Step 23, loss: 106.487060546875
Step 24, loss: 105.98651885986328
Step 25, loss: 105.49726867675781
Step 26, loss: 105.04755401611328
Step 27, loss: 104.67316436767578
Step 28, loss: 104.25324249267578
Step 29, loss: 103.9412841796875
Step 30, loss: 103.61199951171875
Step 31, loss: 103.3691177368164
Step 32, loss: 103.13512420654297
Step 33, loss: 102.93413543701172
Step 34, loss: 102.72103881835938
Step 35, loss: 102.59637451171875
Step 36, loss: 102.34286499023438
Step 37, loss: 102.05944061279297
Step 38, loss: 101.6971435546875
Step 39, loss: 101.33474731445312
Step 40, loss: 100.95906829833984
Step 41, loss: 100.58586883544922
Step 42, loss: 100.22115325927734
Step 43, loss: 99.79452514648438
Step 44, loss: 99.32672882080078
Step 45, loss: 98.78764343261719
Step 46, loss: 98.3199234008789
Step 47, loss: 97.802490234375
Step 48, loss: 97.32881927490234
Step 49, loss: 96.85162353515625
Step 50, loss: 96.40876007080078
Step 51, loss: 96.00621795654297
Step 52, loss: 95.6881103515625
Step 53, loss: 95.37311553955078
Step 54, loss: 94.85812377929688
Step 55, loss: 94.365234375
Step 56, loss: 93.69568634033203
Step 57, loss: 92.93064880371094
Step 58, loss: 92.06084442138672
Step 59, loss: 91.03498077392578
Step 60, loss: 89.97810363769531
Step 61, loss: 89.04411315917969
Step 62, loss: 88.35589599609375
Step 63, loss: 87.99740600585938
Step 64, loss: 87.98332214355469
Step 65, loss: 88.1506118774414
Step 66, loss: 88.30523681640625
Step 67, loss: 88.33126068115234
Step 68, loss: 88.22217559814453
Step 69, loss: 87.84429168701172
Step 70, loss: 87.22700500488281
Step 71, loss: 86.47257995605469
Step 72, loss: 85.74269104003906
Step 73, loss: 85.06401824951172
Step 74, loss: 84.587646484375
Step 75, loss: 84.07989501953125
Step 76, loss: 83.53440856933594
Step 77, loss: 83.1734619140625
Step 78, loss: 82.98242950439453
Step 79, loss: 83.10578918457031
Step 80, loss: 83.2795639038086
Step 81, loss: 83.40611267089844
Step 82, loss: 83.52460479736328
Step 83, loss: 83.84378051757812
Step 84, loss: 84.17882537841797
Step 85, loss: 84.56829071044922
Step 86, loss: 84.95845031738281
Step 87, loss: 85.37548828125
Step 88, loss: 85.79698181152344
Step 89, loss: 86.20494842529297
Step 90, loss: 86.52294158935547
Step 91, loss: 86.65579223632812
Step 92, loss: 86.80623626708984
Step 93, loss: 86.84581756591797
Step 94, loss: 86.79755401611328
Step 95, loss: 86.72539520263672
Step 96, loss: 86.58892059326172
Step 97, loss: 86.45579528808594
Step 98, loss: 86.27383422851562
Step 99, loss: 86.1378402709961
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 138.8565673828125
Step 1, loss: 130.3621826171875
Step 2, loss: 122.9167709350586
Step 3, loss: 119.3521499633789
Step 4, loss: 117.06978607177734
Step 5, loss: 115.3089370727539
Step 6, loss: 113.90980529785156
Step 7, loss: 112.38492584228516
Step 8, loss: 111.57704162597656
Step 9, loss: 111.04403686523438
Step 10, loss: 110.62715148925781
Step 11, loss: 110.18010711669922
Step 12, loss: 109.78755950927734
Step 13, loss: 109.77096557617188
Step 14, loss: 109.96454620361328
Step 15, loss: 110.1593246459961
Step 16, loss: 110.179443359375
Step 17, loss: 110.06550598144531
Step 18, loss: 109.88961791992188
Step 19, loss: 109.51342010498047
Step 20, loss: 109.1260986328125
Step 21, loss: 108.625244140625
Step 22, loss: 108.11914825439453
Step 23, loss: 107.6269760131836
Step 24, loss: 107.07716369628906
Step 25, loss: 106.53654479980469
Step 26, loss: 106.01177215576172
Step 27, loss: 105.5771484375
Step 28, loss: 105.1949234008789
Step 29, loss: 104.85496520996094
Step 30, loss: 104.57097625732422
Step 31, loss: 104.29953002929688
Step 32, loss: 104.03771209716797
Step 33, loss: 103.81498718261719
Step 34, loss: 103.59231567382812
Step 35, loss: 103.33892822265625
Step 36, loss: 103.09178161621094
Step 37, loss: 102.85670471191406
Step 38, loss: 102.54500579833984
Step 39, loss: 102.1712646484375
Step 40, loss: 101.78536224365234
Step 41, loss: 101.35186004638672
Step 42, loss: 100.9164047241211
Step 43, loss: 100.51522827148438
Step 44, loss: 100.1161117553711
Step 45, loss: 99.6812973022461
Step 46, loss: 99.22046661376953
Step 47, loss: 98.7527084350586
Step 48, loss: 98.27701568603516
Step 49, loss: 97.83836364746094
Step 50, loss: 97.44914245605469
Step 51, loss: 97.0629653930664
Step 52, loss: 96.63433074951172
Step 53, loss: 96.25947570800781
Step 54, loss: 95.98639678955078
Step 55, loss: 95.678466796875
Step 56, loss: 95.22415924072266
Step 57, loss: 94.76116180419922
Step 58, loss: 94.13727569580078
Step 59, loss: 93.42277526855469
Step 60, loss: 92.59489440917969
Step 61, loss: 91.6069564819336
Step 62, loss: 90.64662170410156
Step 63, loss: 89.84867095947266
Step 64, loss: 89.21916961669922
Step 65, loss: 88.8286361694336
Step 66, loss: 88.65875244140625
Step 67, loss: 88.74622344970703
Step 68, loss: 89.01637268066406
Step 69, loss: 89.1075439453125
Step 70, loss: 89.07804107666016
Step 71, loss: 88.82904815673828
Step 72, loss: 88.26649475097656
Step 73, loss: 87.45208740234375
Step 74, loss: 86.37055206298828
Step 75, loss: 85.4175796508789
Step 76, loss: 84.54949188232422
Step 77, loss: 83.87598419189453
Step 78, loss: 83.34225463867188
Step 79, loss: 83.0616683959961
Step 80, loss: 82.98918151855469
Step 81, loss: 83.06755828857422
Step 82, loss: 83.12742614746094
Step 83, loss: 83.24607849121094
Step 84, loss: 83.32860565185547
Step 85, loss: 83.53945922851562
Step 86, loss: 83.63641357421875
Step 87, loss: 84.00117492675781
Step 88, loss: 84.56542205810547
Step 89, loss: 85.13246154785156
Step 90, loss: 85.66400146484375
Step 91, loss: 86.12757873535156
Step 92, loss: 86.49108123779297
Step 93, loss: 86.7162094116211
Step 94, loss: 86.91096496582031
Step 95, loss: 86.95826721191406
Step 96, loss: 86.965087890625
Step 97, loss: 86.90067291259766
Step 98, loss: 86.79655456542969
Step 99, loss: 86.59712982177734
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 138.82264709472656
Step 1, loss: 130.36720275878906
Step 2, loss: 122.85220336914062
Step 3, loss: 119.32768249511719
Step 4, loss: 117.03594970703125
Step 5, loss: 115.29523468017578
Step 6, loss: 113.69577026367188
Step 7, loss: 112.18889617919922
Step 8, loss: 111.34716033935547
Step 9, loss: 110.8604736328125
Step 10, loss: 110.38925170898438
Step 11, loss: 109.92183685302734
Step 12, loss: 109.54119873046875
Step 13, loss: 109.20513916015625
Step 14, loss: 109.22097778320312
Step 15, loss: 109.28560638427734
Step 16, loss: 109.23635864257812
Step 17, loss: 109.14259338378906
Step 18, loss: 108.95342254638672
Step 19, loss: 108.75944519042969
Step 20, loss: 108.50820922851562
Step 21, loss: 108.17532348632812
Step 22, loss: 107.81756591796875
Step 23, loss: 107.46324920654297
Step 24, loss: 107.07040405273438
Step 25, loss: 106.66409301757812
Step 26, loss: 106.28097534179688
Step 27, loss: 105.93587493896484
Step 28, loss: 105.62761688232422
Step 29, loss: 105.2996597290039
Step 30, loss: 104.9685287475586
Step 31, loss: 104.65291595458984
Step 32, loss: 104.40985107421875
Step 33, loss: 104.16383361816406
Step 34, loss: 103.8914794921875
Step 35, loss: 103.64386749267578
Step 36, loss: 103.4016342163086
Step 37, loss: 103.15335845947266
Step 38, loss: 102.82435607910156
Step 39, loss: 102.43806457519531
Step 40, loss: 102.05205535888672
Step 41, loss: 101.63209533691406
Step 42, loss: 101.17744445800781
Step 43, loss: 100.76651000976562
Step 44, loss: 100.3872299194336
Step 45, loss: 99.91986083984375
Step 46, loss: 99.43986511230469
Step 47, loss: 98.91519927978516
Step 48, loss: 98.42994689941406
Step 49, loss: 97.89714813232422
Step 50, loss: 97.42991638183594
Step 51, loss: 96.97730255126953
Step 52, loss: 96.55857849121094
Step 53, loss: 96.12728881835938
Step 54, loss: 95.77593231201172
Step 55, loss: 95.45868682861328
Step 56, loss: 95.01514434814453
Step 57, loss: 94.48419952392578
Step 58, loss: 93.81330108642578
Step 59, loss: 93.03048706054688
Step 60, loss: 92.09429931640625
Step 61, loss: 90.95283508300781
Step 62, loss: 89.93364715576172
Step 63, loss: 89.0191421508789
Step 64, loss: 88.39546203613281
Step 65, loss: 88.22822570800781
Step 66, loss: 88.39498901367188
Step 67, loss: 88.52661895751953
Step 68, loss: 88.76264953613281
Step 69, loss: 88.85448455810547
Step 70, loss: 88.63815307617188
Step 71, loss: 88.1964340209961
Step 72, loss: 87.75070190429688
Step 73, loss: 87.42900848388672
Step 74, loss: 87.03684997558594
Step 75, loss: 86.64291381835938
Step 76, loss: 86.09538269042969
Step 77, loss: 85.59452056884766
Step 78, loss: 85.30870819091797
Step 79, loss: 85.11420440673828
Step 80, loss: 84.90132141113281
Step 81, loss: 84.95024871826172
Step 82, loss: 85.35137939453125
Step 83, loss: 85.86621856689453
Step 84, loss: 86.33614349365234
Step 85, loss: 86.67821502685547
Step 86, loss: 86.89273834228516
Step 87, loss: 87.02010345458984
Step 88, loss: 87.06935119628906
Step 89, loss: 87.24058532714844
Step 90, loss: 87.52999114990234
Step 91, loss: 87.78992462158203
Step 92, loss: 88.06145477294922
Step 93, loss: 88.23009490966797
Step 94, loss: 88.33069610595703
Step 95, loss: 88.34932708740234
Step 96, loss: 88.2674331665039
Step 97, loss: 88.02796173095703
Step 98, loss: 87.80045318603516
Step 99, loss: 87.51313781738281
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 138.7950897216797
Step 1, loss: 130.407958984375
Step 2, loss: 122.80862426757812
Step 3, loss: 119.35169982910156
Step 4, loss: 117.10570526123047
Step 5, loss: 115.45254516601562
Step 6, loss: 113.9053955078125
Step 7, loss: 112.48697662353516
Step 8, loss: 111.75125885009766
Step 9, loss: 111.27320098876953
Step 10, loss: 110.75852966308594
Step 11, loss: 110.40849304199219
Step 12, loss: 110.10192108154297
Step 13, loss: 110.03665161132812
Step 14, loss: 110.10757446289062
Step 15, loss: 110.12785339355469
Step 16, loss: 110.04270935058594
Step 17, loss: 109.90816497802734
Step 18, loss: 109.75726318359375
Step 19, loss: 109.44488525390625
Step 20, loss: 109.074462890625
Step 21, loss: 108.62934112548828
Step 22, loss: 108.18090057373047
Step 23, loss: 107.70709991455078
Step 24, loss: 107.22647857666016
Step 25, loss: 106.76424407958984
Step 26, loss: 106.38519287109375
Step 27, loss: 106.06024169921875
Step 28, loss: 105.73365783691406
Step 29, loss: 105.45877075195312
Step 30, loss: 105.15576171875
Step 31, loss: 104.92232513427734
Step 32, loss: 104.66960144042969
Step 33, loss: 104.45034790039062
Step 34, loss: 104.22003936767578
Step 35, loss: 103.9735336303711
Step 36, loss: 103.74613952636719
Step 37, loss: 103.5127182006836
Step 38, loss: 103.2562255859375
Step 39, loss: 102.920166015625
Step 40, loss: 102.5809097290039
Step 41, loss: 102.1973876953125
Step 42, loss: 101.8012466430664
Step 43, loss: 101.35284423828125
Step 44, loss: 100.93818664550781
Step 45, loss: 100.54505920410156
Step 46, loss: 100.18116760253906
Step 47, loss: 99.79615783691406
Step 48, loss: 99.31835174560547
Step 49, loss: 98.879150390625
Step 50, loss: 98.48164367675781
Step 51, loss: 98.0368881225586
Step 52, loss: 97.67069244384766
Step 53, loss: 97.30183410644531
Step 54, loss: 96.8687515258789
Step 55, loss: 96.51472473144531
Step 56, loss: 96.26509857177734
Step 57, loss: 95.92481231689453
Step 58, loss: 95.50263214111328
Step 59, loss: 95.05529022216797
Step 60, loss: 94.45108032226562
Step 61, loss: 93.72508239746094
Step 62, loss: 92.9129638671875
Step 63, loss: 91.98567962646484
Step 64, loss: 90.98847198486328
Step 65, loss: 90.22412872314453
Step 66, loss: 89.54348754882812
Step 67, loss: 89.13327026367188
Step 68, loss: 88.99492645263672
Step 69, loss: 89.06778717041016
Step 70, loss: 89.3454360961914
Step 71, loss: 89.53882598876953
Step 72, loss: 89.6265640258789
Step 73, loss: 89.55397033691406
Step 74, loss: 89.12086486816406
Step 75, loss: 88.44657897949219
Step 76, loss: 87.62998962402344
Step 77, loss: 86.91387176513672
Step 78, loss: 86.22755432128906
Step 79, loss: 85.65460968017578
Step 80, loss: 85.0827865600586
Step 81, loss: 84.56546020507812
Step 82, loss: 84.2127914428711
Step 83, loss: 84.09459686279297
Step 84, loss: 84.28790283203125
Step 85, loss: 84.50248718261719
Step 86, loss: 84.55894470214844
Step 87, loss: 84.84481048583984
Step 88, loss: 85.29959106445312
Step 89, loss: 85.61957550048828
Step 90, loss: 85.82380676269531
Step 91, loss: 86.03070831298828
Step 92, loss: 86.2684097290039
Step 93, loss: 86.62415313720703
Step 94, loss: 87.05233001708984
Step 95, loss: 87.47378540039062
Step 96, loss: 87.7469253540039
Step 97, loss: 87.99164581298828
Step 98, loss: 88.0653305053711
Step 99, loss: 88.06116485595703
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 0.48389215,  0.00277383,  0.87512325,  0.05200909],
       [-0.87512308, -0.00169639,  0.48389743,  0.75312294],
       [ 0.0028268 , -0.99999471,  0.00160657,  0.15592035],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.64485259,  0.33051381,  0.68914858, -0.169008  ],
       [-0.26209651,  0.94261764, -0.20682699,  0.45849465],
       [-0.71796278, -0.04725052,  0.69447594,  0.33773013],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 0.58481313,  0.00496414,  0.81115286, -0.42193514],
       [-0.81114405, -0.00411346,  0.58483195,  1.61529571],
       [ 0.00623984, -0.99997922,  0.00162103,  0.13349777],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[ 0.48389215,  0.00277383,  0.87512325,  0.05200909],
       [-0.87512308, -0.00169639,  0.48389743,  0.75312294],
       [ 0.0028268 , -0.99999471,  0.00160657,  0.15592035],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[ 0.54287773, -0.08567484,  0.83543056,  0.03397676],
       [-0.48618513, -0.8431921 ,  0.22946091,  0.9580097 ],
       [ 0.68476868, -0.53074229, -0.49940458, -0.10828629],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True

loading data:   0%|          | 0/5 [00:00<?, ?it/s]
loading data: 100%|██████████| 5/5 [00:00<00:00, 56.63it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/66 [00:00<?, ?it/s]
loading data:  11%|█         | 7/66 [00:00<00:00, 65.56it/s]
loading data:  21%|██        | 14/66 [00:00<00:00, 64.42it/s]
loading data:  32%|███▏      | 21/66 [00:00<00:00, 65.40it/s]
loading data:  42%|████▏     | 28/66 [00:00<00:00, 65.25it/s]
loading data:  53%|█████▎    | 35/66 [00:00<00:00, 65.41it/s]
loading data:  64%|██████▎   | 42/66 [00:00<00:00, 65.36it/s]
loading data:  74%|███████▍  | 49/66 [00:00<00:00, 65.02it/s]
loading data:  85%|████████▍ | 56/66 [00:00<00:00, 65.63it/s]
loading data:  95%|█████████▌| 63/66 [00:00<00:00, 65.32it/s]
loading data: 100%|██████████| 66/66 [00:01<00:00, 65.39it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(p, device='cuda', dtype=torch.float32)
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.9358795136213303
Loss after optimization 0.3919263333082199
logm result may be inaccurate, approximate err = 4.513725454034538e-07
Loss at initial point 0.9069727957248688
Loss after optimization 0.4073995500802994
logm result may be inaccurate, approximate err = 3.349515571050945e-07
Loss at initial point 1.0562538355588913
Loss after optimization 0.23356369510293007
logm result may be inaccurate, approximate err = 2.7573945087750223e-07
Loss at initial point 0.9997777938842773
Loss after optimization 0.3815532848238945
logm result may be inaccurate, approximate err = 3.2693911026494004e-07
Loss at initial point 1.0404484421014786
Loss after optimization 0.4474537745118141
logm result may be inaccurate, approximate err = 6.763568034599716e-07
Loss at initial point 1.1211774051189423
Loss after optimization 0.40486904233694077
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.9358717501163483
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 4.513725454034538e-07
Loss at initial point 0.906960517168045
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 3.349515571050945e-07
Loss at initial point 1.0561190843582153
Loss after optimization 0.5915266945958138
logm result may be inaccurate, approximate err = 2.7573945087750223e-07
Loss at initial point 1.0000586360692978
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 3.2693911026494004e-07
Loss at initial point 1.0405577719211578
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 6.763568034599716e-07
Loss at initial point 1.1212584376335144
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.9359873235225677
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 4.513725454034538e-07
Loss at initial point 0.906885638833046
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 3.349515571050945e-07
Loss at initial point 1.0564128905534744
Loss after optimization 0.5663121566176414
logm result may be inaccurate, approximate err = 2.7573945087750223e-07
Loss at initial point 0.9997199326753616
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 3.2693911026494004e-07
Loss at initial point 1.040491670370102
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 6.763568034599716e-07
Loss at initial point 1.1211400032043457
Loss after optimization 0.5680074617266655
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 582.7243041992188
Step 1, loss: 590.6939697265625
Step 2, loss: 595.4871826171875
Step 3, loss: 601.2813720703125
Step 4, loss: 609.0346069335938
Step 5, loss: 616.899658203125
Step 6, loss: 624.4926147460938
Step 7, loss: 629.5064697265625
Step 8, loss: 632.3170166015625
Step 9, loss: 633.694091796875
Step 10, loss: 632.9369506835938
Step 11, loss: 628.8118286132812
Step 12, loss: 622.0951538085938
Step 13, loss: 613.1273803710938
Step 14, loss: 603.2178344726562
Step 15, loss: 590.7745971679688
Step 16, loss: 577.02587890625
Step 17, loss: 565.6044921875
Step 18, loss: 554.6064453125
Step 19, loss: 543.9218139648438
Step 20, loss: 532.4964599609375
Step 21, loss: 520.8441772460938
Step 22, loss: 508.97723388671875
Step 23, loss: 498.91009521484375
Step 24, loss: 492.24627685546875
Step 25, loss: 488.35430908203125
Step 26, loss: 485.59356689453125
Step 27, loss: 484.3390808105469
Step 28, loss: 483.41790771484375
Step 29, loss: 482.5615539550781
Step 30, loss: 481.858154296875
Step 31, loss: 481.2498474121094
Step 32, loss: 480.701904296875
Step 33, loss: 480.20501708984375
Step 34, loss: 479.7110900878906
Step 35, loss: 479.2277526855469
Step 36, loss: 478.76702880859375
Step 37, loss: 478.2986145019531
Step 38, loss: 477.8507080078125
Step 39, loss: 477.3847351074219
Step 40, loss: 476.9199523925781
Step 41, loss: 476.4862976074219
Step 42, loss: 476.1231384277344
Step 43, loss: 475.7391357421875
Step 44, loss: 475.38238525390625
Step 45, loss: 475.043212890625
Step 46, loss: 474.7343444824219
Step 47, loss: 474.42303466796875
Step 48, loss: 474.1224365234375
Step 49, loss: 473.8119812011719
Step 50, loss: 473.57080078125
Step 51, loss: 473.3260192871094
Step 52, loss: 473.1095275878906
Step 53, loss: 472.8770751953125
Step 54, loss: 472.6924743652344
Step 55, loss: 472.4994201660156
Step 56, loss: 472.3099060058594
Step 57, loss: 472.1197204589844
Step 58, loss: 471.9432678222656
Step 59, loss: 471.81634521484375
Step 60, loss: 471.6000671386719
Step 61, loss: 471.5022277832031
Step 62, loss: 471.34759521484375
Step 63, loss: 471.1983337402344
Step 64, loss: 471.0125732421875
Step 65, loss: 470.8918762207031
Step 66, loss: 470.6754455566406
Step 67, loss: 470.536865234375
Step 68, loss: 470.3415832519531
Step 69, loss: 470.16497802734375
Step 70, loss: 469.97412109375
Step 71, loss: 469.8207702636719
Step 72, loss: 469.63226318359375
Step 73, loss: 469.45172119140625
Step 74, loss: 469.2823181152344
Step 75, loss: 469.1263427734375
Step 76, loss: 468.9846496582031
Step 77, loss: 468.8543701171875
Step 78, loss: 468.6791076660156
Step 79, loss: 468.5450744628906
Step 80, loss: 468.3903503417969
Step 81, loss: 468.283447265625
Step 82, loss: 468.16278076171875
Step 83, loss: 468.0423889160156
Step 84, loss: 467.9189147949219
Step 85, loss: 467.8187561035156
Step 86, loss: 467.7236633300781
Step 87, loss: 467.6195068359375
Step 88, loss: 467.5372619628906
Step 89, loss: 467.4491882324219
Step 90, loss: 467.3916931152344
Step 91, loss: 467.3492126464844
Step 92, loss: 467.3087158203125
Step 93, loss: 467.27734375
Step 94, loss: 467.2394104003906
Step 95, loss: 467.2070007324219
Step 96, loss: 467.1581115722656
Step 97, loss: 467.1429748535156
Step 98, loss: 467.1294860839844
Step 99, loss: 467.0970764160156
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 582.708984375
Step 1, loss: 567.5142822265625
Step 2, loss: 554.962158203125
Step 3, loss: 543.0025634765625
Step 4, loss: 532.792724609375
Step 5, loss: 525.8831787109375
Step 6, loss: 522.9825439453125
Step 7, loss: 525.052001953125
Step 8, loss: 529.9363403320312
Step 9, loss: 533.8234252929688
Step 10, loss: 536.2842407226562
Step 11, loss: 536.3948974609375
Step 12, loss: 533.89794921875
Step 13, loss: 529.5985107421875
Step 14, loss: 524.5547485351562
Step 15, loss: 519.022705078125
Step 16, loss: 514.0879516601562
Step 17, loss: 508.7895202636719
Step 18, loss: 501.77532958984375
Step 19, loss: 495.69671630859375
Step 20, loss: 491.52569580078125
Step 21, loss: 488.03875732421875
Step 22, loss: 484.11541748046875
Step 23, loss: 480.3966064453125
Step 24, loss: 476.307861328125
Step 25, loss: 474.40277099609375
Step 26, loss: 473.8360595703125
Step 27, loss: 472.01416015625
Step 28, loss: 468.7032775878906
Step 29, loss: 465.48797607421875
Step 30, loss: 461.850830078125
Step 31, loss: 458.1627197265625
Step 32, loss: 454.65191650390625
Step 33, loss: 451.3822021484375
Step 34, loss: 449.16259765625
Step 35, loss: 447.16192626953125
Step 36, loss: 445.25482177734375
Step 37, loss: 444.1878356933594
Step 38, loss: 444.26214599609375
Step 39, loss: 445.4252014160156
Step 40, loss: 446.9041748046875
Step 41, loss: 448.4258117675781
Step 42, loss: 450.10809326171875
Step 43, loss: 451.8567199707031
Step 44, loss: 453.37030029296875
Step 45, loss: 454.74169921875
Step 46, loss: 455.99041748046875
Step 47, loss: 456.9014892578125
Step 48, loss: 457.6102294921875
Step 49, loss: 458.13848876953125
Step 50, loss: 458.5240478515625
Step 51, loss: 458.8170471191406
Step 52, loss: 459.01422119140625
Step 53, loss: 459.17938232421875
Step 54, loss: 459.2486572265625
Step 55, loss: 459.3352355957031
Step 56, loss: 459.38275146484375
Step 57, loss: 459.3575439453125
Step 58, loss: 459.32769775390625
Step 59, loss: 459.2607116699219
Step 60, loss: 459.1510009765625
Step 61, loss: 458.99554443359375
Step 62, loss: 458.8525390625
Step 63, loss: 458.65802001953125
Step 64, loss: 458.45758056640625
Step 65, loss: 458.23602294921875
Step 66, loss: 458.015625
Step 67, loss: 457.7716369628906
Step 68, loss: 457.494873046875
Step 69, loss: 457.2191467285156
Step 70, loss: 456.9271545410156
Step 71, loss: 456.58111572265625
Step 72, loss: 456.27056884765625
Step 73, loss: 455.9662170410156
Step 74, loss: 455.60186767578125
Step 75, loss: 455.25634765625
Step 76, loss: 454.8791809082031
Step 77, loss: 454.5238037109375
Step 78, loss: 454.17706298828125
Step 79, loss: 453.80426025390625
Step 80, loss: 453.50396728515625
Step 81, loss: 453.2012634277344
Step 82, loss: 452.97052001953125
Step 83, loss: 452.739501953125
Step 84, loss: 452.51837158203125
Step 85, loss: 452.290771484375
Step 86, loss: 452.1038818359375
Step 87, loss: 451.9029846191406
Step 88, loss: 451.77410888671875
Step 89, loss: 451.6083984375
Step 90, loss: 451.45233154296875
Step 91, loss: 451.34716796875
Step 92, loss: 451.21624755859375
Step 93, loss: 451.1505126953125
Step 94, loss: 451.0484924316406
Step 95, loss: 451.0249938964844
Step 96, loss: 450.9559631347656
Step 97, loss: 450.89453125
Step 98, loss: 450.80230712890625
Step 99, loss: 450.7113342285156
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 582.68603515625
Step 1, loss: 590.6427612304688
Step 2, loss: 596.7184448242188
Step 3, loss: 602.7628784179688
Step 4, loss: 610.43408203125
Step 5, loss: 618.7376708984375
Step 6, loss: 626.2839965820312
Step 7, loss: 630.7764892578125
Step 8, loss: 633.3678588867188
Step 9, loss: 634.1866455078125
Step 10, loss: 633.0626220703125
Step 11, loss: 628.636474609375
Step 12, loss: 621.2529907226562
Step 13, loss: 611.8631591796875
Step 14, loss: 601.9315185546875
Step 15, loss: 589.3755493164062
Step 16, loss: 576.2216796875
Step 17, loss: 564.8877563476562
Step 18, loss: 554.0603637695312
Step 19, loss: 543.3536376953125
Step 20, loss: 532.572509765625
Step 21, loss: 520.6591796875
Step 22, loss: 508.42401123046875
Step 23, loss: 498.6043701171875
Step 24, loss: 492.3183898925781
Step 25, loss: 488.35205078125
Step 26, loss: 485.6803283691406
Step 27, loss: 484.323486328125
Step 28, loss: 483.4143981933594
Step 29, loss: 482.5595397949219
Step 30, loss: 481.86334228515625
Step 31, loss: 481.30108642578125
Step 32, loss: 480.7248840332031
Step 33, loss: 480.14703369140625
Step 34, loss: 479.6575622558594
Step 35, loss: 479.19329833984375
Step 36, loss: 478.7325744628906
Step 37, loss: 478.23040771484375
Step 38, loss: 477.7518005371094
Step 39, loss: 477.2356872558594
Step 40, loss: 476.79248046875
Step 41, loss: 476.3627014160156
Step 42, loss: 475.95977783203125
Step 43, loss: 475.5840148925781
Step 44, loss: 475.2227478027344
Step 45, loss: 474.8678283691406
Step 46, loss: 474.538330078125
Step 47, loss: 474.1918640136719
Step 48, loss: 473.92999267578125
Step 49, loss: 473.6556701660156
Step 50, loss: 473.3836364746094
Step 51, loss: 473.1416320800781
Step 52, loss: 472.9473876953125
Step 53, loss: 472.72247314453125
Step 54, loss: 472.5122985839844
Step 55, loss: 472.345947265625
Step 56, loss: 472.1481018066406
Step 57, loss: 471.99029541015625
Step 58, loss: 471.81451416015625
Step 59, loss: 471.6436767578125
Step 60, loss: 471.48577880859375
Step 61, loss: 471.3495788574219
Step 62, loss: 471.1435241699219
Step 63, loss: 471.0112609863281
Step 64, loss: 470.76141357421875
Step 65, loss: 470.6114501953125
Step 66, loss: 470.3920593261719
Step 67, loss: 470.20452880859375
Step 68, loss: 470.0215148925781
Step 69, loss: 469.85003662109375
Step 70, loss: 469.6834716796875
Step 71, loss: 469.4732971191406
Step 72, loss: 469.3183288574219
Step 73, loss: 469.1593322753906
Step 74, loss: 468.9924011230469
Step 75, loss: 468.8106689453125
Step 76, loss: 468.6509094238281
Step 77, loss: 468.5165710449219
Step 78, loss: 468.3677673339844
Step 79, loss: 468.24462890625
Step 80, loss: 468.1246032714844
Step 81, loss: 467.99078369140625
Step 82, loss: 467.8871154785156
Step 83, loss: 467.7790832519531
Step 84, loss: 467.6862487792969
Step 85, loss: 467.6195983886719
Step 86, loss: 467.5318603515625
Step 87, loss: 467.4388427734375
Step 88, loss: 467.40032958984375
Step 89, loss: 467.3087158203125
Step 90, loss: 467.2648620605469
Step 91, loss: 467.2479553222656
Step 92, loss: 467.23291015625
Step 93, loss: 467.1930847167969
Step 94, loss: 467.1649475097656
Step 95, loss: 467.1145935058594
Step 96, loss: 467.093994140625
Step 97, loss: 467.0660400390625
Step 98, loss: 467.0541076660156
Step 99, loss: 467.0480651855469
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 582.733154296875
Step 1, loss: 590.6484375
Step 2, loss: 596.5609741210938
Step 3, loss: 602.7584228515625
Step 4, loss: 610.656494140625
Step 5, loss: 618.8400268554688
Step 6, loss: 626.22998046875
Step 7, loss: 630.7991943359375
Step 8, loss: 633.2105712890625
Step 9, loss: 633.9552612304688
Step 10, loss: 632.87744140625
Step 11, loss: 628.3421630859375
Step 12, loss: 620.5330810546875
Step 13, loss: 611.1209716796875
Step 14, loss: 600.7798461914062
Step 15, loss: 587.4215087890625
Step 16, loss: 574.46435546875
Step 17, loss: 562.9962158203125
Step 18, loss: 552.0521850585938
Step 19, loss: 541.109375
Step 20, loss: 529.1699829101562
Step 21, loss: 516.8487548828125
Step 22, loss: 505.0599365234375
Step 23, loss: 496.2356872558594
Step 24, loss: 490.83758544921875
Step 25, loss: 487.17608642578125
Step 26, loss: 484.94354248046875
Step 27, loss: 483.900390625
Step 28, loss: 482.9443359375
Step 29, loss: 482.1428527832031
Step 30, loss: 481.5175476074219
Step 31, loss: 480.9032287597656
Step 32, loss: 480.3217468261719
Step 33, loss: 479.8967590332031
Step 34, loss: 479.3645935058594
Step 35, loss: 478.90087890625
Step 36, loss: 478.43438720703125
Step 37, loss: 477.9472961425781
Step 38, loss: 477.4209289550781
Step 39, loss: 476.9595642089844
Step 40, loss: 476.5723571777344
Step 41, loss: 476.14739990234375
Step 42, loss: 475.7684020996094
Step 43, loss: 475.34600830078125
Step 44, loss: 474.9817810058594
Step 45, loss: 474.5846252441406
Step 46, loss: 474.2455139160156
Step 47, loss: 473.8845520019531
Step 48, loss: 473.6010437011719
Step 49, loss: 473.3048400878906
Step 50, loss: 473.0686340332031
Step 51, loss: 472.79962158203125
Step 52, loss: 472.5823974609375
Step 53, loss: 472.3857727050781
Step 54, loss: 472.1746520996094
Step 55, loss: 471.9476318359375
Step 56, loss: 471.7903747558594
Step 57, loss: 471.5997619628906
Step 58, loss: 471.4678039550781
Step 59, loss: 471.2463684082031
Step 60, loss: 471.1064453125
Step 61, loss: 470.9027404785156
Step 62, loss: 470.71539306640625
Step 63, loss: 470.51141357421875
Step 64, loss: 470.3037414550781
Step 65, loss: 470.14068603515625
Step 66, loss: 469.9471435546875
Step 67, loss: 469.7510986328125
Step 68, loss: 469.5820007324219
Step 69, loss: 469.3963928222656
Step 70, loss: 469.23443603515625
Step 71, loss: 469.074462890625
Step 72, loss: 468.90582275390625
Step 73, loss: 468.7310485839844
Step 74, loss: 468.6288757324219
Step 75, loss: 468.45184326171875
Step 76, loss: 468.3203430175781
Step 77, loss: 468.1855163574219
Step 78, loss: 468.07232666015625
Step 79, loss: 467.9587707519531
Step 80, loss: 467.8426208496094
Step 81, loss: 467.7362060546875
Step 82, loss: 467.6536865234375
Step 83, loss: 467.5649108886719
Step 84, loss: 467.4801330566406
Step 85, loss: 467.4468078613281
Step 86, loss: 467.3635559082031
Step 87, loss: 467.2972717285156
Step 88, loss: 467.25762939453125
Step 89, loss: 467.2518310546875
Step 90, loss: 467.22943115234375
Step 91, loss: 467.1876525878906
Step 92, loss: 467.1585388183594
Step 93, loss: 467.1226806640625
Step 94, loss: 467.0738220214844
Step 95, loss: 467.0577392578125
Step 96, loss: 467.0339660644531
Step 97, loss: 467.0333557128906
Step 98, loss: 466.9812316894531
Step 99, loss: 466.956787109375
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 582.7420654296875
Step 1, loss: 590.70068359375
Step 2, loss: 596.9654541015625
Step 3, loss: 603.2352294921875
Step 4, loss: 610.9903564453125
Step 5, loss: 619.20166015625
Step 6, loss: 626.754150390625
Step 7, loss: 630.9592895507812
Step 8, loss: 633.53125
Step 9, loss: 634.2227783203125
Step 10, loss: 632.7503051757812
Step 11, loss: 628.015869140625
Step 12, loss: 620.095947265625
Step 13, loss: 610.6519775390625
Step 14, loss: 600.186279296875
Step 15, loss: 586.7763671875
Step 16, loss: 573.914794921875
Step 17, loss: 562.3563232421875
Step 18, loss: 551.2271728515625
Step 19, loss: 540.3890991210938
Step 20, loss: 528.2576293945312
Step 21, loss: 515.8230590820312
Step 22, loss: 504.189453125
Step 23, loss: 495.6986389160156
Step 24, loss: 490.6004943847656
Step 25, loss: 487.04315185546875
Step 26, loss: 484.8735656738281
Step 27, loss: 483.8617248535156
Step 28, loss: 482.890380859375
Step 29, loss: 482.0888366699219
Step 30, loss: 481.4444580078125
Step 31, loss: 480.87591552734375
Step 32, loss: 480.3450927734375
Step 33, loss: 479.8505859375
Step 34, loss: 479.3324279785156
Step 35, loss: 478.8662414550781
Step 36, loss: 478.4164123535156
Step 37, loss: 477.9266662597656
Step 38, loss: 477.4493103027344
Step 39, loss: 477.0159606933594
Step 40, loss: 476.5731506347656
Step 41, loss: 476.2090148925781
Step 42, loss: 475.762451171875
Step 43, loss: 475.311767578125
Step 44, loss: 474.9621276855469
Step 45, loss: 474.6258544921875
Step 46, loss: 474.2523193359375
Step 47, loss: 473.9479064941406
Step 48, loss: 473.6134948730469
Step 49, loss: 473.3572692871094
Step 50, loss: 473.090087890625
Step 51, loss: 472.8287658691406
Step 52, loss: 472.6268005371094
Step 53, loss: 472.4125671386719
Step 54, loss: 472.2037658691406
Step 55, loss: 472.0378723144531
Step 56, loss: 471.8644104003906
Step 57, loss: 471.6763000488281
Step 58, loss: 471.5212097167969
Step 59, loss: 471.353271484375
Step 60, loss: 471.2029724121094
Step 61, loss: 471.0482177734375
Step 62, loss: 470.8651123046875
Step 63, loss: 470.69403076171875
Step 64, loss: 470.50677490234375
Step 65, loss: 470.3479309082031
Step 66, loss: 470.1579895019531
Step 67, loss: 469.9725036621094
Step 68, loss: 469.7947692871094
Step 69, loss: 469.64141845703125
Step 70, loss: 469.4754943847656
Step 71, loss: 469.2656555175781
Step 72, loss: 469.1091613769531
Step 73, loss: 468.9600524902344
Step 74, loss: 468.8179016113281
Step 75, loss: 468.6765441894531
Step 76, loss: 468.5271911621094
Step 77, loss: 468.4319152832031
Step 78, loss: 468.2781066894531
Step 79, loss: 468.16802978515625
Step 80, loss: 468.0694885253906
Step 81, loss: 467.9336853027344
Step 82, loss: 467.8317565917969
Step 83, loss: 467.736083984375
Step 84, loss: 467.6247253417969
Step 85, loss: 467.5427551269531
Step 86, loss: 467.4803161621094
Step 87, loss: 467.412109375
Step 88, loss: 467.3359680175781
Step 89, loss: 467.2853698730469
Step 90, loss: 467.2684326171875
Step 91, loss: 467.236083984375
Step 92, loss: 467.22802734375
Step 93, loss: 467.170166015625
Step 94, loss: 467.1468811035156
Step 95, loss: 467.1289367675781
Step 96, loss: 467.0983581542969
Step 97, loss: 467.06298828125
Step 98, loss: 467.0448303222656
Step 99, loss: 467.0170593261719
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 582.8048095703125
Step 1, loss: 590.7138061523438
Step 2, loss: 597.5217895507812
Step 3, loss: 604.416015625
Step 4, loss: 612.1593627929688
Step 5, loss: 620.3184814453125
Step 6, loss: 627.5073852539062
Step 7, loss: 631.7261962890625
Step 8, loss: 634.10498046875
Step 9, loss: 634.5337524414062
Step 10, loss: 632.6005249023438
Step 11, loss: 627.7282104492188
Step 12, loss: 619.9285888671875
Step 13, loss: 610.7716674804688
Step 14, loss: 600.7799072265625
Step 15, loss: 587.7332153320312
Step 16, loss: 575.0674438476562
Step 17, loss: 563.8441772460938
Step 18, loss: 553.0362548828125
Step 19, loss: 542.0755615234375
Step 20, loss: 531.1239624023438
Step 21, loss: 519.3148803710938
Step 22, loss: 507.60614013671875
Step 23, loss: 497.8397521972656
Step 24, loss: 491.5806579589844
Step 25, loss: 487.85821533203125
Step 26, loss: 485.30682373046875
Step 27, loss: 484.11004638671875
Step 28, loss: 483.25927734375
Step 29, loss: 482.44183349609375
Step 30, loss: 481.6874084472656
Step 31, loss: 481.0643005371094
Step 32, loss: 480.53643798828125
Step 33, loss: 480.0338439941406
Step 34, loss: 479.482421875
Step 35, loss: 479.0115661621094
Step 36, loss: 478.5014953613281
Step 37, loss: 478.0509948730469
Step 38, loss: 477.5639343261719
Step 39, loss: 477.1045837402344
Step 40, loss: 476.6192321777344
Step 41, loss: 476.2375183105469
Step 42, loss: 475.8673400878906
Step 43, loss: 475.4339904785156
Step 44, loss: 475.0863037109375
Step 45, loss: 474.7613830566406
Step 46, loss: 474.4031677246094
Step 47, loss: 474.07940673828125
Step 48, loss: 473.7804870605469
Step 49, loss: 473.4647521972656
Step 50, loss: 473.2470397949219
Step 51, loss: 473.00274658203125
Step 52, loss: 472.7944030761719
Step 53, loss: 472.5372619628906
Step 54, loss: 472.3734130859375
Step 55, loss: 472.1080017089844
Step 56, loss: 471.97723388671875
Step 57, loss: 471.794921875
Step 58, loss: 471.6504211425781
Step 59, loss: 471.44512939453125
Step 60, loss: 471.2679748535156
Step 61, loss: 471.1038513183594
Step 62, loss: 470.9164733886719
Step 63, loss: 470.69989013671875
Step 64, loss: 470.46380615234375
Step 65, loss: 470.27166748046875
Step 66, loss: 470.1045227050781
Step 67, loss: 469.8784484863281
Step 68, loss: 469.6990661621094
Step 69, loss: 469.4863586425781
Step 70, loss: 469.3184509277344
Step 71, loss: 469.1145324707031
Step 72, loss: 468.9530944824219
Step 73, loss: 468.7762451171875
Step 74, loss: 468.6103820800781
Step 75, loss: 468.45391845703125
Step 76, loss: 468.29632568359375
Step 77, loss: 468.1677551269531
Step 78, loss: 468.0296325683594
Step 79, loss: 467.9126281738281
Step 80, loss: 467.80767822265625
Step 81, loss: 467.6961364746094
Step 82, loss: 467.5887145996094
Step 83, loss: 467.4781494140625
Step 84, loss: 467.4232482910156
Step 85, loss: 467.3531188964844
Step 86, loss: 467.334716796875
Step 87, loss: 467.29986572265625
Step 88, loss: 467.26190185546875
Step 89, loss: 467.2154846191406
Step 90, loss: 467.17352294921875
Step 91, loss: 467.1575622558594
Step 92, loss: 467.1251220703125
Step 93, loss: 467.0795593261719
Step 94, loss: 467.0469665527344
Step 95, loss: 467.0142517089844
Step 96, loss: 467.01556396484375
Step 97, loss: 466.9892883300781
Step 98, loss: 466.93463134765625
Step 99, loss: 466.91546630859375
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 0.69799257,  0.04445023,  0.71472411,  0.04461503],
       [ 0.70917147,  0.09564524, -0.6985183 ,  0.68846016],
       [-0.09940926,  0.99442253,  0.03523683,  0.18537648],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.51668989, -0.41564613,  0.74851176,  0.25333781],
       [-0.70410173, -0.70367722,  0.09528445,  0.93438798],
       [ 0.48710606, -0.57626093, -0.65624006,  0.59592925],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[-0.19725855, -0.06202209,  0.97838761,  0.40945659],
       [-0.96509834, -0.16307728, -0.20491704,  0.8586233 ],
       [ 0.17226217, -0.9846619 , -0.02768903, -0.03995694],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[ 0.69799257,  0.04445023,  0.71472411,  0.04461503],
       [ 0.70917147,  0.09564524, -0.6985183 ,  0.68846016],
       [-0.09940926,  0.99442253,  0.03523683,  0.18537648],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[ 0.76794779, -0.36589932,  0.5257135 ,  0.00296306],
       [-0.06195645, -0.85935628, -0.50761127,  0.8930304 ],
       [ 0.63750941,  0.35724753, -0.68261051, -0.09050175],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True

loading data:   0%|          | 0/5 [00:00<?, ?it/s]
loading data: 100%|██████████| 5/5 [00:00<00:00, 55.48it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/70 [00:00<?, ?it/s]
loading data:  10%|█         | 7/70 [00:00<00:01, 60.00it/s]
loading data:  20%|██        | 14/70 [00:00<00:00, 62.60it/s]
loading data:  30%|███       | 21/70 [00:00<00:00, 63.81it/s]
loading data:  40%|████      | 28/70 [00:00<00:00, 63.21it/s]
loading data:  50%|█████     | 35/70 [00:00<00:00, 63.30it/s]
loading data:  60%|██████    | 42/70 [00:00<00:00, 63.45it/s]
loading data:  70%|███████   | 49/70 [00:00<00:00, 62.95it/s]
loading data:  80%|████████  | 56/70 [00:00<00:00, 61.46it/s]
loading data:  90%|█████████ | 63/70 [00:01<00:00, 60.49it/s]
loading data: 100%|██████████| 70/70 [00:01<00:00, 60.02it/s]
loading data: 100%|██████████| 70/70 [00:01<00:00, 61.62it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.21898546814918518
Loss after optimization 0.046244241297245026
logm result may be inaccurate, approximate err = 5.556911638149558e-07
Loss at initial point 0.19265128672122955
Loss after optimization 0.04729042947292328
logm result may be inaccurate, approximate err = 1.128673642260509e-06
Loss at initial point 0.07006784528493881
Loss after optimization 0.0258978009223938
logm result may be inaccurate, approximate err = 1.3370880886420282e-06
Loss at initial point 0.21413876116275787
Loss after optimization 0.03756657987833023
logm result may be inaccurate, approximate err = 1.4501536906004579e-06
Loss at initial point 0.20250912010669708
Loss after optimization 0.019440680742263794
logm result may be inaccurate, approximate err = 2.7028296072760977e-07
Loss at initial point 0.1894560605287552
Loss after optimization 0.03451661020517349
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.21900574862957
Loss after optimization 0.0814870148897171
logm result may be inaccurate, approximate err = 5.556911638149558e-07
Loss at initial point 0.19254626333713531
Loss after optimization 0.059484343975782394
logm result may be inaccurate, approximate err = 1.128673642260509e-06
Loss at initial point 0.07003554701805115
Loss after optimization 0.02887481264770031
logm result may be inaccurate, approximate err = 1.3370880886420282e-06
Loss at initial point 0.21407781541347504
Loss after optimization 0.04373234882950783
logm result may be inaccurate, approximate err = 1.4501536906004579e-06
Loss at initial point 0.20249994099140167
Loss after optimization 0.06769274175167084
logm result may be inaccurate, approximate err = 2.7028296072760977e-07
Loss at initial point 0.18961390852928162
Loss after optimization 0.0676940530538559
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.21903952956199646
Loss after optimization 0.017978589981794357
logm result may be inaccurate, approximate err = 5.556911638149558e-07
Loss at initial point 0.1925818771123886
Loss after optimization 0.017989058047533035
logm result may be inaccurate, approximate err = 1.128673642260509e-06
Loss at initial point 0.07007641345262527
Loss after optimization 0.020249048247933388
logm result may be inaccurate, approximate err = 1.3370880886420282e-06
Loss at initial point 0.2141111195087433
Loss after optimization 0.018180565908551216
logm result may be inaccurate, approximate err = 1.4501536906004579e-06
Loss at initial point 0.20251822471618652
Loss after optimization 0.01805577427148819
logm result may be inaccurate, approximate err = 2.7028296072760977e-07
Loss at initial point 0.1895737648010254
Loss after optimization 0.03372783213853836
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 140.2376708984375
Step 1, loss: 131.88597106933594
Step 2, loss: 125.52881622314453
Step 3, loss: 119.7314224243164
Step 4, loss: 115.0598373413086
Step 5, loss: 111.51913452148438
Step 6, loss: 108.68021392822266
Step 7, loss: 106.36800384521484
Step 8, loss: 104.90234375
Step 9, loss: 104.1158447265625
Step 10, loss: 103.63007354736328
Step 11, loss: 103.19082641601562
Step 12, loss: 102.6772689819336
Step 13, loss: 102.10964965820312
Step 14, loss: 101.42393493652344
Step 15, loss: 100.40164184570312
Step 16, loss: 99.14466094970703
Step 17, loss: 97.29196166992188
Step 18, loss: 95.19265747070312
Step 19, loss: 93.00501251220703
Step 20, loss: 90.50904846191406
Step 21, loss: 87.94325256347656
Step 22, loss: 85.0224380493164
Step 23, loss: 82.6797103881836
Step 24, loss: 81.56615447998047
Step 25, loss: 82.1170425415039
Step 26, loss: 84.33578491210938
Step 27, loss: 87.34257507324219
Step 28, loss: 90.8896484375
Step 29, loss: 93.27153015136719
Step 30, loss: 94.93035125732422
Step 31, loss: 95.38300323486328
Step 32, loss: 95.76766204833984
Step 33, loss: 96.33064270019531
Step 34, loss: 97.05133056640625
Step 35, loss: 97.49030303955078
Step 36, loss: 97.70352172851562
Step 37, loss: 97.7724609375
Step 38, loss: 97.52952575683594
Step 39, loss: 97.06169891357422
Step 40, loss: 96.53152465820312
Step 41, loss: 95.99561309814453
Step 42, loss: 95.43360137939453
Step 43, loss: 94.86970520019531
Step 44, loss: 94.35273742675781
Step 45, loss: 94.3819808959961
Step 46, loss: 94.4625244140625
Step 47, loss: 94.28926849365234
Step 48, loss: 93.99071502685547
Step 49, loss: 93.71717834472656
Step 50, loss: 93.47293853759766
Step 51, loss: 93.27688598632812
Step 52, loss: 93.0510025024414
Step 53, loss: 92.79810333251953
Step 54, loss: 92.56461334228516
Step 55, loss: 92.22899627685547
Step 56, loss: 91.1965560913086
Step 57, loss: 90.1795654296875
Step 58, loss: 89.72882080078125
Step 59, loss: 89.50887298583984
Step 60, loss: 89.6128158569336
Step 61, loss: 89.83529663085938
Step 62, loss: 90.07060241699219
Step 63, loss: 90.29407501220703
Step 64, loss: 90.49534606933594
Step 65, loss: 90.74925994873047
Step 66, loss: 90.702392578125
Step 67, loss: 90.54685974121094
Step 68, loss: 90.3322982788086
Step 69, loss: 90.41451263427734
Step 70, loss: 90.58476257324219
Step 71, loss: 90.5696029663086
Step 72, loss: 90.37147521972656
Step 73, loss: 90.06363677978516
Step 74, loss: 89.92267608642578
Step 75, loss: 89.92582702636719
Step 76, loss: 90.02459716796875
Step 77, loss: 90.00050354003906
Step 78, loss: 89.82347869873047
Step 79, loss: 89.81246185302734
Step 80, loss: 89.75155639648438
Step 81, loss: 89.40619659423828
Step 82, loss: 89.28057098388672
Step 83, loss: 89.1585693359375
Step 84, loss: 89.0079345703125
Step 85, loss: 89.04861450195312
Step 86, loss: 88.76612091064453
Step 87, loss: 88.7518081665039
Step 88, loss: 88.93760681152344
Step 89, loss: 88.87359619140625
Step 90, loss: 88.9840316772461
Step 91, loss: 89.08686828613281
Step 92, loss: 89.05030059814453
Step 93, loss: 89.0005111694336
Step 94, loss: 89.02603149414062
Step 95, loss: 89.65545654296875
Step 96, loss: 90.41504669189453
Step 97, loss: 91.23524475097656
Step 98, loss: 91.93518829345703
Step 99, loss: 92.5525131225586
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 140.2128143310547
Step 1, loss: 131.89891052246094
Step 2, loss: 125.38016510009766
Step 3, loss: 119.62545013427734
Step 4, loss: 115.04359436035156
Step 5, loss: 111.67243957519531
Step 6, loss: 108.8278579711914
Step 7, loss: 106.70616149902344
Step 8, loss: 105.28717041015625
Step 9, loss: 104.31417846679688
Step 10, loss: 103.45021057128906
Step 11, loss: 102.71971893310547
Step 12, loss: 101.83108520507812
Step 13, loss: 101.02625274658203
Step 14, loss: 100.05148315429688
Step 15, loss: 98.73043060302734
Step 16, loss: 97.031982421875
Step 17, loss: 94.81785583496094
Step 18, loss: 92.26464080810547
Step 19, loss: 89.35919952392578
Step 20, loss: 86.01353454589844
Step 21, loss: 81.98918151855469
Step 22, loss: 78.14092254638672
Step 23, loss: 75.58280181884766
Step 24, loss: 75.62010955810547
Step 25, loss: 77.38048553466797
Step 26, loss: 80.12808990478516
Step 27, loss: 84.44554138183594
Step 28, loss: 89.11395263671875
Step 29, loss: 93.32913970947266
Step 30, loss: 96.17476654052734
Step 31, loss: 96.90243530273438
Step 32, loss: 96.9947738647461
Step 33, loss: 96.7667236328125
Step 34, loss: 96.14437866210938
Step 35, loss: 95.84088897705078
Step 36, loss: 95.9156494140625
Step 37, loss: 96.37857818603516
Step 38, loss: 96.79915618896484
Step 39, loss: 96.96134185791016
Step 40, loss: 96.95964050292969
Step 41, loss: 96.67378234863281
Step 42, loss: 96.13640594482422
Step 43, loss: 95.76014709472656
Step 44, loss: 95.61133575439453
Step 45, loss: 95.32057189941406
Step 46, loss: 95.02082061767578
Step 47, loss: 94.77691650390625
Step 48, loss: 94.39532470703125
Step 49, loss: 94.31983947753906
Step 50, loss: 93.6046371459961
Step 51, loss: 92.87508392333984
Step 52, loss: 92.4209976196289
Step 53, loss: 92.01104736328125
Step 54, loss: 91.7169418334961
Step 55, loss: 91.47956085205078
Step 56, loss: 91.18799591064453
Step 57, loss: 90.69146728515625
Step 58, loss: 90.18352508544922
Step 59, loss: 89.85748291015625
Step 60, loss: 89.71138763427734
Step 61, loss: 89.6952896118164
Step 62, loss: 89.92595672607422
Step 63, loss: 90.06094360351562
Step 64, loss: 90.30992126464844
Step 65, loss: 90.29544067382812
Step 66, loss: 90.2122802734375
Step 67, loss: 90.06568908691406
Step 68, loss: 89.93246459960938
Step 69, loss: 89.77527618408203
Step 70, loss: 89.6624755859375
Step 71, loss: 89.5868911743164
Step 72, loss: 89.58539581298828
Step 73, loss: 89.60167694091797
Step 74, loss: 89.67005157470703
Step 75, loss: 89.67072296142578
Step 76, loss: 89.71630859375
Step 77, loss: 89.48320007324219
Step 78, loss: 89.50299835205078
Step 79, loss: 89.27098083496094
Step 80, loss: 89.32402801513672
Step 81, loss: 89.46768951416016
Step 82, loss: 89.68631744384766
Step 83, loss: 89.86125183105469
Step 84, loss: 89.99274444580078
Step 85, loss: 89.90042877197266
Step 86, loss: 89.80089569091797
Step 87, loss: 89.56288146972656
Step 88, loss: 89.43704223632812
Step 89, loss: 89.27854919433594
Step 90, loss: 89.0754165649414
Step 91, loss: 88.9849853515625
Step 92, loss: 88.90695190429688
Step 93, loss: 88.83002471923828
Step 94, loss: 88.74132537841797
Step 95, loss: 88.63859558105469
Step 96, loss: 88.60749816894531
Step 97, loss: 88.5625228881836
Step 98, loss: 88.81253051757812
Step 99, loss: 89.23397064208984
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 140.1697235107422
Step 1, loss: 131.89816284179688
Step 2, loss: 125.45368194580078
Step 3, loss: 119.69244384765625
Step 4, loss: 115.07713317871094
Step 5, loss: 111.72127532958984
Step 6, loss: 108.8685531616211
Step 7, loss: 106.80076599121094
Step 8, loss: 105.49278259277344
Step 9, loss: 104.62073516845703
Step 10, loss: 103.92493438720703
Step 11, loss: 103.28433227539062
Step 12, loss: 102.81385040283203
Step 13, loss: 102.3613510131836
Step 14, loss: 101.81590270996094
Step 15, loss: 101.0593032836914
Step 16, loss: 99.98509979248047
Step 17, loss: 98.48939514160156
Step 18, loss: 96.3723373413086
Step 19, loss: 93.9833755493164
Step 20, loss: 91.4477310180664
Step 21, loss: 88.5787582397461
Step 22, loss: 85.55699157714844
Step 23, loss: 83.18195343017578
Step 24, loss: 82.23677825927734
Step 25, loss: 83.12965393066406
Step 26, loss: 85.14080810546875
Step 27, loss: 88.06989288330078
Step 28, loss: 91.75172424316406
Step 29, loss: 94.64445495605469
Step 30, loss: 95.93568420410156
Step 31, loss: 96.15288543701172
Step 32, loss: 96.58808135986328
Step 33, loss: 96.87938690185547
Step 34, loss: 97.42345428466797
Step 35, loss: 97.81585693359375
Step 36, loss: 98.33135986328125
Step 37, loss: 98.9478530883789
Step 38, loss: 99.39977264404297
Step 39, loss: 99.5211410522461
Step 40, loss: 99.31570434570312
Step 41, loss: 99.21537780761719
Step 42, loss: 98.78844451904297
Step 43, loss: 98.07160949707031
Step 44, loss: 97.06340789794922
Step 45, loss: 95.9380111694336
Step 46, loss: 94.76010131835938
Step 47, loss: 93.9736328125
Step 48, loss: 93.50204467773438
Step 49, loss: 92.1602783203125
Step 50, loss: 91.95925903320312
Step 51, loss: 91.62153625488281
Step 52, loss: 91.67240905761719
Step 53, loss: 91.81461334228516
Step 54, loss: 92.0020523071289
Step 55, loss: 91.98858642578125
Step 56, loss: 91.28423309326172
Step 57, loss: 90.60372924804688
Step 58, loss: 90.03731536865234
Step 59, loss: 89.5818862915039
Step 60, loss: 89.14820098876953
Step 61, loss: 88.95105743408203
Step 62, loss: 88.7265625
Step 63, loss: 88.462890625
Step 64, loss: 88.12202453613281
Step 65, loss: 87.94326782226562
Step 66, loss: 88.0021743774414
Step 67, loss: 88.1094741821289
Step 68, loss: 88.43989562988281
Step 69, loss: 88.75971221923828
Step 70, loss: 89.2030029296875
Step 71, loss: 89.44170379638672
Step 72, loss: 89.76824951171875
Step 73, loss: 90.04826354980469
Step 74, loss: 90.327392578125
Step 75, loss: 90.50992584228516
Step 76, loss: 90.48230743408203
Step 77, loss: 90.39384460449219
Step 78, loss: 90.41820526123047
Step 79, loss: 90.078857421875
Step 80, loss: 89.81507110595703
Step 81, loss: 89.63311767578125
Step 82, loss: 89.5440673828125
Step 83, loss: 89.5796127319336
Step 84, loss: 89.75098419189453
Step 85, loss: 89.9422378540039
Step 86, loss: 90.1443862915039
Step 87, loss: 90.27832794189453
Step 88, loss: 90.32119750976562
Step 89, loss: 90.01944732666016
Step 90, loss: 89.74921417236328
Step 91, loss: 89.7173080444336
Step 92, loss: 89.55941772460938
Step 93, loss: 89.47126770019531
Step 94, loss: 89.33450317382812
Step 95, loss: 88.96924591064453
Step 96, loss: 88.8714599609375
Step 97, loss: 89.2608642578125
Step 98, loss: 89.86029052734375
Step 99, loss: 90.6133041381836
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 140.2125244140625
Step 1, loss: 131.90406799316406
Step 2, loss: 125.44693756103516
Step 3, loss: 119.7048568725586
Step 4, loss: 115.07763671875
Step 5, loss: 111.68062591552734
Step 6, loss: 108.82019805908203
Step 7, loss: 106.67130279541016
Step 8, loss: 105.32244110107422
Step 9, loss: 104.4670181274414
Step 10, loss: 103.82767486572266
Step 11, loss: 103.225830078125
Step 12, loss: 102.64252471923828
Step 13, loss: 102.05833435058594
Step 14, loss: 101.15361022949219
Step 15, loss: 99.98539733886719
Step 16, loss: 98.34613037109375
Step 17, loss: 96.34391021728516
Step 18, loss: 93.87690734863281
Step 19, loss: 91.35016632080078
Step 20, loss: 88.7419662475586
Step 21, loss: 85.82170867919922
Step 22, loss: 82.88726043701172
Step 23, loss: 80.94803619384766
Step 24, loss: 80.3425064086914
Step 25, loss: 81.50796508789062
Step 26, loss: 84.3245620727539
Step 27, loss: 88.1027603149414
Step 28, loss: 92.33290100097656
Step 29, loss: 96.17240142822266
Step 30, loss: 98.0858383178711
Step 31, loss: 98.1285629272461
Step 32, loss: 97.08258819580078
Step 33, loss: 96.50920867919922
Step 34, loss: 96.41458129882812
Step 35, loss: 96.93941497802734
Step 36, loss: 97.19377899169922
Step 37, loss: 98.1119613647461
Step 38, loss: 98.8412857055664
Step 39, loss: 99.29484558105469
Step 40, loss: 99.35887908935547
Step 41, loss: 99.14933776855469
Step 42, loss: 98.90835571289062
Step 43, loss: 98.9167709350586
Step 44, loss: 98.49451446533203
Step 45, loss: 97.90190124511719
Step 46, loss: 97.07981872558594
Step 47, loss: 96.06290435791016
Step 48, loss: 94.99385070800781
Step 49, loss: 94.11262512207031
Step 50, loss: 92.07038116455078
Step 51, loss: 91.13103485107422
Step 52, loss: 90.27104949951172
Step 53, loss: 89.83622741699219
Step 54, loss: 89.57481384277344
Step 55, loss: 89.7099609375
Step 56, loss: 89.98945617675781
Step 57, loss: 89.91146087646484
Step 58, loss: 90.08724975585938
Step 59, loss: 90.41805267333984
Step 60, loss: 90.98957061767578
Step 61, loss: 91.2877197265625
Step 62, loss: 91.45867919921875
Step 63, loss: 91.4495849609375
Step 64, loss: 91.21385955810547
Step 65, loss: 90.75773620605469
Step 66, loss: 90.16392517089844
Step 67, loss: 89.7876968383789
Step 68, loss: 89.3543930053711
Step 69, loss: 89.10098266601562
Step 70, loss: 88.86927795410156
Step 71, loss: 88.74909210205078
Step 72, loss: 88.82566833496094
Step 73, loss: 89.0731430053711
Step 74, loss: 89.33661651611328
Step 75, loss: 89.61782836914062
Step 76, loss: 89.71581268310547
Step 77, loss: 89.7276611328125
Step 78, loss: 89.64470672607422
Step 79, loss: 89.68022918701172
Step 80, loss: 89.51339721679688
Step 81, loss: 89.49020385742188
Step 82, loss: 89.51952362060547
Step 83, loss: 89.5240478515625
Step 84, loss: 89.63030242919922
Step 85, loss: 89.81070709228516
Step 86, loss: 89.83164978027344
Step 87, loss: 90.0325698852539
Step 88, loss: 90.0541763305664
Step 89, loss: 89.98703002929688
Step 90, loss: 89.79885864257812
Step 91, loss: 89.21208953857422
Step 92, loss: 88.83222961425781
Step 93, loss: 88.50357055664062
Step 94, loss: 88.3252182006836
Step 95, loss: 88.2773208618164
Step 96, loss: 88.26232147216797
Step 97, loss: 88.40372467041016
Step 98, loss: 88.47432708740234
Step 99, loss: 88.63801574707031
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 140.16265869140625
Step 1, loss: 131.9208526611328
Step 2, loss: 125.42774200439453
Step 3, loss: 119.66612243652344
Step 4, loss: 115.10088348388672
Step 5, loss: 111.65961456298828
Step 6, loss: 108.76033782958984
Step 7, loss: 106.64833068847656
Step 8, loss: 105.24259185791016
Step 9, loss: 104.26527404785156
Step 10, loss: 103.41944122314453
Step 11, loss: 102.6036376953125
Step 12, loss: 101.72196960449219
Step 13, loss: 100.66094207763672
Step 14, loss: 99.2068862915039
Step 15, loss: 97.4337387084961
Step 16, loss: 95.20063781738281
Step 17, loss: 92.25025177001953
Step 18, loss: 88.69853210449219
Step 19, loss: 84.2748031616211
Step 20, loss: 78.85965728759766
Step 21, loss: 73.46739959716797
Step 22, loss: 68.23152923583984
Step 23, loss: 64.26502990722656
Step 24, loss: 62.12455749511719
Step 25, loss: 62.05281448364258
Step 26, loss: 64.48788452148438
Step 27, loss: 67.9763412475586
Step 28, loss: 70.39087677001953
Step 29, loss: 72.1057357788086
Step 30, loss: 75.41071319580078
Step 31, loss: 78.34444427490234
Step 32, loss: 80.71641540527344
Step 33, loss: 82.81455993652344
Step 34, loss: 85.09210968017578
Step 35, loss: 87.1962890625
Step 36, loss: 88.94950103759766
Step 37, loss: 90.40227508544922
Step 38, loss: 91.10514068603516
Step 39, loss: 91.27796173095703
Step 40, loss: 91.18071746826172
Step 41, loss: 90.74506378173828
Step 42, loss: 90.0760726928711
Step 43, loss: 89.40541076660156
Step 44, loss: 88.60265350341797
Step 45, loss: 88.21125030517578
Step 46, loss: 87.98371124267578
Step 47, loss: 88.3858871459961
Step 48, loss: 89.09972381591797
Step 49, loss: 89.69001770019531
Step 50, loss: 90.05326080322266
Step 51, loss: 90.32012939453125
Step 52, loss: 90.4237060546875
Step 53, loss: 90.6309585571289
Step 54, loss: 90.66641235351562
Step 55, loss: 90.50426483154297
Step 56, loss: 90.30115509033203
Step 57, loss: 90.0130386352539
Step 58, loss: 89.47573852539062
Step 59, loss: 89.04096984863281
Step 60, loss: 88.6632308959961
Step 61, loss: 88.20409393310547
Step 62, loss: 88.16474151611328
Step 63, loss: 88.2896728515625
Step 64, loss: 88.65107727050781
Step 65, loss: 89.0872573852539
Step 66, loss: 89.45015716552734
Step 67, loss: 90.00048065185547
Step 68, loss: 90.39566802978516
Step 69, loss: 90.57703399658203
Step 70, loss: 90.61766052246094
Step 71, loss: 90.56502532958984
Step 72, loss: 90.19644927978516
Step 73, loss: 89.9652328491211
Step 74, loss: 89.75180053710938
Step 75, loss: 89.72346496582031
Step 76, loss: 89.73426818847656
Step 77, loss: 89.68885040283203
Step 78, loss: 89.53304290771484
Step 79, loss: 89.30845642089844
Step 80, loss: 89.00372314453125
Step 81, loss: 88.92243957519531
Step 82, loss: 88.89197540283203
Step 83, loss: 88.98188781738281
Step 84, loss: 89.20406341552734
Step 85, loss: 89.23593139648438
Step 86, loss: 89.11292266845703
Step 87, loss: 88.99169921875
Step 88, loss: 88.8010482788086
Step 89, loss: 88.82311248779297
Step 90, loss: 88.911376953125
Step 91, loss: 88.9874267578125
Step 92, loss: 89.0009994506836
Step 93, loss: 89.17686462402344
Step 94, loss: 89.4382553100586
Step 95, loss: 89.76692962646484
Step 96, loss: 90.1064682006836
Step 97, loss: 90.36729431152344
Step 98, loss: 90.4976577758789
Step 99, loss: 90.51380920410156
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 140.2222900390625
Step 1, loss: 131.93304443359375
Step 2, loss: 125.49054718017578
Step 3, loss: 119.68839263916016
Step 4, loss: 115.12981414794922
Step 5, loss: 111.73019409179688
Step 6, loss: 108.89214324951172
Step 7, loss: 106.78941345214844
Step 8, loss: 105.48609924316406
Step 9, loss: 104.68656921386719
Step 10, loss: 104.06747436523438
Step 11, loss: 103.43695831298828
Step 12, loss: 103.0531005859375
Step 13, loss: 102.57551574707031
Step 14, loss: 101.7017822265625
Step 15, loss: 100.48493957519531
Step 16, loss: 98.8492660522461
Step 17, loss: 96.59786224365234
Step 18, loss: 93.71066284179688
Step 19, loss: 90.29010772705078
Step 20, loss: 86.0604248046875
Step 21, loss: 81.17932891845703
Step 22, loss: 75.38815307617188
Step 23, loss: 70.17912292480469
Step 24, loss: 66.24378967285156
Step 25, loss: 64.76016235351562
Step 26, loss: 65.7022705078125
Step 27, loss: 68.22969818115234
Step 28, loss: 70.48779296875
Step 29, loss: 72.48416137695312
Step 30, loss: 74.59077453613281
Step 31, loss: 76.83773803710938
Step 32, loss: 78.99910736083984
Step 33, loss: 81.01500701904297
Step 34, loss: 83.19426727294922
Step 35, loss: 85.70882415771484
Step 36, loss: 88.50425720214844
Step 37, loss: 90.59384155273438
Step 38, loss: 91.80175018310547
Step 39, loss: 92.68795013427734
Step 40, loss: 93.34534454345703
Step 41, loss: 93.46357727050781
Step 42, loss: 93.51344299316406
Step 43, loss: 92.99906921386719
Step 44, loss: 92.16536712646484
Step 45, loss: 91.15425872802734
Step 46, loss: 89.94646453857422
Step 47, loss: 88.75262451171875
Step 48, loss: 88.00206756591797
Step 49, loss: 87.85645294189453
Step 50, loss: 88.4112548828125
Step 51, loss: 89.04165649414062
Step 52, loss: 89.55757141113281
Step 53, loss: 89.89368438720703
Step 54, loss: 90.36116027832031
Step 55, loss: 90.52487182617188
Step 56, loss: 90.6598892211914
Step 57, loss: 90.83946228027344
Step 58, loss: 90.97464752197266
Step 59, loss: 91.10608673095703
Step 60, loss: 90.9300537109375
Step 61, loss: 90.72931671142578
Step 62, loss: 90.54955291748047
Step 63, loss: 90.25615692138672
Step 64, loss: 89.97003173828125
Step 65, loss: 89.59961700439453
Step 66, loss: 89.31004333496094
Step 67, loss: 88.97261810302734
Step 68, loss: 88.72025299072266
Step 69, loss: 88.60021209716797
Step 70, loss: 88.60631561279297
Step 71, loss: 88.61353302001953
Step 72, loss: 88.65678405761719
Step 73, loss: 88.535400390625
Step 74, loss: 88.42265319824219
Step 75, loss: 88.48519134521484
Step 76, loss: 88.59172821044922
Step 77, loss: 88.71186828613281
Step 78, loss: 88.84896850585938
Step 79, loss: 89.04443359375
Step 80, loss: 89.16748809814453
Step 81, loss: 89.28129577636719
Step 82, loss: 89.46959686279297
Step 83, loss: 89.65602111816406
Step 84, loss: 89.802978515625
Step 85, loss: 89.95877075195312
Step 86, loss: 89.94595336914062
Step 87, loss: 89.96161651611328
Step 88, loss: 89.7700424194336
Step 89, loss: 89.59664916992188
Step 90, loss: 89.69883728027344
Step 91, loss: 89.5616455078125
Step 92, loss: 89.3435287475586
Step 93, loss: 89.08720397949219
Step 94, loss: 88.63849639892578
Step 95, loss: 88.45299530029297
Step 96, loss: 88.38800811767578
Step 97, loss: 88.20633697509766
Step 98, loss: 88.23658752441406
Step 99, loss: 88.28204345703125
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 0.60979954, -0.01112029,  0.79247767,  0.18852463],
       [-0.7925256 , -0.01726926,  0.60959409,  0.75705977],
       [ 0.00690663, -0.99978903, -0.0193439 ,  0.05076511],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.63898127,  0.0940444 ,  0.76345176,  0.19027697],
       [-0.75792443,  0.24645311,  0.60399621,  0.77878058],
       [-0.1313526 , -0.96458101,  0.22875747,  0.12194618],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 6.23922062e-01,  9.21351255e-03,  7.81432257e-01,
         3.89964235e-02],
       [-7.81324579e-01, -1.30043036e-02,  6.23989416e-01,
         7.58798633e-01],
       [ 1.59111166e-02, -9.99872992e-01, -9.14928126e-04,
         1.61442236e-01],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'total': array([[ 6.23922062e-01,  9.21351255e-03,  7.81432257e-01,
         3.89964235e-02],
       [-7.81324579e-01, -1.30043036e-02,  6.23989416e-01,
         7.58798633e-01],
       [ 1.59111166e-02, -9.99872992e-01, -9.14928126e-04,
         1.61442236e-01],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'grad': array([[ 0.54687822,  0.02184313,  0.83692694,  0.01406414],
       [-0.54176211, -0.75291014,  0.37365809,  0.79409763],
       [ 0.63829255, -0.65776175, -0.39991635,  0.07305652],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True

loading data:   0%|          | 0/5 [00:00<?, ?it/s]
loading data: 100%|██████████| 5/5 [00:00<00:00, 55.19it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666

loading data:   0%|          | 0/70 [00:00<?, ?it/s]
loading data:  10%|█         | 7/70 [00:00<00:00, 65.13it/s]
loading data:  20%|██        | 14/70 [00:00<00:00, 66.88it/s]
loading data:  30%|███       | 21/70 [00:00<00:00, 66.46it/s]
loading data:  40%|████      | 28/70 [00:00<00:00, 63.92it/s]
loading data:  50%|█████     | 35/70 [00:00<00:00, 63.33it/s]
loading data:  60%|██████    | 42/70 [00:00<00:00, 62.91it/s]
loading data:  70%|███████   | 49/70 [00:00<00:00, 62.92it/s]
loading data:  80%|████████  | 56/70 [00:00<00:00, 61.44it/s]
loading data:  90%|█████████ | 63/70 [00:01<00:00, 60.57it/s]
loading data: 100%|██████████| 70/70 [00:01<00:00, 60.09it/s]
loading data: 100%|██████████| 70/70 [00:01<00:00, 62.16it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(p, device='cuda', dtype=torch.float32)
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[idx])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.8231490850448608
Loss after optimization 0.15547596290707588
logm result may be inaccurate, approximate err = 9.471301345751008e-07
Loss at initial point 0.9063246101140976
Loss after optimization 0.17603129893541336
logm result may be inaccurate, approximate err = 8.03302868217718e-07
Loss at initial point 0.90986467897892
Loss after optimization 0.12431026250123978
logm result may be inaccurate, approximate err = 4.0334876876467477e-07
Loss at initial point 1.0673959851264954
Loss after optimization 0.21059095486998558
logm result may be inaccurate, approximate err = 3.589284847391762e-07
Loss at initial point 1.097362607717514
Loss after optimization 0.6347351744771004
logm result may be inaccurate, approximate err = 2.723542655789555e-07
Loss at initial point 0.3529219999909401
Loss after optimization 0.12330816686153412
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.8230008631944656
Loss after optimization 0.1272407528012991
logm result may be inaccurate, approximate err = 9.471301345751008e-07
Loss at initial point 0.9063460975885391
Loss after optimization 0.8483563512563705
logm result may be inaccurate, approximate err = 8.03302868217718e-07
Loss at initial point 0.9097870290279388
Loss after optimization 0.8499684035778046
logm result may be inaccurate, approximate err = 4.0334876876467477e-07
Loss at initial point 1.0673038959503174
Loss after optimization 0.8393498659133911
logm result may be inaccurate, approximate err = 3.589284847391762e-07
Loss at initial point 1.0973413586616516
Loss after optimization 0.8814588189125061
logm result may be inaccurate, approximate err = 2.723542655789555e-07
Loss at initial point 0.3529685437679291
Loss after optimization 0.12749935500323772
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Loss at initial point 0.8231703341007233
Loss after optimization 0.10841267369687557
logm result may be inaccurate, approximate err = 9.471301345751008e-07
Loss at initial point 0.9064273983240128
Loss after optimization 0.8401584029197693
logm result may be inaccurate, approximate err = 8.03302868217718e-07
Loss at initial point 0.9097259342670441
Loss after optimization 0.8398588299751282
logm result may be inaccurate, approximate err = 4.0334876876467477e-07
Loss at initial point 1.0673757493495941
Loss after optimization 0.8232966959476471
logm result may be inaccurate, approximate err = 3.589284847391762e-07
Loss at initial point 1.0974711775779724
Loss after optimization 0.8231665045022964
logm result may be inaccurate, approximate err = 2.723542655789555e-07
Loss at initial point 0.35298047214746475
Loss after optimization 0.10831709206104279
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 530.7532348632812
Step 1, loss: 493.23687744140625
Step 2, loss: 472.3177795410156
Step 3, loss: 461.48101806640625
Step 4, loss: 452.59332275390625
Step 5, loss: 433.54638671875
Step 6, loss: 405.81591796875
Step 7, loss: 370.3907775878906
Step 8, loss: 334.1398620605469
Step 9, loss: 301.6215515136719
Step 10, loss: 284.5062561035156
Step 11, loss: 284.924072265625
Step 12, loss: 291.4730224609375
Step 13, loss: 293.9080810546875
Step 14, loss: 298.1368103027344
Step 15, loss: 302.7911376953125
Step 16, loss: 305.2745361328125
Step 17, loss: 306.5782165527344
Step 18, loss: 305.7823181152344
Step 19, loss: 303.6778869628906
Step 20, loss: 300.49591064453125
Step 21, loss: 295.22625732421875
Step 22, loss: 287.35125732421875
Step 23, loss: 277.21734619140625
Step 24, loss: 264.63330078125
Step 25, loss: 249.6435546875
Step 26, loss: 231.37045288085938
Step 27, loss: 210.73358154296875
Step 28, loss: 195.24989318847656
Step 29, loss: 187.84278869628906
Step 30, loss: 189.41661071777344
Step 31, loss: 199.664794921875
Step 32, loss: 209.04312133789062
Step 33, loss: 210.56593322753906
Step 34, loss: 201.31895446777344
Step 35, loss: 189.5797576904297
Step 36, loss: 188.7991943359375
Step 37, loss: 188.70909118652344
Step 38, loss: 184.31170654296875
Step 39, loss: 175.5372772216797
Step 40, loss: 165.1275177001953
Step 41, loss: 156.44357299804688
Step 42, loss: 150.40589904785156
Step 43, loss: 140.75408935546875
Step 44, loss: 128.76113891601562
Step 45, loss: 130.09194946289062
Step 46, loss: 121.9735107421875
Step 47, loss: 120.64593505859375
Step 48, loss: 123.8893814086914
Step 49, loss: 137.95114135742188
Step 50, loss: 147.54486083984375
Step 51, loss: 147.90013122558594
Step 52, loss: 141.47366333007812
Step 53, loss: 141.68478393554688
Step 54, loss: 147.71624755859375
Step 55, loss: 153.34625244140625
Step 56, loss: 158.99517822265625
Step 57, loss: 164.0954132080078
Step 58, loss: 170.70489501953125
Step 59, loss: 175.4395751953125
Step 60, loss: 176.176513671875
Step 61, loss: 176.04025268554688
Step 62, loss: 176.0755615234375
Step 63, loss: 175.87136840820312
Step 64, loss: 174.34402465820312
Step 65, loss: 173.9656524658203
Step 66, loss: 174.81338500976562
Step 67, loss: 175.19097900390625
Step 68, loss: 175.15753173828125
Step 69, loss: 178.5020751953125
Step 70, loss: 177.090087890625
Step 71, loss: 173.10708618164062
Step 72, loss: 170.54299926757812
Step 73, loss: 167.23089599609375
Step 74, loss: 165.18524169921875
Step 75, loss: 166.2223358154297
Step 76, loss: 165.89755249023438
Step 77, loss: 164.37576293945312
Step 78, loss: 164.56063842773438
Step 79, loss: 165.11940002441406
Step 80, loss: 166.11911010742188
Step 81, loss: 165.1812286376953
Step 82, loss: 163.94619750976562
Step 83, loss: 162.1783447265625
Step 84, loss: 160.95823669433594
Step 85, loss: 159.12655639648438
Step 86, loss: 157.97433471679688
Step 87, loss: 157.79510498046875
Step 88, loss: 158.76466369628906
Step 89, loss: 160.7767333984375
Step 90, loss: 163.0682373046875
Step 91, loss: 162.30001831054688
Step 92, loss: 160.13046264648438
Step 93, loss: 158.2571258544922
Step 94, loss: 157.10633850097656
Step 95, loss: 156.62815856933594
Step 96, loss: 156.91758728027344
Step 97, loss: 157.5361328125
Step 98, loss: 158.97964477539062
Step 99, loss: 158.41403198242188
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 530.7654418945312
Step 1, loss: 493.2515869140625
Step 2, loss: 471.6243896484375
Step 3, loss: 461.904541015625
Step 4, loss: 448.6400146484375
Step 5, loss: 426.0612487792969
Step 6, loss: 395.4360656738281
Step 7, loss: 360.578125
Step 8, loss: 327.465087890625
Step 9, loss: 300.2931213378906
Step 10, loss: 286.2470397949219
Step 11, loss: 284.621337890625
Step 12, loss: 286.4342956542969
Step 13, loss: 285.3709411621094
Step 14, loss: 287.08648681640625
Step 15, loss: 289.56488037109375
Step 16, loss: 291.040283203125
Step 17, loss: 291.263916015625
Step 18, loss: 290.3223876953125
Step 19, loss: 287.53656005859375
Step 20, loss: 284.05450439453125
Step 21, loss: 279.5682067871094
Step 22, loss: 271.57562255859375
Step 23, loss: 258.51983642578125
Step 24, loss: 240.45126342773438
Step 25, loss: 219.00538635253906
Step 26, loss: 199.77423095703125
Step 27, loss: 188.9589080810547
Step 28, loss: 181.7830047607422
Step 29, loss: 185.4103240966797
Step 30, loss: 197.31930541992188
Step 31, loss: 204.35128784179688
Step 32, loss: 204.2908172607422
Step 33, loss: 194.1083221435547
Step 34, loss: 184.33279418945312
Step 35, loss: 176.33392333984375
Step 36, loss: 170.2685546875
Step 37, loss: 163.34075927734375
Step 38, loss: 155.6735076904297
Step 39, loss: 152.39334106445312
Step 40, loss: 151.36431884765625
Step 41, loss: 136.8985137939453
Step 42, loss: 119.15982055664062
Step 43, loss: 120.35972595214844
Step 44, loss: 119.20806121826172
Step 45, loss: 116.48567962646484
Step 46, loss: 134.30123901367188
Step 47, loss: 142.47402954101562
Step 48, loss: 142.08294677734375
Step 49, loss: 138.2802734375
Step 50, loss: 139.2372589111328
Step 51, loss: 144.4837188720703
Step 52, loss: 150.60716247558594
Step 53, loss: 155.4930419921875
Step 54, loss: 160.06288146972656
Step 55, loss: 163.4527587890625
Step 56, loss: 167.70713806152344
Step 57, loss: 172.6605224609375
Step 58, loss: 175.31820678710938
Step 59, loss: 176.83644104003906
Step 60, loss: 177.70501708984375
Step 61, loss: 175.89915466308594
Step 62, loss: 172.7845916748047
Step 63, loss: 170.78038024902344
Step 64, loss: 170.5089874267578
Step 65, loss: 169.6123046875
Step 66, loss: 173.32821655273438
Step 67, loss: 173.34957885742188
Step 68, loss: 172.59799194335938
Step 69, loss: 170.99688720703125
Step 70, loss: 167.87069702148438
Step 71, loss: 168.540771484375
Step 72, loss: 167.34417724609375
Step 73, loss: 165.25827026367188
Step 74, loss: 164.19590759277344
Step 75, loss: 162.83541870117188
Step 76, loss: 161.98843383789062
Step 77, loss: 161.69602966308594
Step 78, loss: 162.55706787109375
Step 79, loss: 163.42398071289062
Step 80, loss: 162.21229553222656
Step 81, loss: 159.02735900878906
Step 82, loss: 157.96974182128906
Step 83, loss: 158.6006622314453
Step 84, loss: 159.1829833984375
Step 85, loss: 159.84788513183594
Step 86, loss: 160.21786499023438
Step 87, loss: 160.53793334960938
Step 88, loss: 161.5462188720703
Step 89, loss: 158.2950897216797
Step 90, loss: 155.67066955566406
Step 91, loss: 154.63470458984375
Step 92, loss: 154.4345245361328
Step 93, loss: 153.49755859375
Step 94, loss: 152.49285888671875
Step 95, loss: 153.1541748046875
Step 96, loss: 155.49041748046875
Step 97, loss: 156.13186645507812
Step 98, loss: 155.4912109375
Step 99, loss: 155.84312438964844
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 530.8319702148438
Step 1, loss: 493.27325439453125
Step 2, loss: 472.712158203125
Step 3, loss: 465.6178894042969
Step 4, loss: 455.72698974609375
Step 5, loss: 441.039306640625
Step 6, loss: 415.7757263183594
Step 7, loss: 382.4061279296875
Step 8, loss: 344.2348327636719
Step 9, loss: 307.0751647949219
Step 10, loss: 278.2325439453125
Step 11, loss: 273.7218933105469
Step 12, loss: 281.794189453125
Step 13, loss: 290.3309326171875
Step 14, loss: 298.0935363769531
Step 15, loss: 305.7278137207031
Step 16, loss: 311.1165466308594
Step 17, loss: 313.7734069824219
Step 18, loss: 314.1557312011719
Step 19, loss: 312.9823913574219
Step 20, loss: 309.33819580078125
Step 21, loss: 301.17095947265625
Step 22, loss: 291.04449462890625
Step 23, loss: 281.1300048828125
Step 24, loss: 271.58538818359375
Step 25, loss: 260.5592041015625
Step 26, loss: 246.95022583007812
Step 27, loss: 228.3907470703125
Step 28, loss: 209.36480712890625
Step 29, loss: 200.19564819335938
Step 30, loss: 195.69102478027344
Step 31, loss: 195.24720764160156
Step 32, loss: 201.38351440429688
Step 33, loss: 205.6151123046875
Step 34, loss: 201.62030029296875
Step 35, loss: 190.0384063720703
Step 36, loss: 179.2144012451172
Step 37, loss: 176.9462890625
Step 38, loss: 175.52272033691406
Step 39, loss: 170.5966796875
Step 40, loss: 163.13189697265625
Step 41, loss: 153.70570373535156
Step 42, loss: 143.64773559570312
Step 43, loss: 126.49845123291016
Step 44, loss: 116.58234405517578
Step 45, loss: 118.5648193359375
Step 46, loss: 119.41007995605469
Step 47, loss: 122.36009979248047
Step 48, loss: 123.14757537841797
Step 49, loss: 127.10206604003906
Step 50, loss: 137.630859375
Step 51, loss: 143.5157470703125
Step 52, loss: 142.01100158691406
Step 53, loss: 146.02566528320312
Step 54, loss: 153.22787475585938
Step 55, loss: 161.10157775878906
Step 56, loss: 166.6146240234375
Step 57, loss: 169.3523712158203
Step 58, loss: 171.68621826171875
Step 59, loss: 173.16197204589844
Step 60, loss: 171.66656494140625
Step 61, loss: 169.33969116210938
Step 62, loss: 167.80284118652344
Step 63, loss: 167.54859924316406
Step 64, loss: 167.07891845703125
Step 65, loss: 167.5517120361328
Step 66, loss: 170.0129852294922
Step 67, loss: 171.62274169921875
Step 68, loss: 172.08065795898438
Step 69, loss: 172.91818237304688
Step 70, loss: 173.42015075683594
Step 71, loss: 171.81112670898438
Step 72, loss: 170.34864807128906
Step 73, loss: 168.517822265625
Step 74, loss: 166.6317596435547
Step 75, loss: 165.702880859375
Step 76, loss: 166.13954162597656
Step 77, loss: 166.51417541503906
Step 78, loss: 166.81724548339844
Step 79, loss: 166.6033935546875
Step 80, loss: 165.4227752685547
Step 81, loss: 165.03370666503906
Step 82, loss: 165.49476623535156
Step 83, loss: 166.77403259277344
Step 84, loss: 168.75051879882812
Step 85, loss: 171.06695556640625
Step 86, loss: 171.22274780273438
Step 87, loss: 169.3145294189453
Step 88, loss: 166.46975708007812
Step 89, loss: 164.1056671142578
Step 90, loss: 163.505615234375
Step 91, loss: 163.50411987304688
Step 92, loss: 164.11129760742188
Step 93, loss: 165.3534698486328
Step 94, loss: 166.4576873779297
Step 95, loss: 167.8568115234375
Step 96, loss: 168.52609252929688
Step 97, loss: 167.37472534179688
Step 98, loss: 166.6676483154297
Step 99, loss: 165.06202697753906
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 530.8089599609375
Step 1, loss: 493.2874755859375
Step 2, loss: 472.6583251953125
Step 3, loss: 461.9997253417969
Step 4, loss: 451.3172607421875
Step 5, loss: 436.18438720703125
Step 6, loss: 414.20709228515625
Step 7, loss: 383.7594909667969
Step 8, loss: 347.50799560546875
Step 9, loss: 309.1224060058594
Step 10, loss: 277.8107604980469
Step 11, loss: 273.11749267578125
Step 12, loss: 280.7273254394531
Step 13, loss: 288.8636779785156
Step 14, loss: 296.3105163574219
Step 15, loss: 304.0686340332031
Step 16, loss: 308.1085510253906
Step 17, loss: 309.70849609375
Step 18, loss: 309.4937744140625
Step 19, loss: 307.6599426269531
Step 20, loss: 305.359619140625
Step 21, loss: 299.27044677734375
Step 22, loss: 290.5152587890625
Step 23, loss: 279.22918701171875
Step 24, loss: 268.0473937988281
Step 25, loss: 256.1892395019531
Step 26, loss: 241.25588989257812
Step 27, loss: 222.99444580078125
Step 28, loss: 206.24288940429688
Step 29, loss: 196.60548400878906
Step 30, loss: 190.77696228027344
Step 31, loss: 199.0337677001953
Step 32, loss: 210.44528198242188
Step 33, loss: 213.1523895263672
Step 34, loss: 205.81956481933594
Step 35, loss: 192.13037109375
Step 36, loss: 184.10382080078125
Step 37, loss: 181.78758239746094
Step 38, loss: 176.46395874023438
Step 39, loss: 169.03103637695312
Step 40, loss: 161.08428955078125
Step 41, loss: 155.6605987548828
Step 42, loss: 143.41201782226562
Step 43, loss: 121.93548583984375
Step 44, loss: 120.97030639648438
Step 45, loss: 124.20747375488281
Step 46, loss: 123.24616241455078
Step 47, loss: 127.00236511230469
Step 48, loss: 131.6254425048828
Step 49, loss: 137.6357879638672
Step 50, loss: 143.62020874023438
Step 51, loss: 140.86151123046875
Step 52, loss: 143.95706176757812
Step 53, loss: 147.80941772460938
Step 54, loss: 150.58560180664062
Step 55, loss: 155.64608764648438
Step 56, loss: 162.37554931640625
Step 57, loss: 166.05686950683594
Step 58, loss: 167.5233612060547
Step 59, loss: 169.03973388671875
Step 60, loss: 171.90127563476562
Step 61, loss: 172.80882263183594
Step 62, loss: 171.29962158203125
Step 63, loss: 170.11248779296875
Step 64, loss: 171.628173828125
Step 65, loss: 172.0261993408203
Step 66, loss: 170.95616149902344
Step 67, loss: 172.93798828125
Step 68, loss: 173.59524536132812
Step 69, loss: 169.7686767578125
Step 70, loss: 168.32867431640625
Step 71, loss: 168.02403259277344
Step 72, loss: 166.15145874023438
Step 73, loss: 166.89044189453125
Step 74, loss: 169.57223510742188
Step 75, loss: 167.54873657226562
Step 76, loss: 165.5262908935547
Step 77, loss: 164.30039978027344
Step 78, loss: 164.77728271484375
Step 79, loss: 165.34071350097656
Step 80, loss: 165.95486450195312
Step 81, loss: 166.92803955078125
Step 82, loss: 166.414794921875
Step 83, loss: 166.0642547607422
Step 84, loss: 165.38494873046875
Step 85, loss: 165.181640625
Step 86, loss: 165.75643920898438
Step 87, loss: 167.20501708984375
Step 88, loss: 168.93572998046875
Step 89, loss: 169.18487548828125
Step 90, loss: 168.877685546875
Step 91, loss: 167.86505126953125
Step 92, loss: 164.87384033203125
Step 93, loss: 163.23458862304688
Step 94, loss: 162.60414123535156
Step 95, loss: 163.00856018066406
Step 96, loss: 163.31170654296875
Step 97, loss: 163.96409606933594
Step 98, loss: 163.1820526123047
Step 99, loss: 161.12942504882812
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 530.7570190429688
Step 1, loss: 493.1976318359375
Step 2, loss: 472.48370361328125
Step 3, loss: 463.0181884765625
Step 4, loss: 445.59478759765625
Step 5, loss: 419.5790100097656
Step 6, loss: 387.3252258300781
Step 7, loss: 354.2991943359375
Step 8, loss: 323.7357482910156
Step 9, loss: 300.5164794921875
Step 10, loss: 289.36151123046875
Step 11, loss: 284.7831115722656
Step 12, loss: 281.31976318359375
Step 13, loss: 278.3334655761719
Step 14, loss: 281.95367431640625
Step 15, loss: 287.96795654296875
Step 16, loss: 292.51019287109375
Step 17, loss: 294.7607727050781
Step 18, loss: 294.73126220703125
Step 19, loss: 293.47381591796875
Step 20, loss: 290.21929931640625
Step 21, loss: 282.5959167480469
Step 22, loss: 271.50616455078125
Step 23, loss: 255.89102172851562
Step 24, loss: 238.18368530273438
Step 25, loss: 221.4425048828125
Step 26, loss: 203.99169921875
Step 27, loss: 190.69888305664062
Step 28, loss: 186.9423370361328
Step 29, loss: 189.315185546875
Step 30, loss: 193.80120849609375
Step 31, loss: 195.92071533203125
Step 32, loss: 195.7411346435547
Step 33, loss: 189.05810546875
Step 34, loss: 181.61293029785156
Step 35, loss: 180.3717498779297
Step 36, loss: 176.09429931640625
Step 37, loss: 167.04635620117188
Step 38, loss: 157.82261657714844
Step 39, loss: 151.40982055664062
Step 40, loss: 142.38787841796875
Step 41, loss: 124.77748107910156
Step 42, loss: 124.99840545654297
Step 43, loss: 129.24758911132812
Step 44, loss: 125.81401062011719
Step 45, loss: 120.91251373291016
Step 46, loss: 123.76643371582031
Step 47, loss: 132.1600799560547
Step 48, loss: 141.84791564941406
Step 49, loss: 143.6954345703125
Step 50, loss: 137.65206909179688
Step 51, loss: 138.39373779296875
Step 52, loss: 144.72048950195312
Step 53, loss: 154.74928283691406
Step 54, loss: 158.29083251953125
Step 55, loss: 160.9898681640625
Step 56, loss: 165.19476318359375
Step 57, loss: 168.47872924804688
Step 58, loss: 169.0106201171875
Step 59, loss: 167.85263061523438
Step 60, loss: 168.88885498046875
Step 61, loss: 170.5908203125
Step 62, loss: 170.0308837890625
Step 63, loss: 168.66754150390625
Step 64, loss: 168.72503662109375
Step 65, loss: 168.84747314453125
Step 66, loss: 168.7098388671875
Step 67, loss: 166.9156494140625
Step 68, loss: 167.4516143798828
Step 69, loss: 166.16836547851562
Step 70, loss: 165.75787353515625
Step 71, loss: 168.8240509033203
Step 72, loss: 168.81100463867188
Step 73, loss: 171.28787231445312
Step 74, loss: 172.91867065429688
Step 75, loss: 171.208740234375
Step 76, loss: 167.65731811523438
Step 77, loss: 166.90914916992188
Step 78, loss: 164.87442016601562
Step 79, loss: 164.16104125976562
Step 80, loss: 166.6863250732422
Step 81, loss: 168.14324951171875
Step 82, loss: 165.99832153320312
Step 83, loss: 166.33082580566406
Step 84, loss: 166.3402099609375
Step 85, loss: 166.07373046875
Step 86, loss: 166.6697998046875
Step 87, loss: 167.43356323242188
Step 88, loss: 169.53839111328125
Step 89, loss: 170.1315460205078
Step 90, loss: 166.96389770507812
Step 91, loss: 164.40289306640625
Step 92, loss: 164.16787719726562
Step 93, loss: 163.5911407470703
Step 94, loss: 162.9814910888672
Step 95, loss: 164.41392517089844
Step 96, loss: 167.50009155273438
Step 97, loss: 167.92388916015625
Step 98, loss: 164.80142211914062
Step 99, loss: 163.38720703125
logm result may be inaccurate, approximate err = 1.613971497836108e-06
Step 0, loss: 530.8001708984375
Step 1, loss: 493.24639892578125
Step 2, loss: 472.37908935546875
Step 3, loss: 467.2836608886719
Step 4, loss: 459.67413330078125
Step 5, loss: 445.13739013671875
Step 6, loss: 420.929443359375
Step 7, loss: 387.91058349609375
Step 8, loss: 350.3459167480469
Step 9, loss: 312.297607421875
Step 10, loss: 280.70745849609375
Step 11, loss: 272.068359375
Step 12, loss: 274.6437072753906
Step 13, loss: 279.507080078125
Step 14, loss: 287.17230224609375
Step 15, loss: 292.7145690917969
Step 16, loss: 294.503662109375
Step 17, loss: 293.3914794921875
Step 18, loss: 289.95672607421875
Step 19, loss: 284.61053466796875
Step 20, loss: 278.00994873046875
Step 21, loss: 271.681884765625
Step 22, loss: 264.76727294921875
Step 23, loss: 253.32749938964844
Step 24, loss: 238.13186645507812
Step 25, loss: 221.49810791015625
Step 26, loss: 208.74276733398438
Step 27, loss: 196.9927215576172
Step 28, loss: 184.4312744140625
Step 29, loss: 189.09988403320312
Step 30, loss: 198.05162048339844
Step 31, loss: 205.12106323242188
Step 32, loss: 207.10250854492188
Step 33, loss: 202.72608947753906
Step 34, loss: 198.09579467773438
Step 35, loss: 188.61276245117188
Step 36, loss: 175.97366333007812
Step 37, loss: 166.38267517089844
Step 38, loss: 161.34957885742188
Step 39, loss: 156.80911254882812
Step 40, loss: 146.01162719726562
Step 41, loss: 132.40493774414062
Step 42, loss: 127.85356903076172
Step 43, loss: 140.64105224609375
Step 44, loss: 146.67721557617188
Step 45, loss: 134.71971130371094
Step 46, loss: 121.55552673339844
Step 47, loss: 127.17532348632812
Step 48, loss: 137.69320678710938
Step 49, loss: 143.7196044921875
Step 50, loss: 145.02203369140625
Step 51, loss: 138.1067657470703
Step 52, loss: 142.1097412109375
Step 53, loss: 150.53369140625
Step 54, loss: 156.57638549804688
Step 55, loss: 157.84381103515625
Step 56, loss: 160.3323974609375
Step 57, loss: 164.01068115234375
Step 58, loss: 165.38270568847656
Step 59, loss: 163.1571807861328
Step 60, loss: 160.22235107421875
Step 61, loss: 158.80477905273438
Step 62, loss: 158.01373291015625
Step 63, loss: 158.25021362304688
Step 64, loss: 158.5746612548828
Step 65, loss: 159.2464599609375
Step 66, loss: 159.5992431640625
Step 67, loss: 160.13059997558594
Step 68, loss: 161.08544921875
Step 69, loss: 161.01736450195312
Step 70, loss: 160.82272338867188
Step 71, loss: 161.9083251953125
Step 72, loss: 164.2664337158203
Step 73, loss: 163.74517822265625
Step 74, loss: 162.55709838867188
Step 75, loss: 161.2381591796875
Step 76, loss: 160.5257568359375
Step 77, loss: 160.50149536132812
Step 78, loss: 160.93841552734375
Step 79, loss: 161.9285430908203
Step 80, loss: 163.90774536132812
Step 81, loss: 163.4609375
Step 82, loss: 163.8732147216797
Step 83, loss: 164.92254638671875
Step 84, loss: 165.3343505859375
Step 85, loss: 165.05242919921875
Step 86, loss: 164.84983825683594
Step 87, loss: 164.75494384765625
Step 88, loss: 164.37091064453125
Step 89, loss: 163.02764892578125
Step 90, loss: 161.99954223632812
Step 91, loss: 162.52444458007812
Step 92, loss: 161.8254852294922
Step 93, loss: 161.19219970703125
Step 94, loss: 162.00677490234375
Step 95, loss: 161.9088134765625
Step 96, loss: 161.1265869140625
Step 97, loss: 160.90435791015625
Step 98, loss: 162.232177734375
Step 99, loss: 163.71778869628906
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 0.59553461,  0.01537371,  0.80318253,  0.19212415],
       [-0.80296378,  0.04156386,  0.59457683,  0.75547979],
       [-0.02424251, -0.99901756,  0.03709725,  0.06385399],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.61841662, -0.02083289,  0.78557423,  0.18368121],
       [-0.78284218,  0.07105261,  0.61815018,  0.76739094],
       [-0.06869496, -0.99725499,  0.02763125,  0.06192013],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 0.61177458,  0.01391903,  0.79090968,  0.0392188 ],
       [-0.79080859, -0.01300791,  0.6119253 ,  0.75795209],
       [ 0.01880549, -0.99981851,  0.00304938,  0.16147622],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[ 0.61177458,  0.01391903,  0.79090968,  0.0392188 ],
       [-0.79080859, -0.01300791,  0.6119253 ,  0.75795209],
       [ 0.01880549, -0.99981851,  0.00304938,  0.16147622],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[ 0.64375138,  0.11800564,  0.75608116,  0.01104888],
       [-0.76211071,  0.00967779,  0.64737457,  0.84313782],
       [ 0.06907669, -0.99296588,  0.09616343,  0.05468226],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
loading data:   0%|          | 0/5 [00:00<?, ?it/s]loading data: 100%|██████████| 5/5 [00:00<00:00, 58.93it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666
loading data:   0%|          | 0/66 [00:00<?, ?it/s]loading data:  11%|█         | 7/66 [00:00<00:00, 62.50it/s]loading data:  21%|██        | 14/66 [00:00<00:00, 62.53it/s]loading data:  33%|███▎      | 22/66 [00:00<00:00, 66.49it/s]loading data:  44%|████▍     | 29/66 [00:00<00:00, 66.87it/s]loading data:  56%|█████▌    | 37/66 [00:00<00:00, 68.32it/s]loading data:  68%|██████▊   | 45/66 [00:00<00:00, 69.07it/s]loading data:  80%|████████  | 53/66 [00:00<00:00, 69.70it/s]loading data:  91%|█████████ | 60/66 [00:00<00:00, 68.13it/s]loading data: 100%|██████████| 66/66 [00:00<00:00, 67.58it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:384: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(get_se3_pose_grad(pose_6d_vars), device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.2057545930147171
Loss after optimization 0.06857780367136002
logm result may be inaccurate, approximate err = 3.420829249488071e-07
Loss at initial point 0.21916097402572632
Loss after optimization 0.10674253106117249
logm result may be inaccurate, approximate err = 2.664359732585912e-07
Loss at initial point 0.2474389225244522
Loss after optimization 0.06580937653779984
logm result may be inaccurate, approximate err = 1.0086927141307791e-06
Loss at initial point 0.22600208222866058
Loss after optimization 0.06879230588674545
logm result may be inaccurate, approximate err = 3.6483672169144887e-07
Loss at initial point 0.300357848405838
Loss after optimization 0.06843122094869614
logm result may be inaccurate, approximate err = 5.928659599461187e-07
Loss at initial point 0.23035332560539246
Loss after optimization 0.06993388384580612
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.20592905580997467
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 3.420829249488071e-07
Loss at initial point 0.21923202276229858
Loss after optimization 0.11624011397361755
logm result may be inaccurate, approximate err = 2.664359732585912e-07
Loss at initial point 0.2474580705165863
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 1.0086927141307791e-06
Loss at initial point 0.22599488496780396
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 3.6483672169144887e-07
Loss at initial point 0.3004322648048401
Loss after optimization 0.11623860895633698
logm result may be inaccurate, approximate err = 5.928659599461187e-07
Loss at initial point 0.23025745153427124
Loss after optimization 0.11623857915401459
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.20587961375713348
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 3.420829249488071e-07
Loss at initial point 0.2191687524318695
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 2.664359732585912e-07
Loss at initial point 0.24747858941555023
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 1.0086927141307791e-06
Loss at initial point 0.22590820491313934
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 3.6483672169144887e-07
Loss at initial point 0.3003173768520355
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 5.928659599461187e-07
Loss at initial point 0.23036867380142212
Loss after optimization 0.1162385568022728
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Step 0, loss: 138.78622436523438
Step 1, loss: 130.42767333984375
Step 2, loss: 122.85620880126953
Step 3, loss: 119.30040740966797
Step 4, loss: 117.01752471923828
Step 5, loss: 115.34722900390625
Step 6, loss: 113.84574127197266
Step 7, loss: 112.36488342285156
Step 8, loss: 111.56019592285156
Step 9, loss: 110.9883041381836
Step 10, loss: 110.60287475585938
Step 11, loss: 110.058837890625
Step 12, loss: 109.73009490966797
Step 13, loss: 109.55033111572266
Step 14, loss: 109.63218688964844
Step 15, loss: 109.67027282714844
Step 16, loss: 109.56278991699219
Step 17, loss: 109.40950012207031
Step 18, loss: 109.25914001464844
Step 19, loss: 108.92461395263672
Step 20, loss: 108.57075500488281
Step 21, loss: 108.13837432861328
Step 22, loss: 107.65672302246094
Step 23, loss: 107.2123031616211
Step 24, loss: 106.79206085205078
Step 25, loss: 106.39005279541016
Step 26, loss: 105.96401977539062
Step 27, loss: 105.57418060302734
Step 28, loss: 105.26761627197266
Step 29, loss: 104.977294921875
Step 30, loss: 104.72209167480469
Step 31, loss: 104.50940704345703
Step 32, loss: 104.3140640258789
Step 33, loss: 104.16085052490234
Step 34, loss: 103.99669647216797
Step 35, loss: 103.7846450805664
Step 36, loss: 103.5866470336914
Step 37, loss: 103.35086822509766
Step 38, loss: 103.05335235595703
Step 39, loss: 102.70026397705078
Step 40, loss: 102.34440612792969
Step 41, loss: 101.91221618652344
Step 42, loss: 101.49439239501953
Step 43, loss: 101.04852294921875
Step 44, loss: 100.64459991455078
Step 45, loss: 100.30097198486328
Step 46, loss: 99.90250396728516
Step 47, loss: 99.38728332519531
Step 48, loss: 98.91352081298828
Step 49, loss: 98.46917724609375
Step 50, loss: 97.99615478515625
Step 51, loss: 97.56788635253906
Step 52, loss: 97.18075561523438
Step 53, loss: 96.77285766601562
Step 54, loss: 96.39949035644531
Step 55, loss: 96.06681823730469
Step 56, loss: 95.75841522216797
Step 57, loss: 95.39014434814453
Step 58, loss: 94.89525604248047
Step 59, loss: 94.3177261352539
Step 60, loss: 93.6444091796875
Step 61, loss: 92.89398956298828
Step 62, loss: 91.95820617675781
Step 63, loss: 90.94265747070312
Step 64, loss: 90.02831268310547
Step 65, loss: 89.27847290039062
Step 66, loss: 88.7716064453125
Step 67, loss: 88.5909423828125
Step 68, loss: 88.59171295166016
Step 69, loss: 88.8062744140625
Step 70, loss: 88.98657989501953
Step 71, loss: 89.0462417602539
Step 72, loss: 88.96527862548828
Step 73, loss: 88.68424987792969
Step 74, loss: 87.99198150634766
Step 75, loss: 87.14069366455078
Step 76, loss: 86.33634948730469
Step 77, loss: 85.5330581665039
Step 78, loss: 84.77093505859375
Step 79, loss: 84.22007751464844
Step 80, loss: 83.81735229492188
Step 81, loss: 83.42976379394531
Step 82, loss: 83.22515106201172
Step 83, loss: 83.1941909790039
Step 84, loss: 83.24766540527344
Step 85, loss: 83.46378326416016
Step 86, loss: 83.76513671875
Step 87, loss: 83.8324203491211
Step 88, loss: 84.09110260009766
Step 89, loss: 84.56101989746094
Step 90, loss: 85.06906127929688
Step 91, loss: 85.55134582519531
Step 92, loss: 85.97148895263672
Step 93, loss: 86.3821029663086
Step 94, loss: 86.72584533691406
Step 95, loss: 86.97673797607422
Step 96, loss: 87.1499252319336
Step 97, loss: 87.2345962524414
Step 98, loss: 87.25448608398438
Step 99, loss: 87.12474822998047
logm result may be inaccurate, approximate err = 3.420829249488071e-07
Step 0, loss: 144.85250854492188
Step 1, loss: 143.21543884277344
Step 2, loss: 140.68113708496094
Step 3, loss: 138.02500915527344
Step 4, loss: 134.87295532226562
Step 5, loss: 132.16018676757812
Step 6, loss: 129.73863220214844
Step 7, loss: 127.58547973632812
Step 8, loss: 125.6718521118164
Step 9, loss: 124.08403015136719
Step 10, loss: 122.68016052246094
Step 11, loss: 121.47093963623047
Step 12, loss: 120.42388153076172
Step 13, loss: 119.57433319091797
Step 14, loss: 118.8841323852539
Step 15, loss: 118.2421875
Step 16, loss: 117.7076644897461
Step 17, loss: 117.20000457763672
Step 18, loss: 116.75423431396484
Step 19, loss: 116.36851501464844
Step 20, loss: 116.01943969726562
Step 21, loss: 115.71176147460938
Step 22, loss: 115.38711547851562
Step 23, loss: 115.16021728515625
Step 24, loss: 114.97806549072266
Step 25, loss: 114.78372192382812
Step 26, loss: 114.5749740600586
Step 27, loss: 114.36410522460938
Step 28, loss: 114.17768859863281
Step 29, loss: 114.0114517211914
Step 30, loss: 113.86485290527344
Step 31, loss: 113.6690444946289
Step 32, loss: 113.51045227050781
Step 33, loss: 113.33907318115234
Step 34, loss: 113.20947265625
Step 35, loss: 113.04471588134766
Step 36, loss: 112.88009643554688
Step 37, loss: 112.72201538085938
Step 38, loss: 112.56791687011719
Step 39, loss: 112.4250259399414
Step 40, loss: 112.30532836914062
Step 41, loss: 112.17686462402344
Step 42, loss: 112.06619262695312
Step 43, loss: 111.87920379638672
Step 44, loss: 111.76107788085938
Step 45, loss: 111.64920806884766
Step 46, loss: 111.52228546142578
Step 47, loss: 111.40253448486328
Step 48, loss: 111.2782974243164
Step 49, loss: 111.15869903564453
Step 50, loss: 111.05101013183594
Step 51, loss: 110.91044616699219
Step 52, loss: 110.7732162475586
Step 53, loss: 110.6402816772461
Step 54, loss: 110.50434875488281
Step 55, loss: 110.36221313476562
Step 56, loss: 110.25959777832031
Step 57, loss: 110.12541198730469
Step 58, loss: 110.01478576660156
Step 59, loss: 109.9022445678711
Step 60, loss: 109.80712127685547
Step 61, loss: 109.6988296508789
Step 62, loss: 109.61140441894531
Step 63, loss: 109.50428009033203
Step 64, loss: 109.38227844238281
Step 65, loss: 109.29724884033203
Step 66, loss: 109.19995880126953
Step 67, loss: 109.11499786376953
Step 68, loss: 109.01980590820312
Step 69, loss: 108.95281219482422
Step 70, loss: 108.85987091064453
Step 71, loss: 108.8030014038086
Step 72, loss: 108.73977661132812
Step 73, loss: 108.64400482177734
Step 74, loss: 108.5916748046875
Step 75, loss: 108.49553680419922
Step 76, loss: 108.41532135009766
Step 77, loss: 108.3560791015625
Step 78, loss: 108.30891418457031
Step 79, loss: 108.2477035522461
Step 80, loss: 108.1967544555664
Step 81, loss: 108.13392639160156
Step 82, loss: 108.07508850097656
Step 83, loss: 108.02418518066406
Step 84, loss: 107.96048736572266
Step 85, loss: 107.90521240234375
Step 86, loss: 107.83828735351562
Step 87, loss: 107.7914810180664
Step 88, loss: 107.74244689941406
Step 89, loss: 107.6773452758789
Step 90, loss: 107.62812805175781
Step 91, loss: 107.56806182861328
Step 92, loss: 107.51528930664062
Step 93, loss: 107.4616928100586
Step 94, loss: 107.43373107910156
Step 95, loss: 107.37469482421875
Step 96, loss: 107.33828735351562
Step 97, loss: 107.29255676269531
Step 98, loss: 107.26290130615234
Step 99, loss: 107.20323181152344
logm result may be inaccurate, approximate err = 2.664359732585912e-07
Step 0, loss: 155.61138916015625
Step 1, loss: 152.17333984375
Step 2, loss: 148.28172302246094
Step 3, loss: 144.4542999267578
Step 4, loss: 140.89486694335938
Step 5, loss: 137.70111083984375
Step 6, loss: 134.61697387695312
Step 7, loss: 131.89743041992188
Step 8, loss: 129.35128784179688
Step 9, loss: 127.1166763305664
Step 10, loss: 125.05348205566406
Step 11, loss: 123.18212127685547
Step 12, loss: 121.49850463867188
Step 13, loss: 120.03515625
Step 14, loss: 118.68126678466797
Step 15, loss: 117.5902099609375
Step 16, loss: 116.62101745605469
Step 17, loss: 115.82257080078125
Step 18, loss: 115.17206573486328
Step 19, loss: 114.67729187011719
Step 20, loss: 114.24845123291016
Step 21, loss: 113.90221405029297
Step 22, loss: 113.6180648803711
Step 23, loss: 113.34107208251953
Step 24, loss: 113.08174133300781
Step 25, loss: 112.8683090209961
Step 26, loss: 112.70645904541016
Step 27, loss: 112.6097640991211
Step 28, loss: 112.51631927490234
Step 29, loss: 112.42050170898438
Step 30, loss: 112.32048034667969
Step 31, loss: 112.23526000976562
Step 32, loss: 112.15646362304688
Step 33, loss: 112.05667114257812
Step 34, loss: 111.97688293457031
Step 35, loss: 111.86247253417969
Step 36, loss: 111.82844543457031
Step 37, loss: 111.71576690673828
Step 38, loss: 111.65283966064453
Step 39, loss: 111.55329895019531
Step 40, loss: 111.45785522460938
Step 41, loss: 111.36925506591797
Step 42, loss: 111.33929443359375
Step 43, loss: 111.22341918945312
Step 44, loss: 111.1236572265625
Step 45, loss: 111.06568908691406
Step 46, loss: 110.97852325439453
Step 47, loss: 110.86656951904297
Step 48, loss: 110.7790298461914
Step 49, loss: 110.68087768554688
Step 50, loss: 110.5842056274414
Step 51, loss: 110.51996612548828
Step 52, loss: 110.427001953125
Step 53, loss: 110.32814025878906
Step 54, loss: 110.26959228515625
Step 55, loss: 110.20340728759766
Step 56, loss: 110.11804962158203
Step 57, loss: 110.04874420166016
Step 58, loss: 109.98355865478516
Step 59, loss: 109.87948608398438
Step 60, loss: 109.84480285644531
Step 61, loss: 109.77278900146484
Step 62, loss: 109.73831939697266
Step 63, loss: 109.64472961425781
Step 64, loss: 109.60108184814453
Step 65, loss: 109.5621337890625
Step 66, loss: 109.4590072631836
Step 67, loss: 109.41016387939453
Step 68, loss: 109.36458587646484
Step 69, loss: 109.3074722290039
Step 70, loss: 109.2547836303711
Step 71, loss: 109.1844253540039
Step 72, loss: 109.15681457519531
Step 73, loss: 109.09857940673828
Step 74, loss: 109.03678131103516
Step 75, loss: 108.98765563964844
Step 76, loss: 108.96344757080078
Step 77, loss: 108.87826538085938
Step 78, loss: 108.83949279785156
Step 79, loss: 108.80390167236328
Step 80, loss: 108.76010131835938
Step 81, loss: 108.69120025634766
Step 82, loss: 108.67450714111328
Step 83, loss: 108.62397766113281
Step 84, loss: 108.56552124023438
Step 85, loss: 108.52555084228516
Step 86, loss: 108.50584411621094
Step 87, loss: 108.4422836303711
Step 88, loss: 108.40330505371094
Step 89, loss: 108.35546875
Step 90, loss: 108.3091049194336
Step 91, loss: 108.29483032226562
Step 92, loss: 108.24861145019531
Step 93, loss: 108.21680450439453
Step 94, loss: 108.1623306274414
Step 95, loss: 108.1301040649414
Step 96, loss: 108.10594177246094
Step 97, loss: 108.07275390625
Step 98, loss: 108.04605865478516
Step 99, loss: 107.99945831298828
logm result may be inaccurate, approximate err = 1.0086927141307791e-06
Step 0, loss: 147.0364990234375
Step 1, loss: 138.06903076171875
Step 2, loss: 129.5753936767578
Step 3, loss: 121.65725708007812
Step 4, loss: 114.74179077148438
Step 5, loss: 108.45466613769531
Step 6, loss: 106.28852844238281
Step 7, loss: 104.79439544677734
Step 8, loss: 103.78486633300781
Step 9, loss: 102.81552124023438
Step 10, loss: 101.2906494140625
Step 11, loss: 99.69074249267578
Step 12, loss: 98.16694641113281
Step 13, loss: 96.40150451660156
Step 14, loss: 94.62407684326172
Step 15, loss: 92.94579315185547
Step 16, loss: 91.30493927001953
Step 17, loss: 89.85140991210938
Step 18, loss: 88.57913970947266
Step 19, loss: 87.16695404052734
Step 20, loss: 86.09954833984375
Step 21, loss: 85.35518646240234
Step 22, loss: 84.85277557373047
Step 23, loss: 84.5350570678711
Step 24, loss: 83.87910461425781
Step 25, loss: 83.30039978027344
Step 26, loss: 82.65670776367188
Step 27, loss: 81.65939331054688
Step 28, loss: 80.40089416503906
Step 29, loss: 79.10763549804688
Step 30, loss: 78.0964584350586
Step 31, loss: 78.34717559814453
Step 32, loss: 79.83001708984375
Step 33, loss: 81.07106018066406
Step 34, loss: 81.93572235107422
Step 35, loss: 82.41737365722656
Step 36, loss: 82.49800872802734
Step 37, loss: 82.32461547851562
Step 38, loss: 81.81925201416016
Step 39, loss: 81.11214447021484
Step 40, loss: 80.234375
Step 41, loss: 79.14693450927734
Step 42, loss: 77.9239501953125
Step 43, loss: 77.06320190429688
Step 44, loss: 77.10009765625
Step 45, loss: 77.52388763427734
Step 46, loss: 77.76306915283203
Step 47, loss: 77.74713134765625
Step 48, loss: 77.50029754638672
Step 49, loss: 77.17030334472656
Step 50, loss: 77.11339569091797
Step 51, loss: 77.53291320800781
Step 52, loss: 78.08130645751953
Step 53, loss: 78.44159698486328
Step 54, loss: 78.59342956542969
Step 55, loss: 78.4847640991211
Step 56, loss: 78.17866516113281
Step 57, loss: 77.64336395263672
Step 58, loss: 77.17957305908203
Step 59, loss: 76.6589126586914
Step 60, loss: 76.34595489501953
Step 61, loss: 76.21632385253906
Step 62, loss: 76.18019104003906
Step 63, loss: 76.0691146850586
Step 64, loss: 76.07243347167969
Step 65, loss: 76.10135650634766
Step 66, loss: 76.27926635742188
Step 67, loss: 76.46336364746094
Step 68, loss: 76.66136169433594
Step 69, loss: 76.72616577148438
Step 70, loss: 76.65592193603516
Step 71, loss: 76.53308868408203
Step 72, loss: 76.31781005859375
Step 73, loss: 76.01885986328125
Step 74, loss: 75.77073669433594
Step 75, loss: 75.47593688964844
Step 76, loss: 75.13957214355469
Step 77, loss: 74.85643005371094
Step 78, loss: 74.56971740722656
Step 79, loss: 74.34599304199219
Step 80, loss: 74.14167022705078
Step 81, loss: 73.89620971679688
Step 82, loss: 73.7386474609375
Step 83, loss: 73.64698791503906
Step 84, loss: 73.45201110839844
Step 85, loss: 73.31893157958984
Step 86, loss: 73.04854583740234
Step 87, loss: 72.76000213623047
Step 88, loss: 72.44695281982422
Step 89, loss: 72.08490753173828
Step 90, loss: 71.710205078125
Step 91, loss: 71.33680725097656
Step 92, loss: 70.83061981201172
Step 93, loss: 70.3818359375
Step 94, loss: 69.8693618774414
Step 95, loss: 69.40390014648438
Step 96, loss: 68.9457015991211
Step 97, loss: 68.4699478149414
Step 98, loss: 68.14669799804688
Step 99, loss: 67.9473648071289
logm result may be inaccurate, approximate err = 3.6483672169144887e-07
Step 0, loss: 168.8327178955078
Step 1, loss: 161.77056884765625
Step 2, loss: 156.1045379638672
Step 3, loss: 150.67745971679688
Step 4, loss: 145.58322143554688
Step 5, loss: 141.10072326660156
Step 6, loss: 137.09349060058594
Step 7, loss: 133.58897399902344
Step 8, loss: 130.6478729248047
Step 9, loss: 128.25559997558594
Step 10, loss: 126.29163360595703
Step 11, loss: 124.8281021118164
Step 12, loss: 123.59496307373047
Step 13, loss: 122.6048812866211
Step 14, loss: 121.81836700439453
Step 15, loss: 121.10552978515625
Step 16, loss: 120.55075073242188
Step 17, loss: 120.07463836669922
Step 18, loss: 119.64264678955078
Step 19, loss: 119.3267822265625
Step 20, loss: 119.0038833618164
Step 21, loss: 118.72264862060547
Step 22, loss: 118.43746948242188
Step 23, loss: 118.12088012695312
Step 24, loss: 117.82669067382812
Step 25, loss: 117.5467529296875
Step 26, loss: 117.22120666503906
Step 27, loss: 116.947998046875
Step 28, loss: 116.65646362304688
Step 29, loss: 116.40520477294922
Step 30, loss: 116.16117095947266
Step 31, loss: 115.8641586303711
Step 32, loss: 115.62371063232422
Step 33, loss: 115.35150909423828
Step 34, loss: 115.07410430908203
Step 35, loss: 114.80941772460938
Step 36, loss: 114.51362609863281
Step 37, loss: 114.26301574707031
Step 38, loss: 113.98727416992188
Step 39, loss: 113.7649154663086
Step 40, loss: 113.4848403930664
Step 41, loss: 113.2733154296875
Step 42, loss: 113.01675415039062
Step 43, loss: 112.80794525146484
Step 44, loss: 112.58736419677734
Step 45, loss: 112.3885269165039
Step 46, loss: 112.23214721679688
Step 47, loss: 112.01551818847656
Step 48, loss: 111.87210845947266
Step 49, loss: 111.67731475830078
Step 50, loss: 111.51226806640625
Step 51, loss: 111.35406494140625
Step 52, loss: 111.20050811767578
Step 53, loss: 111.0546646118164
Step 54, loss: 110.90535736083984
Step 55, loss: 110.76480865478516
Step 56, loss: 110.64183044433594
Step 57, loss: 110.52628326416016
Step 58, loss: 110.4375991821289
Step 59, loss: 110.31452941894531
Step 60, loss: 110.2475357055664
Step 61, loss: 110.13644409179688
Step 62, loss: 110.05320739746094
Step 63, loss: 109.9627685546875
Step 64, loss: 109.87440490722656
Step 65, loss: 109.8048095703125
Step 66, loss: 109.73310852050781
Step 67, loss: 109.6674575805664
Step 68, loss: 109.58783721923828
Step 69, loss: 109.51878356933594
Step 70, loss: 109.45356750488281
Step 71, loss: 109.38848876953125
Step 72, loss: 109.32200622558594
Step 73, loss: 109.25902557373047
Step 74, loss: 109.2197036743164
Step 75, loss: 109.1343765258789
Step 76, loss: 109.08292388916016
Step 77, loss: 109.0116195678711
Step 78, loss: 108.96422576904297
Step 79, loss: 108.92826843261719
Step 80, loss: 108.88325500488281
Step 81, loss: 108.8289566040039
Step 82, loss: 108.77372741699219
Step 83, loss: 108.73565673828125
Step 84, loss: 108.67259979248047
Step 85, loss: 108.64600372314453
Step 86, loss: 108.60079193115234
Step 87, loss: 108.54690551757812
Step 88, loss: 108.5084228515625
Step 89, loss: 108.47615814208984
Step 90, loss: 108.42701721191406
Step 91, loss: 108.3930435180664
Step 92, loss: 108.35890197753906
Step 93, loss: 108.31375122070312
Step 94, loss: 108.26756286621094
Step 95, loss: 108.2412109375
Step 96, loss: 108.2064437866211
Step 97, loss: 108.16524505615234
Step 98, loss: 108.11736297607422
Step 99, loss: 108.08967590332031
logm result may be inaccurate, approximate err = 5.928659599461187e-07
Step 0, loss: 146.3787384033203
Step 1, loss: 131.62496948242188
Step 2, loss: 118.03699493408203
Step 3, loss: 105.97985076904297
Step 4, loss: 94.8843002319336
Step 5, loss: 84.4921646118164
Step 6, loss: 82.37724304199219
Step 7, loss: 84.8509750366211
Step 8, loss: 86.25020599365234
Step 9, loss: 87.32766723632812
Step 10, loss: 88.4403076171875
Step 11, loss: 89.50636291503906
Step 12, loss: 90.6111068725586
Step 13, loss: 91.69510650634766
Step 14, loss: 92.58191680908203
Step 15, loss: 93.45268249511719
Step 16, loss: 94.16497802734375
Step 17, loss: 93.96467590332031
Step 18, loss: 93.5945816040039
Step 19, loss: 92.93438720703125
Step 20, loss: 92.12285614013672
Step 21, loss: 91.09968566894531
Step 22, loss: 89.78791809082031
Step 23, loss: 88.20671844482422
Step 24, loss: 86.34443664550781
Step 25, loss: 84.32862091064453
Step 26, loss: 81.99832916259766
Step 27, loss: 79.53412628173828
Step 28, loss: 77.40271759033203
Step 29, loss: 75.29395294189453
Step 30, loss: 73.4635238647461
Step 31, loss: 71.70955657958984
Step 32, loss: 70.73633575439453
Step 33, loss: 70.4381103515625
Step 34, loss: 70.9168701171875
Step 35, loss: 71.9773941040039
Step 36, loss: 73.46708679199219
Step 37, loss: 75.34699249267578
Step 38, loss: 77.31771087646484
Step 39, loss: 79.18706512451172
Step 40, loss: 80.8400650024414
Step 41, loss: 82.1859130859375
Step 42, loss: 83.32132720947266
Step 43, loss: 84.07256317138672
Step 44, loss: 84.48800659179688
Step 45, loss: 84.61564636230469
Step 46, loss: 84.36734008789062
Step 47, loss: 84.16292572021484
Step 48, loss: 83.82072448730469
Step 49, loss: 83.43163299560547
Step 50, loss: 83.06731414794922
Step 51, loss: 82.71805572509766
Step 52, loss: 82.37106323242188
Step 53, loss: 82.22467041015625
Step 54, loss: 82.1099624633789
Step 55, loss: 82.0652084350586
Step 56, loss: 82.18175506591797
Step 57, loss: 82.20724487304688
Step 58, loss: 82.29901885986328
Step 59, loss: 82.49969482421875
Step 60, loss: 82.61166381835938
Step 61, loss: 82.82666778564453
Step 62, loss: 82.93324279785156
Step 63, loss: 83.1075210571289
Step 64, loss: 83.27313995361328
Step 65, loss: 83.38587188720703
Step 66, loss: 83.52545166015625
Step 67, loss: 83.64569854736328
Step 68, loss: 83.79239654541016
Step 69, loss: 83.96477508544922
Step 70, loss: 84.08673858642578
Step 71, loss: 84.28800201416016
Step 72, loss: 84.48728942871094
Step 73, loss: 84.60250091552734
Step 74, loss: 84.74994659423828
Step 75, loss: 84.88619232177734
Step 76, loss: 85.05826568603516
Step 77, loss: 85.16261291503906
Step 78, loss: 85.28736877441406
Step 79, loss: 85.37527465820312
Step 80, loss: 85.46888732910156
Step 81, loss: 85.53080749511719
Step 82, loss: 85.55726623535156
Step 83, loss: 85.49694061279297
Step 84, loss: 85.54118347167969
Step 85, loss: 85.44359588623047
Step 86, loss: 85.36941528320312
Step 87, loss: 85.38601684570312
Step 88, loss: 85.39710235595703
Step 89, loss: 85.33995056152344
Step 90, loss: 85.32156372070312
Step 91, loss: 85.32142639160156
Step 92, loss: 85.33528900146484
Step 93, loss: 85.28575897216797
Step 94, loss: 85.22649383544922
Step 95, loss: 85.17501831054688
Step 96, loss: 85.14220428466797
Step 97, loss: 85.08289337158203
Step 98, loss: 85.04203033447266
Step 99, loss: 85.02531433105469
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
torch.float32 torch.float32
{'Nelder-Mead': array([[ 0.93545127, -0.33673589,  0.10742378, -0.00147419],
       [-0.12495763, -0.59936507, -0.79066245,  0.65398343],
       [ 0.33063048,  0.72620277, -0.60275453,  0.14064723],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'COBYLA': array([[ 0.17093027,  0.07178943,  0.9826643 , -0.1236691 ],
       [-0.65281388,  0.75526558,  0.05837758,  0.50634426],
       [-0.73798162, -0.65147539,  0.17596288,  0.59144079],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'Powell': array([[ 0.58481313,  0.00496414,  0.81115286, -0.42193514],
       [-0.81114405, -0.00411346,  0.58483195,  1.61529571],
       [ 0.00623984, -0.99997922,  0.00162103,  0.13349777],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'total': array([[ 0.93545127, -0.33673589,  0.10742378, -0.00147419],
       [-0.12495763, -0.59936507, -0.79066245,  0.65398343],
       [ 0.33063048,  0.72620277, -0.60275453,  0.14064723],
       [ 0.        ,  0.        ,  0.        ,  1.        ]]), 'grad': array([[ 0.54534554, -0.06736487,  0.83549941,  0.00876621],
       [-0.7278564 , -0.53241426,  0.43215758,  0.91065269],
       [ 0.41571957, -0.84379929, -0.33938175,  0.01256133],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])}
['val', 'train', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']
<class 'wisp.datasets.formats.rtmv_dataset.RTMVDataset'>
False
<class 'wisp.datasets.formats.nerf_standard_dataset.NeRFSyntheticDataset'>
True
loading data:   0%|          | 0/5 [00:00<?, ?it/s]loading data: 100%|██████████| 5/5 [00:00<00:00, 60.92it/s]
/home/saptarshi/miniconda3/envs/wisp2/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
128.66666666666666
loading data:   0%|          | 0/66 [00:00<?, ?it/s]loading data:  11%|█         | 7/66 [00:00<00:00, 65.90it/s]loading data:  21%|██        | 14/66 [00:00<00:00, 63.80it/s]loading data:  32%|███▏      | 21/66 [00:00<00:00, 65.05it/s]loading data:  42%|████▏     | 28/66 [00:00<00:00, 65.42it/s]loading data:  53%|█████▎    | 35/66 [00:00<00:00, 65.44it/s]loading data:  64%|██████▎   | 42/66 [00:00<00:00, 65.31it/s]loading data:  74%|███████▍  | 49/66 [00:00<00:00, 64.89it/s]loading data:  85%|████████▍ | 56/66 [00:00<00:00, 65.87it/s]loading data:  95%|█████████▌| 63/66 [00:00<00:00, 65.48it/s]loading data: 100%|██████████| 66/66 [00:01<00:00, 65.42it/s]
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0])
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:218: ComplexWarning: Casting complex values to real discards the imaginary part
  pose_6d[3:] = np.array((skm[0,1], skm[0,2], skm[1,2]))
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_pose = torch.tensor(target_poses[0]).detach()
/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  camera.extrinsics._backend.params = torch.tensor(p, device='cuda', dtype=torch.float32)
155.46740898662756
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.936084121465683
Loss after optimization 0.393703818321228
logm result may be inaccurate, approximate err = 7.031527684499531e-07
Loss at initial point 0.895940288901329
Loss after optimization 0.41217947006225586
logm result may be inaccurate, approximate err = 2.074488562152596e-07
Loss at initial point 1.1057226806879044
Loss after optimization 0.4002379849553108
logm result may be inaccurate, approximate err = 3.1991526384767345e-07
Loss at initial point 1.045529067516327
Loss after optimization 0.5767034292221069
logm result may be inaccurate, approximate err = 3.0249704045210487e-07
Loss at initial point 0.9698903858661652
Loss after optimization 0.46726230531930923
logm result may be inaccurate, approximate err = 6.248129815937785e-07
Loss at initial point 0.7846586257219315
Loss after optimization 0.2908485606312752
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.935811772942543
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 7.031527684499531e-07
Loss at initial point 0.8959047794342041
Loss after optimization 0.5668358951807022
logm result may be inaccurate, approximate err = 2.074488562152596e-07
Loss at initial point 1.105693370103836
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 3.1991526384767345e-07
Loss at initial point 1.0457140058279037
Loss after optimization 0.5901918336749077
logm result may be inaccurate, approximate err = 3.0249704045210487e-07
Loss at initial point 0.9700672179460526
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 6.248129815937785e-07
Loss at initial point 0.7847901433706284
Loss after optimization 0.5716328397393227
logm result may be inaccurate, approximate err = 9.995835174264334e-07
Loss at initial point 0.9360495954751968
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 7.031527684499531e-07
Loss at initial point 0.8958341628313065
Loss after optimization 0.5670625492930412
logm result may be inaccurate, approximate err = 2.074488562152596e-07
Loss at initial point 1.105572685599327
Loss after optimization 0.5915266647934914
logm result may be inaccurate, approximate err = 3.1991526384767345e-07
Loss at initial point 1.0456853806972504
Loss after optimization 0.5587029904127121
logm result may be inaccurate, approximate err = 3.0249704045210487e-07
Loss at initial point 0.9700595140457153
Loss after optimization 0.5670234486460686
logm result may be inaccurate, approximate err = 6.248129815937785e-07
Loss at initial point 0.7847438752651215
Loss after optimization 0.5915266647934914
Traceback (most recent call last):
  File "/home/saptarshi/dev/kaolin-wisp/notebooks/inerf_results_multi_image.py", line 439, in <module>
    pickle.dump([losses, poses], open(f'{args.output}/losses_poses.pkl', 'wb'))
FileNotFoundError: [Errno 2] No such file or directory: '/home/saptarshi/dev/CustomComposer/inerf_res/spam_multi_img/losses_poses.pkl'
